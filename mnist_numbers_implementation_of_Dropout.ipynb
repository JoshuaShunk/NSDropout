{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist numbers implementation of Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_numbers_implementation_of_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYgI3SFHqm4"
      },
      "source": [
        "# MNIST Numbers Implementation of Old Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HD9nTuEVHD"
      },
      "source": [
        "np.random.seed(seed=22) #Random seed used for comparison between old dropout"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cag8ZraxEZbF",
        "outputId": "7fbe2024-06b3-4cb3-e78a-421b918e74be"
      },
      "source": [
        "print(np.random.random(size=3)) #Check that seeds line up"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20846054 0.48168106 0.42053804]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers (Credit to Harrison Kinsley & Daniel Kukiela for raw python implementation)\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "60322c40-31f4-468b-9784-e8457930a666"
      },
      "source": [
        "#(X, y), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "# load dataset\n",
        "(X, y), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Label index to label name relation\n",
        "number_mnist_labels = {\n",
        "    0: '0',\n",
        "    1: '1',\n",
        "    2: '2',\n",
        "    3: '3',\n",
        "    4: '4',\n",
        "    5: '5',\n",
        "    6: '6',\n",
        "    7: '7',\n",
        "    8: '8',\n",
        "    9: '9'\n",
        "}\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "\n",
        "X = X[:8000,:,:]\n",
        "X_test = X_test[:1600,:,:]\n",
        "y = y[:8000]\n",
        "y_test  = y_test[:1600]\n",
        "\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 784)\n",
            "(8000,)\n",
            "(1600, 784)\n",
            "(1600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nW7mqGTnem"
      },
      "source": [
        "Sorting Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtvA9y81TpGf",
        "outputId": "d1e0a774-08f4-48ae-95c6-26f3c8a29381"
      },
      "source": [
        "idx = np.argsort(y)\n",
        "X_sorted = X[idx]\n",
        "y_sorted = y[idx]\n",
        "\n",
        "sorted_x = {}\n",
        "sorted_y = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x[\"X_{0}\".format(classes)] = X[y == classes]\n",
        "  sorted_y[\"y_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x[sorted_lists].shape[0]}')\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_0: 818\n",
            "Number of Samples for X_1: 866\n",
            "Number of Samples for X_2: 808\n",
            "Number of Samples for X_3: 774\n",
            "Number of Samples for X_4: 768\n",
            "Number of Samples for X_5: 702\n",
            "Number of Samples for X_6: 735\n",
            "Number of Samples for X_7: 891\n",
            "Number of Samples for X_8: 820\n",
            "Number of Samples for X_9: 818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaNaUO3kP2G"
      },
      "source": [
        "Sorting Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBLFeGAUkSOs",
        "outputId": "70b8395f-b1ee-4644-d9e9-1e019d2c8869"
      },
      "source": [
        "idx = np.argsort(y_test)\n",
        "X_test_sorted = X_test[idx]\n",
        "y_test_sorted = y_test[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_test = {}\n",
        "sorted_y_test = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_test[\"X_test_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "  sorted_y_test[\"y_test_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_test:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_test[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_test[sorted_lists].shape[0])\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_test_0: 137\n",
            "Number of Samples for X_test_1: 185\n",
            "Number of Samples for X_test_2: 180\n",
            "Number of Samples for X_test_3: 162\n",
            "Number of Samples for X_test_4: 181\n",
            "Number of Samples for X_test_5: 142\n",
            "Number of Samples for X_test_6: 139\n",
            "Number of Samples for X_test_7: 165\n",
            "Number of Samples for X_test_8: 154\n",
            "Number of Samples for X_test_9: 155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbnq4TJp1cl",
        "outputId": "edf6f907-7442-4fb6-c293-0d5ead7ef788"
      },
      "source": [
        "print(f'Found {X.shape[0]} images belonging to {len(set(y))} unique classes')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 10 unique classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_Dropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,len(set(y)))\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7,learning_rate=0.005)\n",
        "#optimizer = Optimizer_SGD(learning_rate=0.01)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "c1b049ba-ec7e-46bb-98a9-5933c84cf3eb"
      },
      "source": [
        "epochs = 178\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    dropout1.forward(activation1.output)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        "    \n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.095, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.005000000 validation, acc: 0.103, loss: 2.302 \n",
            "epoch: 1, acc: 0.113, loss: 2.304 (data_loss: 2.302, reg_loss: 0.002), lr: 0.004999998 validation, acc: 0.112, loss: 2.299 \n",
            "epoch: 2, acc: 0.152, loss: 2.300 (data_loss: 2.299, reg_loss: 0.001), lr: 0.004999995 validation, acc: 0.103, loss: 2.282 \n",
            "epoch: 3, acc: 0.111, loss: 2.282 (data_loss: 2.281, reg_loss: 0.001), lr: 0.004999993 validation, acc: 0.175, loss: 2.234 \n",
            "epoch: 4, acc: 0.200, loss: 2.233 (data_loss: 2.232, reg_loss: 0.001), lr: 0.004999990 validation, acc: 0.103, loss: 2.582 \n",
            "epoch: 5, acc: 0.113, loss: 2.565 (data_loss: 2.563, reg_loss: 0.002), lr: 0.004999988 validation, acc: 0.324, loss: 2.154 \n",
            "epoch: 6, acc: 0.348, loss: 2.153 (data_loss: 2.151, reg_loss: 0.002), lr: 0.004999985 validation, acc: 0.106, loss: 2.242 \n",
            "epoch: 7, acc: 0.133, loss: 2.228 (data_loss: 2.225, reg_loss: 0.003), lr: 0.004999983 validation, acc: 0.093, loss: 2.219 \n",
            "epoch: 8, acc: 0.113, loss: 2.200 (data_loss: 2.196, reg_loss: 0.004), lr: 0.004999980 validation, acc: 0.168, loss: 2.160 \n",
            "epoch: 9, acc: 0.177, loss: 2.133 (data_loss: 2.128, reg_loss: 0.005), lr: 0.004999978 validation, acc: 0.188, loss: 2.080 \n",
            "epoch: 10, acc: 0.208, loss: 2.046 (data_loss: 2.040, reg_loss: 0.006), lr: 0.004999975 validation, acc: 0.271, loss: 1.967 \n",
            "epoch: 11, acc: 0.277, loss: 1.937 (data_loss: 1.930, reg_loss: 0.007), lr: 0.004999973 validation, acc: 0.276, loss: 1.880 \n",
            "epoch: 12, acc: 0.280, loss: 1.873 (data_loss: 1.865, reg_loss: 0.009), lr: 0.004999970 validation, acc: 0.247, loss: 1.959 \n",
            "epoch: 13, acc: 0.264, loss: 1.898 (data_loss: 1.888, reg_loss: 0.010), lr: 0.004999968 validation, acc: 0.272, loss: 1.759 \n",
            "epoch: 14, acc: 0.288, loss: 1.724 (data_loss: 1.714, reg_loss: 0.010), lr: 0.004999965 validation, acc: 0.268, loss: 1.759 \n",
            "epoch: 15, acc: 0.285, loss: 1.737 (data_loss: 1.726, reg_loss: 0.011), lr: 0.004999963 validation, acc: 0.374, loss: 1.699 \n",
            "epoch: 16, acc: 0.362, loss: 1.658 (data_loss: 1.646, reg_loss: 0.012), lr: 0.004999960 validation, acc: 0.361, loss: 1.692 \n",
            "epoch: 17, acc: 0.376, loss: 1.640 (data_loss: 1.628, reg_loss: 0.012), lr: 0.004999958 validation, acc: 0.347, loss: 1.666 \n",
            "epoch: 18, acc: 0.371, loss: 1.605 (data_loss: 1.593, reg_loss: 0.013), lr: 0.004999955 validation, acc: 0.406, loss: 1.637 \n",
            "epoch: 19, acc: 0.407, loss: 1.573 (data_loss: 1.560, reg_loss: 0.013), lr: 0.004999953 validation, acc: 0.417, loss: 1.531 \n",
            "epoch: 20, acc: 0.429, loss: 1.475 (data_loss: 1.461, reg_loss: 0.014), lr: 0.004999950 validation, acc: 0.413, loss: 1.483 \n",
            "epoch: 21, acc: 0.427, loss: 1.417 (data_loss: 1.402, reg_loss: 0.015), lr: 0.004999948 validation, acc: 0.478, loss: 1.405 \n",
            "epoch: 22, acc: 0.492, loss: 1.341 (data_loss: 1.326, reg_loss: 0.015), lr: 0.004999945 validation, acc: 0.516, loss: 1.424 \n",
            "epoch: 23, acc: 0.526, loss: 1.368 (data_loss: 1.352, reg_loss: 0.016), lr: 0.004999943 validation, acc: 0.399, loss: 1.499 \n",
            "epoch: 24, acc: 0.435, loss: 1.409 (data_loss: 1.393, reg_loss: 0.017), lr: 0.004999940 validation, acc: 0.542, loss: 1.288 \n",
            "epoch: 25, acc: 0.585, loss: 1.203 (data_loss: 1.186, reg_loss: 0.017), lr: 0.004999938 validation, acc: 0.501, loss: 1.483 \n",
            "epoch: 26, acc: 0.507, loss: 1.450 (data_loss: 1.433, reg_loss: 0.018), lr: 0.004999935 validation, acc: 0.601, loss: 1.205 \n",
            "epoch: 27, acc: 0.618, loss: 1.148 (data_loss: 1.129, reg_loss: 0.018), lr: 0.004999933 validation, acc: 0.531, loss: 1.292 \n",
            "epoch: 28, acc: 0.564, loss: 1.215 (data_loss: 1.196, reg_loss: 0.019), lr: 0.004999930 validation, acc: 0.583, loss: 1.219 \n",
            "epoch: 29, acc: 0.617, loss: 1.135 (data_loss: 1.115, reg_loss: 0.019), lr: 0.004999928 validation, acc: 0.622, loss: 1.171 \n",
            "epoch: 30, acc: 0.615, loss: 1.118 (data_loss: 1.098, reg_loss: 0.020), lr: 0.004999925 validation, acc: 0.658, loss: 1.102 \n",
            "epoch: 31, acc: 0.660, loss: 1.054 (data_loss: 1.034, reg_loss: 0.020), lr: 0.004999923 validation, acc: 0.636, loss: 1.078 \n",
            "epoch: 32, acc: 0.673, loss: 0.997 (data_loss: 0.976, reg_loss: 0.021), lr: 0.004999920 validation, acc: 0.639, loss: 1.072 \n",
            "epoch: 33, acc: 0.677, loss: 0.986 (data_loss: 0.965, reg_loss: 0.022), lr: 0.004999918 validation, acc: 0.686, loss: 1.009 \n",
            "epoch: 34, acc: 0.707, loss: 0.950 (data_loss: 0.928, reg_loss: 0.022), lr: 0.004999915 validation, acc: 0.696, loss: 1.014 \n",
            "epoch: 35, acc: 0.710, loss: 0.943 (data_loss: 0.920, reg_loss: 0.023), lr: 0.004999913 validation, acc: 0.695, loss: 0.958 \n",
            "epoch: 36, acc: 0.716, loss: 0.890 (data_loss: 0.866, reg_loss: 0.023), lr: 0.004999910 validation, acc: 0.706, loss: 0.950 \n",
            "epoch: 37, acc: 0.732, loss: 0.873 (data_loss: 0.849, reg_loss: 0.024), lr: 0.004999908 validation, acc: 0.733, loss: 0.896 \n",
            "epoch: 38, acc: 0.751, loss: 0.848 (data_loss: 0.824, reg_loss: 0.024), lr: 0.004999905 validation, acc: 0.742, loss: 0.888 \n",
            "epoch: 39, acc: 0.758, loss: 0.838 (data_loss: 0.813, reg_loss: 0.025), lr: 0.004999903 validation, acc: 0.750, loss: 0.828 \n",
            "epoch: 40, acc: 0.752, loss: 0.809 (data_loss: 0.784, reg_loss: 0.025), lr: 0.004999900 validation, acc: 0.758, loss: 0.864 \n",
            "epoch: 41, acc: 0.775, loss: 0.798 (data_loss: 0.773, reg_loss: 0.025), lr: 0.004999898 validation, acc: 0.795, loss: 0.761 \n",
            "epoch: 42, acc: 0.803, loss: 0.723 (data_loss: 0.697, reg_loss: 0.026), lr: 0.004999895 validation, acc: 0.815, loss: 0.741 \n",
            "epoch: 43, acc: 0.823, loss: 0.697 (data_loss: 0.672, reg_loss: 0.026), lr: 0.004999893 validation, acc: 0.812, loss: 0.728 \n",
            "epoch: 44, acc: 0.824, loss: 0.673 (data_loss: 0.647, reg_loss: 0.026), lr: 0.004999890 validation, acc: 0.829, loss: 0.685 \n",
            "epoch: 45, acc: 0.835, loss: 0.654 (data_loss: 0.628, reg_loss: 0.026), lr: 0.004999888 validation, acc: 0.824, loss: 0.668 \n",
            "epoch: 46, acc: 0.844, loss: 0.623 (data_loss: 0.597, reg_loss: 0.027), lr: 0.004999885 validation, acc: 0.826, loss: 0.710 \n",
            "epoch: 47, acc: 0.845, loss: 0.627 (data_loss: 0.600, reg_loss: 0.027), lr: 0.004999883 validation, acc: 0.838, loss: 0.635 \n",
            "epoch: 48, acc: 0.865, loss: 0.572 (data_loss: 0.545, reg_loss: 0.027), lr: 0.004999880 validation, acc: 0.841, loss: 0.641 \n",
            "epoch: 49, acc: 0.862, loss: 0.589 (data_loss: 0.561, reg_loss: 0.027), lr: 0.004999878 validation, acc: 0.845, loss: 0.616 \n",
            "epoch: 50, acc: 0.875, loss: 0.542 (data_loss: 0.515, reg_loss: 0.028), lr: 0.004999875 validation, acc: 0.847, loss: 0.628 \n",
            "epoch: 51, acc: 0.876, loss: 0.534 (data_loss: 0.506, reg_loss: 0.028), lr: 0.004999873 validation, acc: 0.855, loss: 0.600 \n",
            "epoch: 52, acc: 0.878, loss: 0.543 (data_loss: 0.515, reg_loss: 0.028), lr: 0.004999870 validation, acc: 0.856, loss: 0.565 \n",
            "epoch: 53, acc: 0.888, loss: 0.503 (data_loss: 0.475, reg_loss: 0.028), lr: 0.004999868 validation, acc: 0.861, loss: 0.558 \n",
            "epoch: 54, acc: 0.887, loss: 0.495 (data_loss: 0.467, reg_loss: 0.028), lr: 0.004999865 validation, acc: 0.871, loss: 0.529 \n",
            "epoch: 55, acc: 0.899, loss: 0.464 (data_loss: 0.436, reg_loss: 0.028), lr: 0.004999863 validation, acc: 0.880, loss: 0.524 \n",
            "epoch: 56, acc: 0.900, loss: 0.462 (data_loss: 0.434, reg_loss: 0.028), lr: 0.004999860 validation, acc: 0.882, loss: 0.509 \n",
            "epoch: 57, acc: 0.907, loss: 0.443 (data_loss: 0.415, reg_loss: 0.028), lr: 0.004999858 validation, acc: 0.876, loss: 0.510 \n",
            "epoch: 58, acc: 0.904, loss: 0.444 (data_loss: 0.416, reg_loss: 0.028), lr: 0.004999855 validation, acc: 0.881, loss: 0.488 \n",
            "epoch: 59, acc: 0.910, loss: 0.423 (data_loss: 0.396, reg_loss: 0.028), lr: 0.004999853 validation, acc: 0.887, loss: 0.492 \n",
            "epoch: 60, acc: 0.910, loss: 0.432 (data_loss: 0.405, reg_loss: 0.027), lr: 0.004999850 validation, acc: 0.882, loss: 0.470 \n",
            "epoch: 61, acc: 0.917, loss: 0.398 (data_loss: 0.371, reg_loss: 0.027), lr: 0.004999848 validation, acc: 0.887, loss: 0.467 \n",
            "epoch: 62, acc: 0.913, loss: 0.398 (data_loss: 0.371, reg_loss: 0.027), lr: 0.004999845 validation, acc: 0.885, loss: 0.464 \n",
            "epoch: 63, acc: 0.919, loss: 0.382 (data_loss: 0.356, reg_loss: 0.027), lr: 0.004999843 validation, acc: 0.885, loss: 0.454 \n",
            "epoch: 64, acc: 0.920, loss: 0.373 (data_loss: 0.346, reg_loss: 0.026), lr: 0.004999840 validation, acc: 0.887, loss: 0.456 \n",
            "epoch: 65, acc: 0.923, loss: 0.369 (data_loss: 0.343, reg_loss: 0.026), lr: 0.004999838 validation, acc: 0.893, loss: 0.430 \n",
            "epoch: 66, acc: 0.927, loss: 0.345 (data_loss: 0.320, reg_loss: 0.026), lr: 0.004999835 validation, acc: 0.892, loss: 0.428 \n",
            "epoch: 67, acc: 0.930, loss: 0.345 (data_loss: 0.320, reg_loss: 0.026), lr: 0.004999833 validation, acc: 0.891, loss: 0.435 \n",
            "epoch: 68, acc: 0.933, loss: 0.339 (data_loss: 0.314, reg_loss: 0.025), lr: 0.004999830 validation, acc: 0.892, loss: 0.421 \n",
            "epoch: 69, acc: 0.928, loss: 0.336 (data_loss: 0.311, reg_loss: 0.025), lr: 0.004999828 validation, acc: 0.891, loss: 0.412 \n",
            "epoch: 70, acc: 0.934, loss: 0.319 (data_loss: 0.294, reg_loss: 0.025), lr: 0.004999825 validation, acc: 0.901, loss: 0.396 \n",
            "epoch: 71, acc: 0.935, loss: 0.314 (data_loss: 0.290, reg_loss: 0.024), lr: 0.004999823 validation, acc: 0.896, loss: 0.393 \n",
            "epoch: 72, acc: 0.935, loss: 0.313 (data_loss: 0.289, reg_loss: 0.024), lr: 0.004999820 validation, acc: 0.894, loss: 0.413 \n",
            "epoch: 73, acc: 0.933, loss: 0.317 (data_loss: 0.294, reg_loss: 0.024), lr: 0.004999818 validation, acc: 0.892, loss: 0.422 \n",
            "epoch: 74, acc: 0.931, loss: 0.332 (data_loss: 0.308, reg_loss: 0.023), lr: 0.004999815 validation, acc: 0.897, loss: 0.391 \n",
            "epoch: 75, acc: 0.939, loss: 0.294 (data_loss: 0.271, reg_loss: 0.023), lr: 0.004999813 validation, acc: 0.894, loss: 0.401 \n",
            "epoch: 76, acc: 0.940, loss: 0.302 (data_loss: 0.280, reg_loss: 0.023), lr: 0.004999810 validation, acc: 0.894, loss: 0.410 \n",
            "epoch: 77, acc: 0.923, loss: 0.330 (data_loss: 0.308, reg_loss: 0.023), lr: 0.004999808 validation, acc: 0.893, loss: 0.394 \n",
            "epoch: 78, acc: 0.939, loss: 0.296 (data_loss: 0.274, reg_loss: 0.022), lr: 0.004999805 validation, acc: 0.901, loss: 0.381 \n",
            "epoch: 79, acc: 0.946, loss: 0.280 (data_loss: 0.258, reg_loss: 0.022), lr: 0.004999803 validation, acc: 0.902, loss: 0.381 \n",
            "epoch: 80, acc: 0.937, loss: 0.295 (data_loss: 0.273, reg_loss: 0.022), lr: 0.004999800 validation, acc: 0.909, loss: 0.351 \n",
            "epoch: 81, acc: 0.947, loss: 0.267 (data_loss: 0.245, reg_loss: 0.022), lr: 0.004999798 validation, acc: 0.900, loss: 0.394 \n",
            "epoch: 82, acc: 0.939, loss: 0.289 (data_loss: 0.267, reg_loss: 0.022), lr: 0.004999795 validation, acc: 0.902, loss: 0.376 \n",
            "epoch: 83, acc: 0.943, loss: 0.270 (data_loss: 0.249, reg_loss: 0.022), lr: 0.004999793 validation, acc: 0.909, loss: 0.354 \n",
            "epoch: 84, acc: 0.950, loss: 0.260 (data_loss: 0.239, reg_loss: 0.022), lr: 0.004999790 validation, acc: 0.906, loss: 0.357 \n",
            "epoch: 85, acc: 0.946, loss: 0.267 (data_loss: 0.245, reg_loss: 0.021), lr: 0.004999788 validation, acc: 0.913, loss: 0.337 \n",
            "epoch: 86, acc: 0.953, loss: 0.246 (data_loss: 0.225, reg_loss: 0.021), lr: 0.004999785 validation, acc: 0.919, loss: 0.337 \n",
            "epoch: 87, acc: 0.950, loss: 0.254 (data_loss: 0.232, reg_loss: 0.021), lr: 0.004999783 validation, acc: 0.914, loss: 0.328 \n",
            "epoch: 88, acc: 0.957, loss: 0.234 (data_loss: 0.213, reg_loss: 0.021), lr: 0.004999780 validation, acc: 0.909, loss: 0.349 \n",
            "epoch: 89, acc: 0.955, loss: 0.238 (data_loss: 0.217, reg_loss: 0.021), lr: 0.004999778 validation, acc: 0.916, loss: 0.331 \n",
            "epoch: 90, acc: 0.957, loss: 0.228 (data_loss: 0.207, reg_loss: 0.021), lr: 0.004999775 validation, acc: 0.913, loss: 0.313 \n",
            "epoch: 91, acc: 0.957, loss: 0.227 (data_loss: 0.206, reg_loss: 0.021), lr: 0.004999773 validation, acc: 0.916, loss: 0.317 \n",
            "epoch: 92, acc: 0.956, loss: 0.233 (data_loss: 0.212, reg_loss: 0.020), lr: 0.004999770 validation, acc: 0.913, loss: 0.323 \n",
            "epoch: 93, acc: 0.958, loss: 0.221 (data_loss: 0.201, reg_loss: 0.020), lr: 0.004999768 validation, acc: 0.916, loss: 0.336 \n",
            "epoch: 94, acc: 0.958, loss: 0.232 (data_loss: 0.212, reg_loss: 0.020), lr: 0.004999765 validation, acc: 0.902, loss: 0.337 \n",
            "epoch: 95, acc: 0.952, loss: 0.239 (data_loss: 0.219, reg_loss: 0.020), lr: 0.004999763 validation, acc: 0.912, loss: 0.349 \n",
            "epoch: 96, acc: 0.947, loss: 0.259 (data_loss: 0.240, reg_loss: 0.020), lr: 0.004999760 validation, acc: 0.888, loss: 0.393 \n",
            "epoch: 97, acc: 0.932, loss: 0.279 (data_loss: 0.260, reg_loss: 0.019), lr: 0.004999758 validation, acc: 0.882, loss: 0.442 \n",
            "epoch: 98, acc: 0.917, loss: 0.343 (data_loss: 0.324, reg_loss: 0.019), lr: 0.004999755 validation, acc: 0.898, loss: 0.345 \n",
            "epoch: 99, acc: 0.946, loss: 0.244 (data_loss: 0.225, reg_loss: 0.019), lr: 0.004999753 validation, acc: 0.894, loss: 0.383 \n",
            "epoch: 100, acc: 0.938, loss: 0.273 (data_loss: 0.254, reg_loss: 0.020), lr: 0.004999750 validation, acc: 0.869, loss: 0.466 \n",
            "epoch: 101, acc: 0.905, loss: 0.370 (data_loss: 0.351, reg_loss: 0.020), lr: 0.004999748 validation, acc: 0.849, loss: 0.546 \n",
            "epoch: 102, acc: 0.888, loss: 0.432 (data_loss: 0.413, reg_loss: 0.020), lr: 0.004999745 validation, acc: 0.836, loss: 0.642 \n",
            "epoch: 103, acc: 0.872, loss: 0.504 (data_loss: 0.484, reg_loss: 0.020), lr: 0.004999743 validation, acc: 0.822, loss: 0.622 \n",
            "epoch: 104, acc: 0.851, loss: 0.511 (data_loss: 0.490, reg_loss: 0.021), lr: 0.004999740 validation, acc: 0.841, loss: 0.602 \n",
            "epoch: 105, acc: 0.860, loss: 0.494 (data_loss: 0.472, reg_loss: 0.021), lr: 0.004999738 validation, acc: 0.858, loss: 0.476 \n",
            "epoch: 106, acc: 0.890, loss: 0.402 (data_loss: 0.380, reg_loss: 0.022), lr: 0.004999735 validation, acc: 0.777, loss: 0.777 \n",
            "epoch: 107, acc: 0.819, loss: 0.645 (data_loss: 0.623, reg_loss: 0.023), lr: 0.004999733 validation, acc: 0.868, loss: 0.460 \n",
            "epoch: 108, acc: 0.899, loss: 0.397 (data_loss: 0.373, reg_loss: 0.023), lr: 0.004999730 validation, acc: 0.853, loss: 0.536 \n",
            "epoch: 109, acc: 0.862, loss: 0.486 (data_loss: 0.462, reg_loss: 0.024), lr: 0.004999728 validation, acc: 0.864, loss: 0.483 \n",
            "epoch: 110, acc: 0.889, loss: 0.422 (data_loss: 0.397, reg_loss: 0.025), lr: 0.004999725 validation, acc: 0.840, loss: 0.573 \n",
            "epoch: 111, acc: 0.873, loss: 0.471 (data_loss: 0.445, reg_loss: 0.026), lr: 0.004999723 validation, acc: 0.881, loss: 0.500 \n",
            "epoch: 112, acc: 0.887, loss: 0.439 (data_loss: 0.412, reg_loss: 0.027), lr: 0.004999720 validation, acc: 0.873, loss: 0.448 \n",
            "epoch: 113, acc: 0.893, loss: 0.407 (data_loss: 0.379, reg_loss: 0.028), lr: 0.004999718 validation, acc: 0.848, loss: 0.524 \n",
            "epoch: 114, acc: 0.870, loss: 0.484 (data_loss: 0.456, reg_loss: 0.029), lr: 0.004999715 validation, acc: 0.885, loss: 0.391 \n",
            "epoch: 115, acc: 0.899, loss: 0.390 (data_loss: 0.361, reg_loss: 0.029), lr: 0.004999713 validation, acc: 0.904, loss: 0.361 \n",
            "epoch: 116, acc: 0.914, loss: 0.358 (data_loss: 0.328, reg_loss: 0.030), lr: 0.004999710 validation, acc: 0.884, loss: 0.437 \n",
            "epoch: 117, acc: 0.906, loss: 0.386 (data_loss: 0.356, reg_loss: 0.031), lr: 0.004999708 validation, acc: 0.889, loss: 0.366 \n",
            "epoch: 118, acc: 0.922, loss: 0.321 (data_loss: 0.290, reg_loss: 0.031), lr: 0.004999705 validation, acc: 0.886, loss: 0.368 \n",
            "epoch: 119, acc: 0.920, loss: 0.331 (data_loss: 0.299, reg_loss: 0.032), lr: 0.004999703 validation, acc: 0.894, loss: 0.370 \n",
            "epoch: 120, acc: 0.921, loss: 0.329 (data_loss: 0.297, reg_loss: 0.032), lr: 0.004999700 validation, acc: 0.902, loss: 0.361 \n",
            "epoch: 121, acc: 0.928, loss: 0.313 (data_loss: 0.281, reg_loss: 0.033), lr: 0.004999698 validation, acc: 0.902, loss: 0.362 \n",
            "epoch: 122, acc: 0.926, loss: 0.311 (data_loss: 0.278, reg_loss: 0.033), lr: 0.004999695 validation, acc: 0.912, loss: 0.338 \n",
            "epoch: 123, acc: 0.934, loss: 0.288 (data_loss: 0.255, reg_loss: 0.033), lr: 0.004999693 validation, acc: 0.912, loss: 0.329 \n",
            "epoch: 124, acc: 0.931, loss: 0.291 (data_loss: 0.258, reg_loss: 0.034), lr: 0.004999690 validation, acc: 0.911, loss: 0.316 \n",
            "epoch: 125, acc: 0.942, loss: 0.272 (data_loss: 0.238, reg_loss: 0.034), lr: 0.004999688 validation, acc: 0.914, loss: 0.328 \n",
            "epoch: 126, acc: 0.940, loss: 0.277 (data_loss: 0.244, reg_loss: 0.034), lr: 0.004999685 validation, acc: 0.909, loss: 0.339 \n",
            "epoch: 127, acc: 0.938, loss: 0.279 (data_loss: 0.245, reg_loss: 0.034), lr: 0.004999683 validation, acc: 0.910, loss: 0.334 \n",
            "epoch: 128, acc: 0.942, loss: 0.266 (data_loss: 0.232, reg_loss: 0.034), lr: 0.004999680 validation, acc: 0.911, loss: 0.323 \n",
            "epoch: 129, acc: 0.947, loss: 0.255 (data_loss: 0.222, reg_loss: 0.034), lr: 0.004999678 validation, acc: 0.910, loss: 0.326 \n",
            "epoch: 130, acc: 0.946, loss: 0.262 (data_loss: 0.229, reg_loss: 0.034), lr: 0.004999675 validation, acc: 0.912, loss: 0.313 \n",
            "epoch: 131, acc: 0.947, loss: 0.254 (data_loss: 0.221, reg_loss: 0.033), lr: 0.004999673 validation, acc: 0.919, loss: 0.303 \n",
            "epoch: 132, acc: 0.954, loss: 0.237 (data_loss: 0.204, reg_loss: 0.033), lr: 0.004999670 validation, acc: 0.921, loss: 0.305 \n",
            "epoch: 133, acc: 0.953, loss: 0.238 (data_loss: 0.205, reg_loss: 0.033), lr: 0.004999668 validation, acc: 0.921, loss: 0.300 \n",
            "epoch: 134, acc: 0.952, loss: 0.242 (data_loss: 0.209, reg_loss: 0.033), lr: 0.004999665 validation, acc: 0.921, loss: 0.295 \n",
            "epoch: 135, acc: 0.954, loss: 0.231 (data_loss: 0.199, reg_loss: 0.032), lr: 0.004999663 validation, acc: 0.921, loss: 0.294 \n",
            "epoch: 136, acc: 0.957, loss: 0.223 (data_loss: 0.191, reg_loss: 0.032), lr: 0.004999660 validation, acc: 0.923, loss: 0.293 \n",
            "epoch: 137, acc: 0.956, loss: 0.224 (data_loss: 0.192, reg_loss: 0.032), lr: 0.004999658 validation, acc: 0.919, loss: 0.290 \n",
            "epoch: 138, acc: 0.958, loss: 0.226 (data_loss: 0.195, reg_loss: 0.031), lr: 0.004999655 validation, acc: 0.919, loss: 0.295 \n",
            "epoch: 139, acc: 0.958, loss: 0.221 (data_loss: 0.190, reg_loss: 0.031), lr: 0.004999653 validation, acc: 0.918, loss: 0.298 \n",
            "epoch: 140, acc: 0.956, loss: 0.224 (data_loss: 0.193, reg_loss: 0.031), lr: 0.004999650 validation, acc: 0.921, loss: 0.292 \n",
            "epoch: 141, acc: 0.962, loss: 0.208 (data_loss: 0.177, reg_loss: 0.030), lr: 0.004999648 validation, acc: 0.921, loss: 0.287 \n",
            "epoch: 142, acc: 0.962, loss: 0.204 (data_loss: 0.174, reg_loss: 0.030), lr: 0.004999645 validation, acc: 0.928, loss: 0.283 \n",
            "epoch: 143, acc: 0.962, loss: 0.208 (data_loss: 0.179, reg_loss: 0.029), lr: 0.004999643 validation, acc: 0.923, loss: 0.288 \n",
            "epoch: 144, acc: 0.963, loss: 0.206 (data_loss: 0.177, reg_loss: 0.029), lr: 0.004999640 validation, acc: 0.924, loss: 0.284 \n",
            "epoch: 145, acc: 0.962, loss: 0.203 (data_loss: 0.174, reg_loss: 0.029), lr: 0.004999638 validation, acc: 0.924, loss: 0.281 \n",
            "epoch: 146, acc: 0.963, loss: 0.199 (data_loss: 0.171, reg_loss: 0.028), lr: 0.004999635 validation, acc: 0.924, loss: 0.280 \n",
            "epoch: 147, acc: 0.966, loss: 0.195 (data_loss: 0.168, reg_loss: 0.028), lr: 0.004999633 validation, acc: 0.927, loss: 0.277 \n",
            "epoch: 148, acc: 0.963, loss: 0.195 (data_loss: 0.168, reg_loss: 0.027), lr: 0.004999630 validation, acc: 0.931, loss: 0.268 \n",
            "epoch: 149, acc: 0.964, loss: 0.192 (data_loss: 0.165, reg_loss: 0.027), lr: 0.004999628 validation, acc: 0.927, loss: 0.267 \n",
            "epoch: 150, acc: 0.967, loss: 0.190 (data_loss: 0.164, reg_loss: 0.026), lr: 0.004999625 validation, acc: 0.930, loss: 0.266 \n",
            "epoch: 151, acc: 0.969, loss: 0.181 (data_loss: 0.155, reg_loss: 0.026), lr: 0.004999623 validation, acc: 0.927, loss: 0.263 \n",
            "epoch: 152, acc: 0.967, loss: 0.184 (data_loss: 0.158, reg_loss: 0.026), lr: 0.004999620 validation, acc: 0.929, loss: 0.266 \n",
            "epoch: 153, acc: 0.972, loss: 0.174 (data_loss: 0.149, reg_loss: 0.025), lr: 0.004999618 validation, acc: 0.929, loss: 0.278 \n",
            "epoch: 154, acc: 0.969, loss: 0.178 (data_loss: 0.153, reg_loss: 0.025), lr: 0.004999615 validation, acc: 0.927, loss: 0.274 \n",
            "epoch: 155, acc: 0.972, loss: 0.171 (data_loss: 0.146, reg_loss: 0.024), lr: 0.004999613 validation, acc: 0.929, loss: 0.258 \n",
            "epoch: 156, acc: 0.971, loss: 0.174 (data_loss: 0.150, reg_loss: 0.024), lr: 0.004999610 validation, acc: 0.929, loss: 0.254 \n",
            "epoch: 157, acc: 0.971, loss: 0.170 (data_loss: 0.147, reg_loss: 0.024), lr: 0.004999608 validation, acc: 0.929, loss: 0.259 \n",
            "epoch: 158, acc: 0.968, loss: 0.175 (data_loss: 0.151, reg_loss: 0.023), lr: 0.004999605 validation, acc: 0.931, loss: 0.262 \n",
            "epoch: 159, acc: 0.970, loss: 0.173 (data_loss: 0.150, reg_loss: 0.023), lr: 0.004999603 validation, acc: 0.930, loss: 0.267 \n",
            "epoch: 160, acc: 0.971, loss: 0.168 (data_loss: 0.145, reg_loss: 0.023), lr: 0.004999600 validation, acc: 0.926, loss: 0.277 \n",
            "epoch: 161, acc: 0.970, loss: 0.172 (data_loss: 0.149, reg_loss: 0.022), lr: 0.004999598 validation, acc: 0.929, loss: 0.265 \n",
            "epoch: 162, acc: 0.971, loss: 0.168 (data_loss: 0.145, reg_loss: 0.022), lr: 0.004999595 validation, acc: 0.931, loss: 0.262 \n",
            "epoch: 163, acc: 0.971, loss: 0.170 (data_loss: 0.149, reg_loss: 0.022), lr: 0.004999593 validation, acc: 0.927, loss: 0.260 \n",
            "epoch: 164, acc: 0.974, loss: 0.158 (data_loss: 0.136, reg_loss: 0.021), lr: 0.004999590 validation, acc: 0.929, loss: 0.258 \n",
            "epoch: 165, acc: 0.973, loss: 0.161 (data_loss: 0.140, reg_loss: 0.021), lr: 0.004999588 validation, acc: 0.929, loss: 0.265 \n",
            "epoch: 166, acc: 0.974, loss: 0.159 (data_loss: 0.138, reg_loss: 0.021), lr: 0.004999585 validation, acc: 0.924, loss: 0.278 \n",
            "epoch: 167, acc: 0.973, loss: 0.156 (data_loss: 0.135, reg_loss: 0.021), lr: 0.004999583 validation, acc: 0.927, loss: 0.258 \n",
            "epoch: 168, acc: 0.974, loss: 0.155 (data_loss: 0.134, reg_loss: 0.020), lr: 0.004999580 validation, acc: 0.928, loss: 0.257 \n",
            "epoch: 169, acc: 0.974, loss: 0.152 (data_loss: 0.132, reg_loss: 0.020), lr: 0.004999578 validation, acc: 0.926, loss: 0.263 \n",
            "epoch: 170, acc: 0.975, loss: 0.154 (data_loss: 0.134, reg_loss: 0.020), lr: 0.004999575 validation, acc: 0.926, loss: 0.269 \n",
            "epoch: 171, acc: 0.977, loss: 0.147 (data_loss: 0.128, reg_loss: 0.020), lr: 0.004999573 validation, acc: 0.928, loss: 0.255 \n",
            "epoch: 172, acc: 0.975, loss: 0.150 (data_loss: 0.131, reg_loss: 0.019), lr: 0.004999570 validation, acc: 0.927, loss: 0.255 \n",
            "epoch: 173, acc: 0.975, loss: 0.144 (data_loss: 0.124, reg_loss: 0.019), lr: 0.004999568 validation, acc: 0.926, loss: 0.265 \n",
            "epoch: 174, acc: 0.976, loss: 0.145 (data_loss: 0.126, reg_loss: 0.019), lr: 0.004999565 validation, acc: 0.926, loss: 0.263 \n",
            "epoch: 175, acc: 0.976, loss: 0.146 (data_loss: 0.128, reg_loss: 0.019), lr: 0.004999563 validation, acc: 0.926, loss: 0.266 \n",
            "epoch: 176, acc: 0.975, loss: 0.146 (data_loss: 0.128, reg_loss: 0.019), lr: 0.004999560 validation, acc: 0.931, loss: 0.266 \n",
            "epoch: 177, acc: 0.977, loss: 0.141 (data_loss: 0.123, reg_loss: 0.018), lr: 0.004999558 validation, acc: 0.931, loss: 0.255 \n",
            "epoch: 178, acc: 0.980, loss: 0.134 (data_loss: 0.116, reg_loss: 0.018), lr: 0.004999555 validation, acc: 0.934, loss: 0.251 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNnDUP_U8Xn",
        "outputId": "c9727849-9aec-420e-dcc7-7196056a876d"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8157716480446927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbXMisqQKqF",
        "outputId": "6b8d7eb6-8456-4024-f511-c7f841781529"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 42 epochs\n",
            "Model hit 85% validation accuracy in 51 epochs\n",
            "Model hit 90% validation accuracy in 70 epochs\n",
            "Max accuracy was 93.4375% at epoch 178.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwSXsZVU8Xo",
        "outputId": "5aa7c252-acc3-4cc9-8cec-defb450102d4"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.934, loss: 0.251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k0Ve2M0bPG3"
      },
      "source": [
        "training_diff = []\n",
        "testing_diff = []\n",
        "combined_diff = []"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByL_RwvlIx3"
      },
      "source": [
        "Individual Training Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTOnqnDXa0ME",
        "outputId": "c7cde700-a564-4246-dc31-bd36d3dc9e1d"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x, sorted_y)):\n",
        "  accuracy = Accuracy_Categorical()\n",
        "\n",
        "  y = sorted_y[y_sorted_lists]\n",
        "  X = sorted_x[X_sorted_lists]\n",
        "  accuracy.init(y)\n",
        "\n",
        "  dense1.forward(X)\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "  train_train_mean = activation1.output\n",
        "\n",
        "  dense2.forward(activation1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, y)\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, y)\n",
        "  print(f'{number_mnist_labels[classes]} Train Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Train Accuracy: 0.993, loss: 0.050\n",
            "1 Train Accuracy: 0.986, loss: 0.092\n",
            "2 Train Accuracy: 0.984, loss: 0.105\n",
            "3 Train Accuracy: 0.992, loss: 0.057\n",
            "4 Train Accuracy: 0.993, loss: 0.067\n",
            "5 Train Accuracy: 0.979, loss: 0.174\n",
            "6 Train Accuracy: 0.995, loss: 0.041\n",
            "7 Train Accuracy: 0.993, loss: 0.063\n",
            "8 Train Accuracy: 0.984, loss: 0.114\n",
            "9 Train Accuracy: 0.980, loss: 0.133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scjb7Wh_sn6b",
        "outputId": "d7a03850-4914-465d-c90a-1abc7fc8b48b"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x_test, sorted_y_test)):\n",
        "  accuracy.init(y_sorted_lists)\n",
        "  #print(sorted_y[y_sorted_lists].shape)\n",
        "  #print(sorted_x[X_sorted_lists].shape)\n",
        "  dense1.forward(sorted_x_test[X_sorted_lists])\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  testmean = np.mean(activation1.output, axis=0)\n",
        "  testing_diff.append(testmean)\n",
        "  dense2.forward(activation1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  print(f'{number_mnist_labels[classes]} Test Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Test Accuracy: 0.985, loss: 0.053\n",
            "1 Test Accuracy: 0.984, loss: 0.073\n",
            "2 Test Accuracy: 0.950, loss: 0.221\n",
            "3 Test Accuracy: 0.957, loss: 0.120\n",
            "4 Test Accuracy: 0.895, loss: 0.346\n",
            "5 Test Accuracy: 0.908, loss: 0.359\n",
            "6 Test Accuracy: 0.942, loss: 0.268\n",
            "7 Test Accuracy: 0.891, loss: 0.391\n",
            "8 Test Accuracy: 0.916, loss: 0.339\n",
            "9 Test Accuracy: 0.916, loss: 0.346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u-O8oNZ0qA"
      },
      "source": [
        "# Full mnist test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbD4KrLMnTcR"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMfBGUHeZ4L5",
        "outputId": "367da8a8-b090-42b9-e388-3ab4632f3e28"
      },
      "source": [
        "(input, label), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "number_mnist_labels = {\n",
        "    0: '0',\n",
        "    1: '1',\n",
        "    2: '2',\n",
        "    3: '3',\n",
        "    4: '4',\n",
        "    5: '5',\n",
        "    6: '6',\n",
        "    7: '7',\n",
        "    8: '8',\n",
        "    9: '9'\n",
        "}\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(input.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "input = input[keys]\n",
        "label = label[keys]\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "input = (input.reshape(input.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(label)\n",
        "\n",
        "dense1.forward(input)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, label)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, label)\n",
        "\n",
        "print(f'Full Training Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full Training Accuracy: 0.948, loss: 0.214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aat-uVF6nYu7"
      },
      "source": [
        "Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hyU1tBDna8x",
        "outputId": "73c42557-317a-4d1c-833a-118484e82641"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Full Testing Accuracy: {testaccuracy:.5f}, loss: {loss:.3f}')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full Testing Accuracy: 0.94410, loss: 0.220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0deRpPodLm04"
      },
      "source": [
        "Change idex to get confidence of different samples of testing data. Index values 0-1600 were refrenced in training. Anything past was never seen during training. Lowest confidence is at index 5046 when trained with 178 epochs and numpy seed set to 22."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "DdktRypGH2Gw",
        "outputId": "b002c9c8-77ab-4335-e36c-e542073ea13e"
      },
      "source": [
        "index = 5046\n",
        "\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {number_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {number_mnist_labels[y_test[index]]}')\n",
        "\n",
        "X_test.resize(X_test.shape[0],28,28)\n",
        "image = X_test[index]\n",
        "fig = plt.figure\n",
        "plt.title(f'{number_mnist_labels[y_test[index]]}')\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24.120% Confident True is 8. True is actually 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmoESVkskwUs",
        "outputId": "b689582f-92ca-4230-dc26-d9051f1053dc"
      },
      "source": [
        "confidence_list = []\n",
        "for index in range(1600,10000):\n",
        "  confidence_list.append(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0])\n",
        "\n",
        "print(confidence_list.index(min(confidence_list))+1600)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "h5c5xUTNXk2v",
        "outputId": "97fff54e-e01a-4fa1-c048-9de126d30ed8"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}