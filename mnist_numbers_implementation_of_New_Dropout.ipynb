{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist numbers implementation of New_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_numbers_implementation_of_New_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYgI3SFHqm4"
      },
      "source": [
        "# MNIST Numbers Implementation of My New Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HD9nTuEVHD"
      },
      "source": [
        "np.random.seed(seed=22) #Random seed used for comparison between old dropout"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cag8ZraxEZbF",
        "outputId": "33966d4e-8d1f-4ceb-b938-5b057e66aff2"
      },
      "source": [
        "print(np.random.random(size=3)) #Check that seeds line up"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20846054 0.48168106 0.42053804]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers (Credit to Harrison Kinsley & Daniel Kukiela for raw python implementation)\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Og9ZrbKtV"
      },
      "source": [
        "# New Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXBWsHDIUSfh"
      },
      "source": [
        "class Layer_BinaryNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = 1 - rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, inputs, val_inputs):\n",
        "        self.inputs = inputs\n",
        "        self.val_inputs = val_inputs\n",
        "        nummask = round(len(self.inputs[0]) * self.rate)\n",
        "        \n",
        "        #Averaging Values\n",
        "        self.meanarray1 = np.mean(inputs, axis=0)\n",
        "        self.meanarray2 = np.mean(val_inputs, axis=0)\n",
        "\n",
        "        if self.iterations != 0:\n",
        "            # Calculating value\n",
        "            self.difference = self.meanarray1 - self.meanarray2\n",
        "            ind = np.argpartition(self.difference, -nummask)[-nummask:]\n",
        "            mask = np.ones(self.meanarray1.shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            self.difference[~mask] = 1\n",
        "            self.difference[mask] = 0. \n",
        "            self.binary_mask = self.difference / self.rate\n",
        "\n",
        "        else:\n",
        "            self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                                  size=inputs.shape) / self.rate\n",
        "        self.output = inputs * self.binary_mask\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiuCzwWxbRl0"
      },
      "source": [
        "class Layer_CatagoricalNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, X_test, y_test, X, y):        \n",
        "        if self.iterations != 0:\n",
        "          #Sorting data into classes\n",
        "          idx = np.argsort(y_test)\n",
        "          X_test_sorted = X_test[idx]\n",
        "          y_test_sorted = y_test[idx]\n",
        "\n",
        "          idx2 = np.argsort(y)\n",
        "          X_train_sorted = X[idx2]\n",
        "          y_train_sorted = y[idx2]\n",
        "\n",
        "          #Adding sorted data into dictionaries \n",
        "          sorted_x = {}\n",
        "          sorted_y = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x[\"class_{0}\".format(classes)] = X[y == classes]\n",
        "            sorted_y[\"label_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "          sorted_x_test = {}\n",
        "          sorted_y_test = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x_test[\"class_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "            sorted_y_test[\"label_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "          #Averaging sorted data from each class then finding the difference between the averaged train and test inputs\n",
        "          differnce_classes = {}\n",
        "          for i, classes, test_classes in zip(range(len(set(y))), sorted_x, sorted_x_test):\n",
        "            differnce_classes[\"diff_{0}\".format(i)] = np.mean(sorted_x[classes], axis=0) - np.mean(sorted_x_test[classes], axis=0)\n",
        "\n",
        "          #Masking the data taking the high values(greatest difference between train and test) and setting their values to 0\n",
        "          self.diff_mask = {}\n",
        "          for i, classes, test_classes, diff in zip(range(len(set(y))), sorted_x, sorted_x_test, differnce_classes):\n",
        "            ind = np.argpartition(differnce_classes[diff], -round(len(X[0]) * self.rate))[-round(len(X[0]) * self.rate):]\n",
        "            mask = np.ones(np.mean(sorted_x[classes],axis=0).shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            differnce_classes[diff][~mask] = 0.\n",
        "            differnce_classes[diff][mask] = 1\n",
        "            self.diff_mask[\"mask_{0}\".format(i)] = differnce_classes[diff]\n",
        "\n",
        "          #Goes through each input values and applies the apprioprite mask based on what the true output should be.\n",
        "          binary_mask = np.empty(shape=X.shape)\n",
        "          for i, input, label in zip(range(len(X)), X, y):\n",
        "            for true, diff in enumerate(self.diff_mask):\n",
        "              if label == true:\n",
        "                self.binary_mask[i] = self.diff_mask[diff]\n",
        "        else:\n",
        "          self.binary_mask = np.random.binomial(1, (1-self.rate), size=X.shape)\n",
        "        \n",
        "        self.output = (self.binary_mask/(1-self.rate)) * X\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def infrence(self, input, label):\n",
        "        self.input = input\n",
        "        self.label = label\n",
        "        idx = np.argsort(self.label)\n",
        "        input_sorted = input[idx]\n",
        "        label_sorted = label[idx]\n",
        "        self.infrence_binary_mask = np.empty(shape=self.input.shape)\n",
        "        for i, (input, label) in enumerate(zip(self.input, self.label)):\n",
        "          #for true, diff in zip(range(len(set(self.label))),self.diff_mask):\n",
        "          for true, diff in enumerate(self.diff_mask):\n",
        "            if label == true:\n",
        "              self.infrence_binary_mask[i] = self.diff_mask[diff]\n",
        "\n",
        "        self.output = self.infrence_binary_mask * self.input\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "b5049936-8be2-4963-c00e-3ee44bbda91b"
      },
      "source": [
        "#(X, y), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "# load dataset\n",
        "(X, y), (X_val, y_val) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Label index to label name relation\n",
        "number_mnist_labels = {\n",
        "    0: '0',\n",
        "    1: '1',\n",
        "    2: '2',\n",
        "    3: '3',\n",
        "    4: '4',\n",
        "    5: '5',\n",
        "    6: '6',\n",
        "    7: '7',\n",
        "    8: '8',\n",
        "    9: '9'\n",
        "}\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "input = X\n",
        "label = y\n",
        "\n",
        "X = X[:10000,:,:]\n",
        "#X_test = X_test[:1600,:,:]\n",
        "y = y[:10000]\n",
        "#y_test  = y_test[:1600]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_train = (X_train.reshape(X_train.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_val = (X_val.reshape(X_val.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "input = (input.reshape(input.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 784)\n",
            "(8000,)\n",
            "(2000, 784)\n",
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nW7mqGTnem"
      },
      "source": [
        "Sorting Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtvA9y81TpGf",
        "outputId": "53e9b9ae-a6c2-47d3-b1f9-ec44c745d3b8"
      },
      "source": [
        "idx = np.argsort(y_train)\n",
        "X_sorted = X_train[idx]\n",
        "y_sorted = y_train[idx]\n",
        "\n",
        "sorted_x = {}\n",
        "sorted_y = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x[\"X_{0}\".format(classes)] = X_train[y_train == classes]\n",
        "  sorted_y[\"y_{0}\".format(classes)] = y_train[y_train == classes]\n",
        "\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x[sorted_lists].shape[0]}')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_0: 804\n",
            "Number of Samples for X_1: 900\n",
            "Number of Samples for X_2: 816\n",
            "Number of Samples for X_3: 762\n",
            "Number of Samples for X_4: 755\n",
            "Number of Samples for X_5: 704\n",
            "Number of Samples for X_6: 757\n",
            "Number of Samples for X_7: 882\n",
            "Number of Samples for X_8: 798\n",
            "Number of Samples for X_9: 822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaNaUO3kP2G"
      },
      "source": [
        "Sorting Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBLFeGAUkSOs",
        "outputId": "4968792e-e13f-48c4-f14e-48901821c046"
      },
      "source": [
        "idx = np.argsort(y_test)\n",
        "X_test_sorted = X_test[idx]\n",
        "y_test_sorted = y_test[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_test = {}\n",
        "sorted_y_test = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_test[\"X_test_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "  sorted_y_test[\"y_test_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_test:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_test[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_test[sorted_lists].shape[0])\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_test_0: 215\n",
            "Number of Samples for X_test_1: 225\n",
            "Number of Samples for X_test_2: 194\n",
            "Number of Samples for X_test_3: 173\n",
            "Number of Samples for X_test_4: 213\n",
            "Number of Samples for X_test_5: 174\n",
            "Number of Samples for X_test_6: 180\n",
            "Number of Samples for X_test_7: 205\n",
            "Number of Samples for X_test_8: 201\n",
            "Number of Samples for X_test_9: 220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icDwD38AwpUc",
        "outputId": "70b6864f-81e7-4a2e-c427-f6240315d5a5"
      },
      "source": [
        "idx = np.argsort(y_val)\n",
        "X_val_sorted = X_val[idx]\n",
        "y_val_sorted = y_val[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_val = {}\n",
        "sorted_y_val = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_val[\"X_val_{0}\".format(classes)] = X_val[y_val == classes]\n",
        "  sorted_y_val[\"y_val_{0}\".format(classes)] = y_val[y_val == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_val:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_val[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_val[sorted_lists].shape[0])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_val_0: 980\n",
            "Number of Samples for X_val_1: 1135\n",
            "Number of Samples for X_val_2: 1032\n",
            "Number of Samples for X_val_3: 1010\n",
            "Number of Samples for X_val_4: 982\n",
            "Number of Samples for X_val_5: 892\n",
            "Number of Samples for X_val_6: 958\n",
            "Number of Samples for X_val_7: 1028\n",
            "Number of Samples for X_val_8: 974\n",
            "Number of Samples for X_val_9: 1009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbnq4TJp1cl",
        "outputId": "91d9df52-b090-4109-c32e-7611007adf7a"
      },
      "source": [
        "print(f'Found {X.shape[0]} images belonging to {len(set(y))} unique classes')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 10 unique classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_CatagoricalNSDropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,len(set(y)))\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7,learning_rate=0.005)\n",
        "#optimizer = Optimizer_SGD(learning_rate=0.01)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "11fd2134-01ab-4d98-ea92-87a61caf3e63"
      },
      "source": [
        "epochs = 359\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X_train)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    if epoch != 0:\n",
        "      cached_val_inputs = cached_val_inputs\n",
        "      cached_train_inputs = activation1.output\n",
        "    else:\n",
        "      cached_val_inputs = np.random.random(size=128) #Never used just needed to pass to dropout\n",
        "      cached_train_inputs = activation1.output\n",
        "\n",
        "\n",
        "    dropout1.forward(X=activation1.output, y=y_train, X_test=cached_val_inputs, y_test=y_test)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y_train)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y_train)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "    dropout1.post_update_params()\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    if epoch == 0:\n",
        "      dense2.forward(activation1.output)\n",
        "    else:\n",
        "      dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "      dense2.forward(dropout1.output)\n",
        "    \n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        "    \n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.085, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.005000000 validation, acc: 0.113, loss: 2.302 \n",
            "epoch: 1, acc: 0.113, loss: 2.304 (data_loss: 2.302, reg_loss: 0.002), lr: 0.004999998 validation, acc: 0.215, loss: 2.298 \n",
            "epoch: 2, acc: 0.223, loss: 2.300 (data_loss: 2.299, reg_loss: 0.001), lr: 0.004999995 validation, acc: 0.215, loss: 2.271 \n",
            "epoch: 3, acc: 0.223, loss: 2.276 (data_loss: 2.275, reg_loss: 0.001), lr: 0.004999993 validation, acc: 0.215, loss: 2.187 \n",
            "epoch: 4, acc: 0.223, loss: 2.162 (data_loss: 2.160, reg_loss: 0.001), lr: 0.004999990 validation, acc: 0.215, loss: 1.951 \n",
            "epoch: 5, acc: 0.223, loss: 1.909 (data_loss: 1.906, reg_loss: 0.003), lr: 0.004999988 validation, acc: 0.215, loss: 1.697 \n",
            "epoch: 6, acc: 0.223, loss: 1.685 (data_loss: 1.681, reg_loss: 0.004), lr: 0.004999985 validation, acc: 0.389, loss: 1.685 \n",
            "epoch: 7, acc: 0.326, loss: 1.685 (data_loss: 1.680, reg_loss: 0.005), lr: 0.004999983 validation, acc: 0.373, loss: 1.654 \n",
            "epoch: 8, acc: 0.416, loss: 1.590 (data_loss: 1.584, reg_loss: 0.006), lr: 0.004999980 validation, acc: 0.416, loss: 1.618 \n",
            "epoch: 9, acc: 0.353, loss: 1.564 (data_loss: 1.558, reg_loss: 0.006), lr: 0.004999978 validation, acc: 0.617, loss: 1.474 \n",
            "epoch: 10, acc: 0.529, loss: 1.507 (data_loss: 1.500, reg_loss: 0.007), lr: 0.004999975 validation, acc: 0.652, loss: 1.331 \n",
            "epoch: 11, acc: 0.605, loss: 1.325 (data_loss: 1.317, reg_loss: 0.008), lr: 0.004999973 validation, acc: 0.603, loss: 1.181 \n",
            "epoch: 12, acc: 0.585, loss: 1.162 (data_loss: 1.153, reg_loss: 0.009), lr: 0.004999970 validation, acc: 0.678, loss: 1.047 \n",
            "epoch: 13, acc: 0.677, loss: 1.028 (data_loss: 1.019, reg_loss: 0.010), lr: 0.004999968 validation, acc: 0.722, loss: 0.930 \n",
            "epoch: 14, acc: 0.590, loss: 0.971 (data_loss: 0.960, reg_loss: 0.011), lr: 0.004999965 validation, acc: 0.772, loss: 0.774 \n",
            "epoch: 15, acc: 0.667, loss: 0.844 (data_loss: 0.831, reg_loss: 0.012), lr: 0.004999963 validation, acc: 0.871, loss: 0.634 \n",
            "epoch: 16, acc: 0.881, loss: 0.615 (data_loss: 0.601, reg_loss: 0.014), lr: 0.004999960 validation, acc: 0.841, loss: 0.484 \n",
            "epoch: 17, acc: 0.774, loss: 0.610 (data_loss: 0.595, reg_loss: 0.015), lr: 0.004999958 validation, acc: 0.706, loss: 1.047 \n",
            "epoch: 18, acc: 0.703, loss: 1.138 (data_loss: 1.121, reg_loss: 0.016), lr: 0.004999955 validation, acc: 0.761, loss: 0.567 \n",
            "epoch: 19, acc: 0.834, loss: 0.508 (data_loss: 0.491, reg_loss: 0.017), lr: 0.004999953 validation, acc: 0.817, loss: 0.606 \n",
            "epoch: 20, acc: 0.808, loss: 0.624 (data_loss: 0.606, reg_loss: 0.018), lr: 0.004999950 validation, acc: 0.919, loss: 0.355 \n",
            "epoch: 21, acc: 0.894, loss: 0.389 (data_loss: 0.371, reg_loss: 0.019), lr: 0.004999948 validation, acc: 0.947, loss: 0.258 \n",
            "epoch: 22, acc: 0.929, loss: 0.249 (data_loss: 0.230, reg_loss: 0.019), lr: 0.004999945 validation, acc: 0.930, loss: 0.210 \n",
            "epoch: 23, acc: 0.929, loss: 0.217 (data_loss: 0.197, reg_loss: 0.020), lr: 0.004999943 validation, acc: 0.911, loss: 0.236 \n",
            "epoch: 24, acc: 0.981, loss: 0.139 (data_loss: 0.118, reg_loss: 0.020), lr: 0.004999940 validation, acc: 0.986, loss: 0.136 \n",
            "epoch: 25, acc: 0.980, loss: 0.141 (data_loss: 0.120, reg_loss: 0.021), lr: 0.004999938 validation, acc: 0.979, loss: 0.138 \n",
            "epoch: 26, acc: 0.869, loss: 0.357 (data_loss: 0.336, reg_loss: 0.021), lr: 0.004999935 validation, acc: 0.756, loss: 0.654 \n",
            "epoch: 27, acc: 0.747, loss: 0.758 (data_loss: 0.737, reg_loss: 0.022), lr: 0.004999933 validation, acc: 0.902, loss: 0.267 \n",
            "epoch: 28, acc: 0.883, loss: 0.311 (data_loss: 0.289, reg_loss: 0.022), lr: 0.004999930 validation, acc: 0.863, loss: 0.364 \n",
            "epoch: 29, acc: 0.813, loss: 0.473 (data_loss: 0.451, reg_loss: 0.023), lr: 0.004999928 validation, acc: 0.892, loss: 0.314 \n",
            "epoch: 30, acc: 0.768, loss: 0.710 (data_loss: 0.687, reg_loss: 0.023), lr: 0.004999925 validation, acc: 0.898, loss: 0.366 \n",
            "epoch: 31, acc: 0.818, loss: 0.555 (data_loss: 0.531, reg_loss: 0.024), lr: 0.004999923 validation, acc: 0.910, loss: 0.279 \n",
            "epoch: 32, acc: 0.860, loss: 0.603 (data_loss: 0.579, reg_loss: 0.024), lr: 0.004999920 validation, acc: 0.841, loss: 0.404 \n",
            "epoch: 33, acc: 0.796, loss: 0.530 (data_loss: 0.505, reg_loss: 0.025), lr: 0.004999918 validation, acc: 0.875, loss: 0.328 \n",
            "epoch: 34, acc: 0.793, loss: 0.507 (data_loss: 0.481, reg_loss: 0.026), lr: 0.004999915 validation, acc: 0.895, loss: 0.309 \n",
            "epoch: 35, acc: 0.909, loss: 0.330 (data_loss: 0.304, reg_loss: 0.026), lr: 0.004999913 validation, acc: 0.907, loss: 0.315 \n",
            "epoch: 36, acc: 0.821, loss: 0.541 (data_loss: 0.514, reg_loss: 0.027), lr: 0.004999910 validation, acc: 0.949, loss: 0.216 \n",
            "epoch: 37, acc: 0.928, loss: 0.293 (data_loss: 0.265, reg_loss: 0.028), lr: 0.004999908 validation, acc: 0.941, loss: 0.173 \n",
            "epoch: 38, acc: 0.949, loss: 0.187 (data_loss: 0.158, reg_loss: 0.029), lr: 0.004999905 validation, acc: 0.929, loss: 0.214 \n",
            "epoch: 39, acc: 0.932, loss: 0.255 (data_loss: 0.225, reg_loss: 0.029), lr: 0.004999903 validation, acc: 0.950, loss: 0.171 \n",
            "epoch: 40, acc: 0.943, loss: 0.248 (data_loss: 0.218, reg_loss: 0.030), lr: 0.004999900 validation, acc: 0.930, loss: 0.260 \n",
            "epoch: 41, acc: 0.933, loss: 0.257 (data_loss: 0.226, reg_loss: 0.031), lr: 0.004999898 validation, acc: 0.956, loss: 0.191 \n",
            "epoch: 42, acc: 0.876, loss: 0.337 (data_loss: 0.305, reg_loss: 0.031), lr: 0.004999895 validation, acc: 0.889, loss: 0.297 \n",
            "epoch: 43, acc: 0.891, loss: 0.327 (data_loss: 0.295, reg_loss: 0.032), lr: 0.004999893 validation, acc: 0.915, loss: 0.232 \n",
            "epoch: 44, acc: 0.924, loss: 0.263 (data_loss: 0.231, reg_loss: 0.033), lr: 0.004999890 validation, acc: 0.940, loss: 0.153 \n",
            "epoch: 45, acc: 0.928, loss: 0.242 (data_loss: 0.209, reg_loss: 0.033), lr: 0.004999888 validation, acc: 0.965, loss: 0.129 \n",
            "epoch: 46, acc: 0.958, loss: 0.166 (data_loss: 0.133, reg_loss: 0.034), lr: 0.004999885 validation, acc: 0.914, loss: 0.204 \n",
            "epoch: 47, acc: 0.911, loss: 0.270 (data_loss: 0.236, reg_loss: 0.034), lr: 0.004999883 validation, acc: 0.945, loss: 0.153 \n",
            "epoch: 48, acc: 0.945, loss: 0.183 (data_loss: 0.149, reg_loss: 0.034), lr: 0.004999880 validation, acc: 0.950, loss: 0.137 \n",
            "epoch: 49, acc: 0.909, loss: 0.276 (data_loss: 0.242, reg_loss: 0.035), lr: 0.004999878 validation, acc: 0.932, loss: 0.188 \n",
            "epoch: 50, acc: 0.871, loss: 0.377 (data_loss: 0.342, reg_loss: 0.035), lr: 0.004999875 validation, acc: 0.886, loss: 0.290 \n",
            "epoch: 51, acc: 0.867, loss: 0.378 (data_loss: 0.343, reg_loss: 0.035), lr: 0.004999873 validation, acc: 0.952, loss: 0.179 \n",
            "epoch: 52, acc: 0.862, loss: 0.351 (data_loss: 0.316, reg_loss: 0.035), lr: 0.004999870 validation, acc: 0.944, loss: 0.190 \n",
            "epoch: 53, acc: 0.944, loss: 0.230 (data_loss: 0.195, reg_loss: 0.035), lr: 0.004999868 validation, acc: 0.935, loss: 0.214 \n",
            "epoch: 54, acc: 0.850, loss: 0.815 (data_loss: 0.780, reg_loss: 0.035), lr: 0.004999865 validation, acc: 0.840, loss: 0.875 \n",
            "epoch: 55, acc: 0.835, loss: 1.028 (data_loss: 0.992, reg_loss: 0.036), lr: 0.004999863 validation, acc: 0.831, loss: 1.015 \n",
            "epoch: 56, acc: 0.819, loss: 1.546 (data_loss: 1.510, reg_loss: 0.036), lr: 0.004999860 validation, acc: 0.814, loss: 1.508 \n",
            "epoch: 57, acc: 0.798, loss: 1.768 (data_loss: 1.732, reg_loss: 0.036), lr: 0.004999858 validation, acc: 0.847, loss: 1.500 \n",
            "epoch: 58, acc: 0.806, loss: 1.846 (data_loss: 1.810, reg_loss: 0.036), lr: 0.004999855 validation, acc: 0.817, loss: 1.895 \n",
            "epoch: 59, acc: 0.725, loss: 2.087 (data_loss: 2.051, reg_loss: 0.036), lr: 0.004999853 validation, acc: 0.809, loss: 1.969 \n",
            "epoch: 60, acc: 0.717, loss: 2.517 (data_loss: 2.481, reg_loss: 0.036), lr: 0.004999850 validation, acc: 0.743, loss: 2.015 \n",
            "epoch: 61, acc: 0.713, loss: 2.075 (data_loss: 2.039, reg_loss: 0.037), lr: 0.004999848 validation, acc: 0.841, loss: 1.875 \n",
            "epoch: 62, acc: 0.826, loss: 1.806 (data_loss: 1.769, reg_loss: 0.037), lr: 0.004999845 validation, acc: 0.834, loss: 1.921 \n",
            "epoch: 63, acc: 0.832, loss: 1.768 (data_loss: 1.730, reg_loss: 0.037), lr: 0.004999843 validation, acc: 0.848, loss: 1.883 \n",
            "epoch: 64, acc: 0.809, loss: 2.241 (data_loss: 2.203, reg_loss: 0.038), lr: 0.004999840 validation, acc: 0.805, loss: 2.154 \n",
            "epoch: 65, acc: 0.796, loss: 2.182 (data_loss: 2.144, reg_loss: 0.038), lr: 0.004999838 validation, acc: 0.811, loss: 2.072 \n",
            "epoch: 66, acc: 0.769, loss: 2.107 (data_loss: 2.069, reg_loss: 0.038), lr: 0.004999835 validation, acc: 0.850, loss: 1.864 \n",
            "epoch: 67, acc: 0.705, loss: 2.990 (data_loss: 2.951, reg_loss: 0.038), lr: 0.004999833 validation, acc: 0.767, loss: 3.230 \n",
            "epoch: 68, acc: 0.769, loss: 3.296 (data_loss: 3.258, reg_loss: 0.039), lr: 0.004999830 validation, acc: 0.747, loss: 3.421 \n",
            "epoch: 69, acc: 0.756, loss: 3.298 (data_loss: 3.259, reg_loss: 0.039), lr: 0.004999828 validation, acc: 0.778, loss: 3.292 \n",
            "epoch: 70, acc: 0.786, loss: 3.263 (data_loss: 3.224, reg_loss: 0.039), lr: 0.004999825 validation, acc: 0.769, loss: 3.340 \n",
            "epoch: 71, acc: 0.746, loss: 3.372 (data_loss: 3.332, reg_loss: 0.039), lr: 0.004999823 validation, acc: 0.768, loss: 3.208 \n",
            "epoch: 72, acc: 0.746, loss: 3.359 (data_loss: 3.319, reg_loss: 0.040), lr: 0.004999820 validation, acc: 0.735, loss: 3.022 \n",
            "epoch: 73, acc: 0.749, loss: 3.225 (data_loss: 3.185, reg_loss: 0.040), lr: 0.004999818 validation, acc: 0.765, loss: 2.849 \n",
            "epoch: 74, acc: 0.722, loss: 3.089 (data_loss: 3.049, reg_loss: 0.040), lr: 0.004999815 validation, acc: 0.786, loss: 2.700 \n",
            "epoch: 75, acc: 0.794, loss: 2.987 (data_loss: 2.947, reg_loss: 0.040), lr: 0.004999813 validation, acc: 0.756, loss: 2.734 \n",
            "epoch: 76, acc: 0.757, loss: 2.918 (data_loss: 2.878, reg_loss: 0.040), lr: 0.004999810 validation, acc: 0.783, loss: 2.669 \n",
            "epoch: 77, acc: 0.792, loss: 2.856 (data_loss: 2.815, reg_loss: 0.040), lr: 0.004999808 validation, acc: 0.782, loss: 2.678 \n",
            "epoch: 78, acc: 0.779, loss: 2.886 (data_loss: 2.846, reg_loss: 0.040), lr: 0.004999805 validation, acc: 0.788, loss: 2.532 \n",
            "epoch: 79, acc: 0.794, loss: 2.725 (data_loss: 2.685, reg_loss: 0.040), lr: 0.004999803 validation, acc: 0.791, loss: 2.585 \n",
            "epoch: 80, acc: 0.773, loss: 2.847 (data_loss: 2.807, reg_loss: 0.040), lr: 0.004999800 validation, acc: 0.782, loss: 2.671 \n",
            "epoch: 81, acc: 0.709, loss: 3.251 (data_loss: 3.211, reg_loss: 0.040), lr: 0.004999798 validation, acc: 0.742, loss: 2.562 \n",
            "epoch: 82, acc: 0.753, loss: 2.820 (data_loss: 2.780, reg_loss: 0.040), lr: 0.004999795 validation, acc: 0.777, loss: 2.541 \n",
            "epoch: 83, acc: 0.788, loss: 2.717 (data_loss: 2.678, reg_loss: 0.040), lr: 0.004999793 validation, acc: 0.790, loss: 2.605 \n",
            "epoch: 84, acc: 0.793, loss: 2.820 (data_loss: 2.780, reg_loss: 0.039), lr: 0.004999790 validation, acc: 0.776, loss: 2.664 \n",
            "epoch: 85, acc: 0.788, loss: 2.861 (data_loss: 2.822, reg_loss: 0.039), lr: 0.004999788 validation, acc: 0.784, loss: 2.661 \n",
            "epoch: 86, acc: 0.795, loss: 2.867 (data_loss: 2.828, reg_loss: 0.039), lr: 0.004999785 validation, acc: 0.795, loss: 2.532 \n",
            "epoch: 87, acc: 0.686, loss: 3.133 (data_loss: 3.095, reg_loss: 0.039), lr: 0.004999783 validation, acc: 0.788, loss: 2.573 \n",
            "epoch: 88, acc: 0.796, loss: 2.869 (data_loss: 2.831, reg_loss: 0.038), lr: 0.004999780 validation, acc: 0.790, loss: 2.711 \n",
            "epoch: 89, acc: 0.742, loss: 2.985 (data_loss: 2.947, reg_loss: 0.038), lr: 0.004999778 validation, acc: 0.794, loss: 2.563 \n",
            "epoch: 90, acc: 0.704, loss: 3.106 (data_loss: 3.068, reg_loss: 0.038), lr: 0.004999775 validation, acc: 0.788, loss: 2.512 \n",
            "epoch: 91, acc: 0.722, loss: 2.875 (data_loss: 2.838, reg_loss: 0.037), lr: 0.004999773 validation, acc: 0.720, loss: 2.491 \n",
            "epoch: 92, acc: 0.709, loss: 2.783 (data_loss: 2.747, reg_loss: 0.037), lr: 0.004999770 validation, acc: 0.592, loss: 2.674 \n",
            "epoch: 93, acc: 0.524, loss: 3.285 (data_loss: 3.248, reg_loss: 0.037), lr: 0.004999768 validation, acc: 0.731, loss: 2.513 \n",
            "epoch: 94, acc: 0.625, loss: 3.123 (data_loss: 3.087, reg_loss: 0.036), lr: 0.004999765 validation, acc: 0.520, loss: 3.311 \n",
            "epoch: 95, acc: 0.593, loss: 3.745 (data_loss: 3.709, reg_loss: 0.036), lr: 0.004999763 validation, acc: 0.576, loss: 3.412 \n",
            "epoch: 96, acc: 0.512, loss: 4.112 (data_loss: 4.076, reg_loss: 0.037), lr: 0.004999760 validation, acc: 0.603, loss: 3.164 \n",
            "epoch: 97, acc: 0.637, loss: 3.161 (data_loss: 3.124, reg_loss: 0.037), lr: 0.004999758 validation, acc: 0.656, loss: 2.770 \n",
            "epoch: 98, acc: 0.576, loss: 3.239 (data_loss: 3.202, reg_loss: 0.038), lr: 0.004999755 validation, acc: 0.649, loss: 2.872 \n",
            "epoch: 99, acc: 0.599, loss: 3.229 (data_loss: 3.191, reg_loss: 0.038), lr: 0.004999753 validation, acc: 0.664, loss: 2.897 \n",
            "epoch: 100, acc: 0.605, loss: 3.469 (data_loss: 3.431, reg_loss: 0.039), lr: 0.004999750 validation, acc: 0.617, loss: 3.300 \n",
            "epoch: 101, acc: 0.585, loss: 4.099 (data_loss: 4.059, reg_loss: 0.040), lr: 0.004999748 validation, acc: 0.596, loss: 3.881 \n",
            "epoch: 102, acc: 0.601, loss: 4.230 (data_loss: 4.189, reg_loss: 0.040), lr: 0.004999745 validation, acc: 0.585, loss: 4.028 \n",
            "epoch: 103, acc: 0.448, loss: 5.606 (data_loss: 5.565, reg_loss: 0.041), lr: 0.004999743 validation, acc: 0.477, loss: 4.742 \n",
            "epoch: 104, acc: 0.467, loss: 5.367 (data_loss: 5.325, reg_loss: 0.042), lr: 0.004999740 validation, acc: 0.536, loss: 4.182 \n",
            "epoch: 105, acc: 0.450, loss: 4.928 (data_loss: 4.885, reg_loss: 0.043), lr: 0.004999738 validation, acc: 0.569, loss: 3.694 \n",
            "epoch: 106, acc: 0.541, loss: 4.396 (data_loss: 4.352, reg_loss: 0.044), lr: 0.004999735 validation, acc: 0.595, loss: 3.410 \n",
            "epoch: 107, acc: 0.596, loss: 3.756 (data_loss: 3.711, reg_loss: 0.045), lr: 0.004999733 validation, acc: 0.599, loss: 3.034 \n",
            "epoch: 108, acc: 0.611, loss: 3.194 (data_loss: 3.149, reg_loss: 0.045), lr: 0.004999730 validation, acc: 0.645, loss: 2.672 \n",
            "epoch: 109, acc: 0.606, loss: 3.388 (data_loss: 3.342, reg_loss: 0.046), lr: 0.004999728 validation, acc: 0.613, loss: 3.258 \n",
            "epoch: 110, acc: 0.481, loss: 3.945 (data_loss: 3.898, reg_loss: 0.047), lr: 0.004999725 validation, acc: 0.660, loss: 3.093 \n",
            "epoch: 111, acc: 0.508, loss: 4.325 (data_loss: 4.278, reg_loss: 0.047), lr: 0.004999723 validation, acc: 0.688, loss: 3.390 \n",
            "epoch: 112, acc: 0.693, loss: 3.862 (data_loss: 3.814, reg_loss: 0.048), lr: 0.004999720 validation, acc: 0.603, loss: 3.546 \n",
            "epoch: 113, acc: 0.595, loss: 4.100 (data_loss: 4.051, reg_loss: 0.049), lr: 0.004999718 validation, acc: 0.592, loss: 3.826 \n",
            "epoch: 114, acc: 0.481, loss: 4.499 (data_loss: 4.449, reg_loss: 0.049), lr: 0.004999715 validation, acc: 0.655, loss: 3.820 \n",
            "epoch: 115, acc: 0.584, loss: 4.422 (data_loss: 4.372, reg_loss: 0.050), lr: 0.004999713 validation, acc: 0.569, loss: 4.000 \n",
            "epoch: 116, acc: 0.462, loss: 5.349 (data_loss: 5.299, reg_loss: 0.050), lr: 0.004999710 validation, acc: 0.535, loss: 4.680 \n",
            "epoch: 117, acc: 0.542, loss: 5.133 (data_loss: 5.083, reg_loss: 0.051), lr: 0.004999708 validation, acc: 0.593, loss: 4.419 \n",
            "epoch: 118, acc: 0.445, loss: 5.144 (data_loss: 5.093, reg_loss: 0.051), lr: 0.004999705 validation, acc: 0.505, loss: 4.599 \n",
            "epoch: 119, acc: 0.510, loss: 5.026 (data_loss: 4.974, reg_loss: 0.052), lr: 0.004999703 validation, acc: 0.511, loss: 4.398 \n",
            "epoch: 120, acc: 0.520, loss: 5.167 (data_loss: 5.115, reg_loss: 0.052), lr: 0.004999700 validation, acc: 0.535, loss: 4.516 \n",
            "epoch: 121, acc: 0.549, loss: 4.997 (data_loss: 4.944, reg_loss: 0.052), lr: 0.004999698 validation, acc: 0.569, loss: 4.558 \n",
            "epoch: 122, acc: 0.575, loss: 4.920 (data_loss: 4.868, reg_loss: 0.053), lr: 0.004999695 validation, acc: 0.601, loss: 4.474 \n",
            "epoch: 123, acc: 0.575, loss: 5.010 (data_loss: 4.957, reg_loss: 0.053), lr: 0.004999693 validation, acc: 0.575, loss: 4.447 \n",
            "epoch: 124, acc: 0.584, loss: 4.963 (data_loss: 4.910, reg_loss: 0.053), lr: 0.004999690 validation, acc: 0.599, loss: 4.229 \n",
            "epoch: 125, acc: 0.610, loss: 4.562 (data_loss: 4.509, reg_loss: 0.053), lr: 0.004999688 validation, acc: 0.601, loss: 4.123 \n",
            "epoch: 126, acc: 0.611, loss: 4.453 (data_loss: 4.400, reg_loss: 0.053), lr: 0.004999685 validation, acc: 0.600, loss: 4.044 \n",
            "epoch: 127, acc: 0.588, loss: 4.425 (data_loss: 4.372, reg_loss: 0.053), lr: 0.004999683 validation, acc: 0.596, loss: 4.036 \n",
            "epoch: 128, acc: 0.607, loss: 4.294 (data_loss: 4.241, reg_loss: 0.053), lr: 0.004999680 validation, acc: 0.595, loss: 3.978 \n",
            "epoch: 129, acc: 0.594, loss: 4.317 (data_loss: 4.263, reg_loss: 0.053), lr: 0.004999678 validation, acc: 0.595, loss: 3.968 \n",
            "epoch: 130, acc: 0.606, loss: 4.226 (data_loss: 4.173, reg_loss: 0.053), lr: 0.004999675 validation, acc: 0.600, loss: 3.845 \n",
            "epoch: 131, acc: 0.612, loss: 4.177 (data_loss: 4.124, reg_loss: 0.053), lr: 0.004999673 validation, acc: 0.601, loss: 3.832 \n",
            "epoch: 132, acc: 0.511, loss: 4.164 (data_loss: 4.112, reg_loss: 0.053), lr: 0.004999670 validation, acc: 0.542, loss: 3.840 \n",
            "epoch: 133, acc: 0.529, loss: 4.094 (data_loss: 4.042, reg_loss: 0.052), lr: 0.004999668 validation, acc: 0.594, loss: 3.871 \n",
            "epoch: 134, acc: 0.601, loss: 4.436 (data_loss: 4.384, reg_loss: 0.052), lr: 0.004999665 validation, acc: 0.575, loss: 4.252 \n",
            "epoch: 135, acc: 0.587, loss: 4.547 (data_loss: 4.496, reg_loss: 0.051), lr: 0.004999663 validation, acc: 0.598, loss: 4.250 \n",
            "epoch: 136, acc: 0.611, loss: 4.446 (data_loss: 4.395, reg_loss: 0.051), lr: 0.004999660 validation, acc: 0.601, loss: 4.133 \n",
            "epoch: 137, acc: 0.593, loss: 4.410 (data_loss: 4.359, reg_loss: 0.050), lr: 0.004999658 validation, acc: 0.572, loss: 4.351 \n",
            "epoch: 138, acc: 0.580, loss: 4.340 (data_loss: 4.290, reg_loss: 0.050), lr: 0.004999655 validation, acc: 0.584, loss: 4.297 \n",
            "epoch: 139, acc: 0.590, loss: 4.537 (data_loss: 4.488, reg_loss: 0.049), lr: 0.004999653 validation, acc: 0.597, loss: 4.309 \n",
            "epoch: 140, acc: 0.520, loss: 4.705 (data_loss: 4.656, reg_loss: 0.049), lr: 0.004999650 validation, acc: 0.565, loss: 4.312 \n",
            "epoch: 141, acc: 0.522, loss: 4.502 (data_loss: 4.454, reg_loss: 0.048), lr: 0.004999648 validation, acc: 0.587, loss: 4.330 \n",
            "epoch: 142, acc: 0.609, loss: 4.603 (data_loss: 4.555, reg_loss: 0.048), lr: 0.004999645 validation, acc: 0.595, loss: 4.588 \n",
            "epoch: 143, acc: 0.606, loss: 4.846 (data_loss: 4.798, reg_loss: 0.048), lr: 0.004999643 validation, acc: 0.583, loss: 4.730 \n",
            "epoch: 144, acc: 0.606, loss: 4.820 (data_loss: 4.772, reg_loss: 0.047), lr: 0.004999640 validation, acc: 0.600, loss: 4.679 \n",
            "epoch: 145, acc: 0.610, loss: 4.778 (data_loss: 4.731, reg_loss: 0.047), lr: 0.004999638 validation, acc: 0.600, loss: 4.629 \n",
            "epoch: 146, acc: 0.613, loss: 4.723 (data_loss: 4.676, reg_loss: 0.047), lr: 0.004999635 validation, acc: 0.601, loss: 4.595 \n",
            "epoch: 147, acc: 0.614, loss: 4.676 (data_loss: 4.629, reg_loss: 0.047), lr: 0.004999633 validation, acc: 0.601, loss: 4.572 \n",
            "epoch: 148, acc: 0.614, loss: 4.644 (data_loss: 4.597, reg_loss: 0.046), lr: 0.004999630 validation, acc: 0.602, loss: 4.543 \n",
            "epoch: 149, acc: 0.615, loss: 4.645 (data_loss: 4.599, reg_loss: 0.046), lr: 0.004999628 validation, acc: 0.602, loss: 4.535 \n",
            "epoch: 150, acc: 0.615, loss: 4.589 (data_loss: 4.543, reg_loss: 0.046), lr: 0.004999625 validation, acc: 0.602, loss: 4.403 \n",
            "epoch: 151, acc: 0.615, loss: 4.442 (data_loss: 4.396, reg_loss: 0.045), lr: 0.004999623 validation, acc: 0.602, loss: 4.262 \n",
            "epoch: 152, acc: 0.615, loss: 4.270 (data_loss: 4.225, reg_loss: 0.045), lr: 0.004999620 validation, acc: 0.612, loss: 4.113 \n",
            "epoch: 153, acc: 0.628, loss: 4.126 (data_loss: 4.082, reg_loss: 0.044), lr: 0.004999618 validation, acc: 0.699, loss: 4.021 \n",
            "epoch: 154, acc: 0.717, loss: 4.008 (data_loss: 3.964, reg_loss: 0.044), lr: 0.004999615 validation, acc: 0.699, loss: 3.917 \n",
            "epoch: 155, acc: 0.717, loss: 3.944 (data_loss: 3.901, reg_loss: 0.043), lr: 0.004999613 validation, acc: 0.699, loss: 3.868 \n",
            "epoch: 156, acc: 0.717, loss: 3.951 (data_loss: 3.908, reg_loss: 0.043), lr: 0.004999610 validation, acc: 0.699, loss: 3.858 \n",
            "epoch: 157, acc: 0.647, loss: 4.042 (data_loss: 3.999, reg_loss: 0.042), lr: 0.004999608 validation, acc: 0.699, loss: 3.741 \n",
            "epoch: 158, acc: 0.716, loss: 3.817 (data_loss: 3.775, reg_loss: 0.042), lr: 0.004999605 validation, acc: 0.699, loss: 3.574 \n",
            "epoch: 159, acc: 0.716, loss: 3.863 (data_loss: 3.822, reg_loss: 0.041), lr: 0.004999603 validation, acc: 0.699, loss: 3.298 \n",
            "epoch: 160, acc: 0.717, loss: 3.702 (data_loss: 3.661, reg_loss: 0.041), lr: 0.004999600 validation, acc: 0.699, loss: 3.048 \n",
            "epoch: 161, acc: 0.712, loss: 3.419 (data_loss: 3.379, reg_loss: 0.040), lr: 0.004999598 validation, acc: 0.693, loss: 2.742 \n",
            "epoch: 162, acc: 0.709, loss: 3.187 (data_loss: 3.147, reg_loss: 0.040), lr: 0.004999595 validation, acc: 0.697, loss: 2.549 \n",
            "epoch: 163, acc: 0.716, loss: 3.034 (data_loss: 2.995, reg_loss: 0.039), lr: 0.004999593 validation, acc: 0.699, loss: 2.396 \n",
            "epoch: 164, acc: 0.717, loss: 2.594 (data_loss: 2.555, reg_loss: 0.039), lr: 0.004999590 validation, acc: 0.779, loss: 1.902 \n",
            "epoch: 165, acc: 0.786, loss: 2.121 (data_loss: 2.083, reg_loss: 0.038), lr: 0.004999588 validation, acc: 0.805, loss: 1.695 \n",
            "epoch: 166, acc: 0.811, loss: 1.931 (data_loss: 1.892, reg_loss: 0.038), lr: 0.004999585 validation, acc: 0.805, loss: 1.604 \n",
            "epoch: 167, acc: 0.811, loss: 1.904 (data_loss: 1.866, reg_loss: 0.038), lr: 0.004999583 validation, acc: 0.790, loss: 1.705 \n",
            "epoch: 168, acc: 0.796, loss: 2.034 (data_loss: 1.996, reg_loss: 0.038), lr: 0.004999580 validation, acc: 0.802, loss: 1.713 \n",
            "epoch: 169, acc: 0.778, loss: 2.055 (data_loss: 2.018, reg_loss: 0.038), lr: 0.004999578 validation, acc: 0.804, loss: 1.709 \n",
            "epoch: 170, acc: 0.716, loss: 2.135 (data_loss: 2.097, reg_loss: 0.037), lr: 0.004999575 validation, acc: 0.798, loss: 1.697 \n",
            "epoch: 171, acc: 0.802, loss: 2.025 (data_loss: 1.988, reg_loss: 0.037), lr: 0.004999573 validation, acc: 0.800, loss: 1.599 \n",
            "epoch: 172, acc: 0.807, loss: 1.872 (data_loss: 1.836, reg_loss: 0.037), lr: 0.004999570 validation, acc: 0.804, loss: 1.405 \n",
            "epoch: 173, acc: 0.811, loss: 1.681 (data_loss: 1.645, reg_loss: 0.036), lr: 0.004999568 validation, acc: 0.732, loss: 1.338 \n",
            "epoch: 174, acc: 0.726, loss: 1.767 (data_loss: 1.731, reg_loss: 0.036), lr: 0.004999565 validation, acc: 0.805, loss: 1.331 \n",
            "epoch: 175, acc: 0.811, loss: 1.620 (data_loss: 1.584, reg_loss: 0.036), lr: 0.004999563 validation, acc: 0.802, loss: 1.370 \n",
            "epoch: 176, acc: 0.808, loss: 1.629 (data_loss: 1.594, reg_loss: 0.035), lr: 0.004999560 validation, acc: 0.805, loss: 1.260 \n",
            "epoch: 177, acc: 0.727, loss: 1.609 (data_loss: 1.574, reg_loss: 0.035), lr: 0.004999558 validation, acc: 0.797, loss: 1.328 \n",
            "epoch: 178, acc: 0.806, loss: 1.454 (data_loss: 1.420, reg_loss: 0.035), lr: 0.004999555 validation, acc: 0.795, loss: 1.238 \n",
            "epoch: 179, acc: 0.781, loss: 1.448 (data_loss: 1.414, reg_loss: 0.034), lr: 0.004999553 validation, acc: 0.799, loss: 1.098 \n",
            "epoch: 180, acc: 0.807, loss: 1.313 (data_loss: 1.279, reg_loss: 0.034), lr: 0.004999550 validation, acc: 0.796, loss: 1.009 \n",
            "epoch: 181, acc: 0.773, loss: 1.220 (data_loss: 1.187, reg_loss: 0.034), lr: 0.004999548 validation, acc: 0.795, loss: 0.990 \n",
            "epoch: 182, acc: 0.804, loss: 1.152 (data_loss: 1.119, reg_loss: 0.033), lr: 0.004999545 validation, acc: 0.798, loss: 0.992 \n",
            "epoch: 183, acc: 0.804, loss: 1.139 (data_loss: 1.106, reg_loss: 0.033), lr: 0.004999543 validation, acc: 0.798, loss: 0.958 \n",
            "epoch: 184, acc: 0.803, loss: 1.299 (data_loss: 1.267, reg_loss: 0.032), lr: 0.004999540 validation, acc: 0.803, loss: 1.025 \n",
            "epoch: 185, acc: 0.810, loss: 0.957 (data_loss: 0.925, reg_loss: 0.032), lr: 0.004999538 validation, acc: 0.811, loss: 0.716 \n",
            "epoch: 186, acc: 0.814, loss: 0.840 (data_loss: 0.809, reg_loss: 0.031), lr: 0.004999535 validation, acc: 0.886, loss: 0.627 \n",
            "epoch: 187, acc: 0.869, loss: 0.764 (data_loss: 0.733, reg_loss: 0.031), lr: 0.004999533 validation, acc: 0.878, loss: 0.600 \n",
            "epoch: 188, acc: 0.884, loss: 0.669 (data_loss: 0.638, reg_loss: 0.030), lr: 0.004999530 validation, acc: 0.888, loss: 0.531 \n",
            "epoch: 189, acc: 0.898, loss: 0.557 (data_loss: 0.527, reg_loss: 0.030), lr: 0.004999528 validation, acc: 0.892, loss: 0.525 \n",
            "epoch: 190, acc: 0.895, loss: 0.554 (data_loss: 0.525, reg_loss: 0.029), lr: 0.004999525 validation, acc: 0.892, loss: 0.505 \n",
            "epoch: 191, acc: 0.811, loss: 0.952 (data_loss: 0.924, reg_loss: 0.029), lr: 0.004999523 validation, acc: 0.805, loss: 0.681 \n",
            "epoch: 192, acc: 0.804, loss: 0.788 (data_loss: 0.760, reg_loss: 0.028), lr: 0.004999520 validation, acc: 0.891, loss: 0.460 \n",
            "epoch: 193, acc: 0.853, loss: 0.545 (data_loss: 0.517, reg_loss: 0.028), lr: 0.004999518 validation, acc: 0.889, loss: 0.420 \n",
            "epoch: 194, acc: 0.788, loss: 0.666 (data_loss: 0.638, reg_loss: 0.028), lr: 0.004999515 validation, acc: 0.801, loss: 0.474 \n",
            "epoch: 195, acc: 0.732, loss: 0.703 (data_loss: 0.676, reg_loss: 0.028), lr: 0.004999513 validation, acc: 0.874, loss: 0.374 \n",
            "epoch: 196, acc: 0.863, loss: 0.430 (data_loss: 0.403, reg_loss: 0.028), lr: 0.004999510 validation, acc: 0.889, loss: 0.379 \n",
            "epoch: 197, acc: 0.812, loss: 0.518 (data_loss: 0.490, reg_loss: 0.028), lr: 0.004999508 validation, acc: 0.914, loss: 0.294 \n",
            "epoch: 198, acc: 0.784, loss: 0.971 (data_loss: 0.943, reg_loss: 0.028), lr: 0.004999505 validation, acc: 0.859, loss: 0.720 \n",
            "epoch: 199, acc: 0.782, loss: 0.888 (data_loss: 0.860, reg_loss: 0.028), lr: 0.004999503 validation, acc: 0.844, loss: 0.642 \n",
            "epoch: 200, acc: 0.651, loss: 0.996 (data_loss: 0.968, reg_loss: 0.028), lr: 0.004999500 validation, acc: 0.839, loss: 0.404 \n",
            "epoch: 201, acc: 0.780, loss: 0.638 (data_loss: 0.609, reg_loss: 0.029), lr: 0.004999498 validation, acc: 0.963, loss: 0.268 \n",
            "epoch: 202, acc: 0.947, loss: 0.301 (data_loss: 0.271, reg_loss: 0.029), lr: 0.004999495 validation, acc: 0.882, loss: 0.278 \n",
            "epoch: 203, acc: 0.827, loss: 0.429 (data_loss: 0.399, reg_loss: 0.030), lr: 0.004999493 validation, acc: 0.874, loss: 0.305 \n",
            "epoch: 204, acc: 0.704, loss: 0.742 (data_loss: 0.711, reg_loss: 0.031), lr: 0.004999490 validation, acc: 0.827, loss: 0.385 \n",
            "epoch: 205, acc: 0.809, loss: 0.442 (data_loss: 0.410, reg_loss: 0.032), lr: 0.004999488 validation, acc: 0.899, loss: 0.291 \n",
            "epoch: 206, acc: 0.891, loss: 0.360 (data_loss: 0.327, reg_loss: 0.033), lr: 0.004999485 validation, acc: 0.914, loss: 0.243 \n",
            "epoch: 207, acc: 0.927, loss: 0.255 (data_loss: 0.221, reg_loss: 0.034), lr: 0.004999483 validation, acc: 0.951, loss: 0.168 \n",
            "epoch: 208, acc: 0.954, loss: 0.178 (data_loss: 0.143, reg_loss: 0.035), lr: 0.004999480 validation, acc: 0.984, loss: 0.099 \n",
            "epoch: 209, acc: 0.976, loss: 0.135 (data_loss: 0.099, reg_loss: 0.036), lr: 0.004999478 validation, acc: 0.962, loss: 0.131 \n",
            "epoch: 210, acc: 0.964, loss: 0.142 (data_loss: 0.105, reg_loss: 0.037), lr: 0.004999475 validation, acc: 0.989, loss: 0.094 \n",
            "epoch: 211, acc: 0.992, loss: 0.113 (data_loss: 0.075, reg_loss: 0.038), lr: 0.004999473 validation, acc: 0.985, loss: 0.096 \n",
            "epoch: 212, acc: 0.897, loss: 0.256 (data_loss: 0.218, reg_loss: 0.038), lr: 0.004999470 validation, acc: 0.992, loss: 0.084 \n",
            "epoch: 213, acc: 0.991, loss: 0.100 (data_loss: 0.061, reg_loss: 0.039), lr: 0.004999468 validation, acc: 0.996, loss: 0.037 \n",
            "epoch: 214, acc: 0.997, loss: 0.060 (data_loss: 0.021, reg_loss: 0.039), lr: 0.004999465 validation, acc: 0.998, loss: 0.024 \n",
            "epoch: 215, acc: 0.999, loss: 0.051 (data_loss: 0.011, reg_loss: 0.040), lr: 0.004999463 validation, acc: 0.997, loss: 0.024 \n",
            "epoch: 216, acc: 0.899, loss: 0.365 (data_loss: 0.324, reg_loss: 0.040), lr: 0.004999460 validation, acc: 0.906, loss: 0.226 \n",
            "epoch: 217, acc: 0.909, loss: 0.277 (data_loss: 0.236, reg_loss: 0.041), lr: 0.004999458 validation, acc: 0.977, loss: 0.099 \n",
            "epoch: 218, acc: 0.889, loss: 0.406 (data_loss: 0.365, reg_loss: 0.041), lr: 0.004999455 validation, acc: 0.895, loss: 0.252 \n",
            "epoch: 219, acc: 0.879, loss: 0.354 (data_loss: 0.313, reg_loss: 0.041), lr: 0.004999453 validation, acc: 0.977, loss: 0.137 \n",
            "epoch: 220, acc: 0.978, loss: 0.155 (data_loss: 0.114, reg_loss: 0.041), lr: 0.004999450 validation, acc: 0.996, loss: 0.048 \n",
            "epoch: 221, acc: 0.997, loss: 0.069 (data_loss: 0.028, reg_loss: 0.041), lr: 0.004999448 validation, acc: 0.994, loss: 0.031 \n",
            "epoch: 222, acc: 0.996, loss: 0.062 (data_loss: 0.021, reg_loss: 0.041), lr: 0.004999445 validation, acc: 0.995, loss: 0.026 \n",
            "epoch: 223, acc: 0.980, loss: 0.098 (data_loss: 0.057, reg_loss: 0.041), lr: 0.004999443 validation, acc: 0.995, loss: 0.028 \n",
            "epoch: 224, acc: 0.995, loss: 0.061 (data_loss: 0.020, reg_loss: 0.041), lr: 0.004999440 validation, acc: 0.999, loss: 0.016 \n",
            "epoch: 225, acc: 0.996, loss: 0.057 (data_loss: 0.016, reg_loss: 0.041), lr: 0.004999438 validation, acc: 0.999, loss: 0.015 \n",
            "epoch: 226, acc: 0.998, loss: 0.056 (data_loss: 0.015, reg_loss: 0.041), lr: 0.004999435 validation, acc: 0.999, loss: 0.015 \n",
            "epoch: 227, acc: 0.904, loss: 0.375 (data_loss: 0.334, reg_loss: 0.040), lr: 0.004999433 validation, acc: 0.976, loss: 0.066 \n",
            "epoch: 228, acc: 0.976, loss: 0.099 (data_loss: 0.059, reg_loss: 0.040), lr: 0.004999430 validation, acc: 0.910, loss: 0.269 \n",
            "epoch: 229, acc: 0.906, loss: 0.351 (data_loss: 0.311, reg_loss: 0.040), lr: 0.004999428 validation, acc: 0.900, loss: 0.446 \n",
            "epoch: 230, acc: 0.897, loss: 0.593 (data_loss: 0.554, reg_loss: 0.039), lr: 0.004999425 validation, acc: 0.899, loss: 0.478 \n",
            "epoch: 231, acc: 0.899, loss: 0.610 (data_loss: 0.571, reg_loss: 0.039), lr: 0.004999423 validation, acc: 0.909, loss: 0.350 \n",
            "epoch: 232, acc: 0.906, loss: 0.464 (data_loss: 0.425, reg_loss: 0.039), lr: 0.004999420 validation, acc: 0.948, loss: 0.127 \n",
            "epoch: 233, acc: 0.847, loss: 0.721 (data_loss: 0.682, reg_loss: 0.038), lr: 0.004999418 validation, acc: 0.893, loss: 0.215 \n",
            "epoch: 234, acc: 0.882, loss: 0.298 (data_loss: 0.260, reg_loss: 0.038), lr: 0.004999415 validation, acc: 0.923, loss: 0.241 \n",
            "epoch: 235, acc: 0.813, loss: 0.882 (data_loss: 0.844, reg_loss: 0.038), lr: 0.004999413 validation, acc: 0.922, loss: 0.251 \n",
            "epoch: 236, acc: 0.817, loss: 0.791 (data_loss: 0.753, reg_loss: 0.038), lr: 0.004999410 validation, acc: 0.961, loss: 0.124 \n",
            "epoch: 237, acc: 0.953, loss: 0.171 (data_loss: 0.133, reg_loss: 0.038), lr: 0.004999408 validation, acc: 0.996, loss: 0.038 \n",
            "epoch: 238, acc: 0.916, loss: 0.316 (data_loss: 0.278, reg_loss: 0.038), lr: 0.004999405 validation, acc: 0.986, loss: 0.066 \n",
            "epoch: 239, acc: 0.884, loss: 0.571 (data_loss: 0.533, reg_loss: 0.038), lr: 0.004999403 validation, acc: 0.947, loss: 0.156 \n",
            "epoch: 240, acc: 0.951, loss: 0.191 (data_loss: 0.153, reg_loss: 0.038), lr: 0.004999400 validation, acc: 0.939, loss: 0.161 \n",
            "epoch: 241, acc: 0.952, loss: 0.189 (data_loss: 0.151, reg_loss: 0.038), lr: 0.004999398 validation, acc: 0.979, loss: 0.076 \n",
            "epoch: 242, acc: 0.971, loss: 0.120 (data_loss: 0.082, reg_loss: 0.038), lr: 0.004999395 validation, acc: 0.979, loss: 0.083 \n",
            "epoch: 243, acc: 0.887, loss: 0.395 (data_loss: 0.357, reg_loss: 0.038), lr: 0.004999393 validation, acc: 0.984, loss: 0.095 \n",
            "epoch: 244, acc: 0.886, loss: 0.870 (data_loss: 0.832, reg_loss: 0.038), lr: 0.004999390 validation, acc: 0.887, loss: 0.731 \n",
            "epoch: 245, acc: 0.890, loss: 0.892 (data_loss: 0.854, reg_loss: 0.038), lr: 0.004999388 validation, acc: 0.886, loss: 0.847 \n",
            "epoch: 246, acc: 0.800, loss: 1.398 (data_loss: 1.361, reg_loss: 0.038), lr: 0.004999385 validation, acc: 0.805, loss: 1.100 \n",
            "epoch: 247, acc: 0.799, loss: 1.656 (data_loss: 1.618, reg_loss: 0.038), lr: 0.004999383 validation, acc: 0.809, loss: 1.281 \n",
            "epoch: 248, acc: 0.774, loss: 1.506 (data_loss: 1.469, reg_loss: 0.038), lr: 0.004999380 validation, acc: 0.803, loss: 1.376 \n",
            "epoch: 249, acc: 0.734, loss: 1.461 (data_loss: 1.423, reg_loss: 0.038), lr: 0.004999378 validation, acc: 0.782, loss: 1.309 \n",
            "epoch: 250, acc: 0.779, loss: 1.372 (data_loss: 1.334, reg_loss: 0.038), lr: 0.004999375 validation, acc: 0.810, loss: 0.955 \n",
            "epoch: 251, acc: 0.798, loss: 1.242 (data_loss: 1.204, reg_loss: 0.038), lr: 0.004999373 validation, acc: 0.826, loss: 0.851 \n",
            "epoch: 252, acc: 0.884, loss: 0.927 (data_loss: 0.889, reg_loss: 0.038), lr: 0.004999370 validation, acc: 0.911, loss: 0.816 \n",
            "epoch: 253, acc: 0.904, loss: 1.075 (data_loss: 1.037, reg_loss: 0.038), lr: 0.004999368 validation, acc: 0.902, loss: 0.797 \n",
            "epoch: 254, acc: 0.827, loss: 1.127 (data_loss: 1.089, reg_loss: 0.038), lr: 0.004999365 validation, acc: 0.913, loss: 0.774 \n",
            "epoch: 255, acc: 0.904, loss: 1.026 (data_loss: 0.987, reg_loss: 0.039), lr: 0.004999363 validation, acc: 0.913, loss: 0.769 \n",
            "epoch: 256, acc: 0.869, loss: 1.150 (data_loss: 1.112, reg_loss: 0.039), lr: 0.004999360 validation, acc: 0.913, loss: 0.843 \n",
            "epoch: 257, acc: 0.905, loss: 1.188 (data_loss: 1.149, reg_loss: 0.039), lr: 0.004999358 validation, acc: 0.913, loss: 0.892 \n",
            "epoch: 258, acc: 0.905, loss: 1.210 (data_loss: 1.171, reg_loss: 0.039), lr: 0.004999355 validation, acc: 0.913, loss: 0.968 \n",
            "epoch: 259, acc: 0.803, loss: 1.672 (data_loss: 1.633, reg_loss: 0.039), lr: 0.004999353 validation, acc: 0.879, loss: 0.980 \n",
            "epoch: 260, acc: 0.871, loss: 1.313 (data_loss: 1.275, reg_loss: 0.039), lr: 0.004999350 validation, acc: 0.913, loss: 0.892 \n",
            "epoch: 261, acc: 0.762, loss: 2.107 (data_loss: 2.068, reg_loss: 0.039), lr: 0.004999348 validation, acc: 0.749, loss: 1.748 \n",
            "epoch: 262, acc: 0.755, loss: 2.248 (data_loss: 2.209, reg_loss: 0.039), lr: 0.004999345 validation, acc: 0.799, loss: 1.728 \n",
            "epoch: 263, acc: 0.676, loss: 2.893 (data_loss: 2.854, reg_loss: 0.039), lr: 0.004999343 validation, acc: 0.682, loss: 1.931 \n",
            "epoch: 264, acc: 0.786, loss: 3.268 (data_loss: 3.229, reg_loss: 0.039), lr: 0.004999340 validation, acc: 0.766, loss: 2.749 \n",
            "epoch: 265, acc: 0.763, loss: 3.118 (data_loss: 3.079, reg_loss: 0.039), lr: 0.004999338 validation, acc: 0.793, loss: 2.669 \n",
            "epoch: 266, acc: 0.783, loss: 3.098 (data_loss: 3.059, reg_loss: 0.039), lr: 0.004999335 validation, acc: 0.799, loss: 2.639 \n",
            "epoch: 267, acc: 0.698, loss: 3.408 (data_loss: 3.369, reg_loss: 0.039), lr: 0.004999333 validation, acc: 0.726, loss: 2.681 \n",
            "epoch: 268, acc: 0.721, loss: 3.164 (data_loss: 3.125, reg_loss: 0.039), lr: 0.004999330 validation, acc: 0.751, loss: 2.459 \n",
            "epoch: 269, acc: 0.745, loss: 2.206 (data_loss: 2.167, reg_loss: 0.039), lr: 0.004999328 validation, acc: 0.749, loss: 1.516 \n",
            "epoch: 270, acc: 0.738, loss: 2.849 (data_loss: 2.810, reg_loss: 0.039), lr: 0.004999325 validation, acc: 0.780, loss: 2.147 \n",
            "epoch: 271, acc: 0.782, loss: 0.858 (data_loss: 0.819, reg_loss: 0.039), lr: 0.004999323 validation, acc: 0.864, loss: 0.319 \n",
            "epoch: 272, acc: 0.871, loss: 0.381 (data_loss: 0.342, reg_loss: 0.039), lr: 0.004999320 validation, acc: 0.892, loss: 0.335 \n",
            "epoch: 273, acc: 0.749, loss: 1.147 (data_loss: 1.109, reg_loss: 0.039), lr: 0.004999318 validation, acc: 0.882, loss: 0.411 \n",
            "epoch: 274, acc: 0.876, loss: 0.531 (data_loss: 0.492, reg_loss: 0.039), lr: 0.004999315 validation, acc: 0.892, loss: 0.248 \n",
            "epoch: 275, acc: 0.890, loss: 0.297 (data_loss: 0.258, reg_loss: 0.039), lr: 0.004999313 validation, acc: 0.942, loss: 0.151 \n",
            "epoch: 276, acc: 0.814, loss: 0.876 (data_loss: 0.836, reg_loss: 0.039), lr: 0.004999310 validation, acc: 0.818, loss: 0.484 \n",
            "epoch: 277, acc: 0.799, loss: 1.154 (data_loss: 1.114, reg_loss: 0.040), lr: 0.004999308 validation, acc: 0.900, loss: 0.593 \n",
            "epoch: 278, acc: 0.855, loss: 1.129 (data_loss: 1.089, reg_loss: 0.040), lr: 0.004999305 validation, acc: 0.891, loss: 0.813 \n",
            "epoch: 279, acc: 0.887, loss: 1.099 (data_loss: 1.059, reg_loss: 0.040), lr: 0.004999303 validation, acc: 0.894, loss: 0.855 \n",
            "epoch: 280, acc: 0.889, loss: 1.183 (data_loss: 1.143, reg_loss: 0.040), lr: 0.004999300 validation, acc: 0.895, loss: 0.898 \n",
            "epoch: 281, acc: 0.788, loss: 1.658 (data_loss: 1.618, reg_loss: 0.040), lr: 0.004999298 validation, acc: 0.882, loss: 0.952 \n",
            "epoch: 282, acc: 0.886, loss: 1.267 (data_loss: 1.227, reg_loss: 0.040), lr: 0.004999295 validation, acc: 0.902, loss: 0.910 \n",
            "epoch: 283, acc: 0.891, loss: 1.283 (data_loss: 1.243, reg_loss: 0.040), lr: 0.004999293 validation, acc: 0.895, loss: 0.962 \n",
            "epoch: 284, acc: 0.838, loss: 1.424 (data_loss: 1.384, reg_loss: 0.040), lr: 0.004999290 validation, acc: 0.901, loss: 1.022 \n",
            "epoch: 285, acc: 0.894, loss: 1.379 (data_loss: 1.338, reg_loss: 0.040), lr: 0.004999288 validation, acc: 0.899, loss: 1.064 \n",
            "epoch: 286, acc: 0.892, loss: 1.419 (data_loss: 1.378, reg_loss: 0.040), lr: 0.004999285 validation, acc: 0.890, loss: 1.140 \n",
            "epoch: 287, acc: 0.788, loss: 1.708 (data_loss: 1.667, reg_loss: 0.040), lr: 0.004999283 validation, acc: 0.899, loss: 1.096 \n",
            "epoch: 288, acc: 0.898, loss: 1.486 (data_loss: 1.446, reg_loss: 0.040), lr: 0.004999280 validation, acc: 0.904, loss: 1.092 \n",
            "epoch: 289, acc: 0.835, loss: 1.693 (data_loss: 1.653, reg_loss: 0.040), lr: 0.004999278 validation, acc: 0.891, loss: 1.184 \n",
            "epoch: 290, acc: 0.887, loss: 1.570 (data_loss: 1.530, reg_loss: 0.040), lr: 0.004999275 validation, acc: 0.854, loss: 1.359 \n",
            "epoch: 291, acc: 0.844, loss: 1.709 (data_loss: 1.669, reg_loss: 0.040), lr: 0.004999273 validation, acc: 0.882, loss: 1.226 \n",
            "epoch: 292, acc: 0.876, loss: 1.550 (data_loss: 1.510, reg_loss: 0.040), lr: 0.004999270 validation, acc: 0.906, loss: 0.984 \n",
            "epoch: 293, acc: 0.899, loss: 1.356 (data_loss: 1.316, reg_loss: 0.040), lr: 0.004999268 validation, acc: 0.901, loss: 0.932 \n",
            "epoch: 294, acc: 0.893, loss: 1.260 (data_loss: 1.220, reg_loss: 0.040), lr: 0.004999265 validation, acc: 0.899, loss: 0.884 \n",
            "epoch: 295, acc: 0.889, loss: 1.308 (data_loss: 1.268, reg_loss: 0.040), lr: 0.004999263 validation, acc: 0.899, loss: 0.951 \n",
            "epoch: 296, acc: 0.841, loss: 1.469 (data_loss: 1.429, reg_loss: 0.040), lr: 0.004999260 validation, acc: 0.879, loss: 0.901 \n",
            "epoch: 297, acc: 0.872, loss: 1.226 (data_loss: 1.187, reg_loss: 0.040), lr: 0.004999258 validation, acc: 0.878, loss: 0.856 \n",
            "epoch: 298, acc: 0.764, loss: 1.618 (data_loss: 1.579, reg_loss: 0.039), lr: 0.004999255 validation, acc: 0.795, loss: 1.131 \n",
            "epoch: 299, acc: 0.878, loss: 1.077 (data_loss: 1.038, reg_loss: 0.039), lr: 0.004999253 validation, acc: 0.897, loss: 0.717 \n",
            "epoch: 300, acc: 0.892, loss: 0.452 (data_loss: 0.413, reg_loss: 0.039), lr: 0.004999250 validation, acc: 0.899, loss: 0.257 \n",
            "epoch: 301, acc: 0.898, loss: 0.312 (data_loss: 0.273, reg_loss: 0.039), lr: 0.004999248 validation, acc: 0.927, loss: 0.159 \n",
            "epoch: 302, acc: 0.921, loss: 0.202 (data_loss: 0.163, reg_loss: 0.039), lr: 0.004999245 validation, acc: 0.945, loss: 0.122 \n",
            "epoch: 303, acc: 0.945, loss: 0.150 (data_loss: 0.112, reg_loss: 0.038), lr: 0.004999243 validation, acc: 0.980, loss: 0.069 \n",
            "epoch: 304, acc: 0.898, loss: 0.602 (data_loss: 0.564, reg_loss: 0.038), lr: 0.004999240 validation, acc: 0.900, loss: 0.489 \n",
            "epoch: 305, acc: 0.899, loss: 0.667 (data_loss: 0.629, reg_loss: 0.038), lr: 0.004999238 validation, acc: 0.898, loss: 0.542 \n",
            "epoch: 306, acc: 0.831, loss: 0.861 (data_loss: 0.824, reg_loss: 0.037), lr: 0.004999235 validation, acc: 0.894, loss: 0.642 \n",
            "epoch: 307, acc: 0.890, loss: 0.948 (data_loss: 0.911, reg_loss: 0.037), lr: 0.004999233 validation, acc: 0.891, loss: 0.817 \n",
            "epoch: 308, acc: 0.889, loss: 1.085 (data_loss: 1.049, reg_loss: 0.037), lr: 0.004999230 validation, acc: 0.889, loss: 0.872 \n",
            "epoch: 309, acc: 0.884, loss: 1.161 (data_loss: 1.125, reg_loss: 0.036), lr: 0.004999228 validation, acc: 0.890, loss: 0.898 \n",
            "epoch: 310, acc: 0.820, loss: 1.278 (data_loss: 1.242, reg_loss: 0.036), lr: 0.004999225 validation, acc: 0.866, loss: 0.958 \n",
            "epoch: 311, acc: 0.745, loss: 1.614 (data_loss: 1.578, reg_loss: 0.036), lr: 0.004999223 validation, acc: 0.864, loss: 1.039 \n",
            "epoch: 312, acc: 0.862, loss: 1.267 (data_loss: 1.232, reg_loss: 0.035), lr: 0.004999220 validation, acc: 0.894, loss: 0.938 \n",
            "epoch: 313, acc: 0.886, loss: 1.224 (data_loss: 1.189, reg_loss: 0.035), lr: 0.004999218 validation, acc: 0.891, loss: 0.948 \n",
            "epoch: 314, acc: 0.776, loss: 2.284 (data_loss: 2.249, reg_loss: 0.035), lr: 0.004999215 validation, acc: 0.785, loss: 1.650 \n",
            "epoch: 315, acc: 0.685, loss: 2.128 (data_loss: 2.093, reg_loss: 0.035), lr: 0.004999213 validation, acc: 0.790, loss: 1.986 \n",
            "epoch: 316, acc: 0.778, loss: 2.452 (data_loss: 2.417, reg_loss: 0.035), lr: 0.004999210 validation, acc: 0.801, loss: 2.097 \n",
            "epoch: 317, acc: 0.697, loss: 2.810 (data_loss: 2.776, reg_loss: 0.035), lr: 0.004999208 validation, acc: 0.729, loss: 2.297 \n",
            "epoch: 318, acc: 0.635, loss: 3.337 (data_loss: 3.303, reg_loss: 0.035), lr: 0.004999205 validation, acc: 0.733, loss: 2.459 \n",
            "epoch: 319, acc: 0.737, loss: 1.352 (data_loss: 1.317, reg_loss: 0.034), lr: 0.004999203 validation, acc: 0.875, loss: 0.682 \n",
            "epoch: 320, acc: 0.850, loss: 0.969 (data_loss: 0.935, reg_loss: 0.034), lr: 0.004999200 validation, acc: 0.830, loss: 0.828 \n",
            "epoch: 321, acc: 0.759, loss: 1.332 (data_loss: 1.298, reg_loss: 0.034), lr: 0.004999198 validation, acc: 0.846, loss: 0.800 \n",
            "epoch: 322, acc: 0.833, loss: 1.014 (data_loss: 0.980, reg_loss: 0.034), lr: 0.004999195 validation, acc: 0.885, loss: 0.719 \n",
            "epoch: 323, acc: 0.884, loss: 0.920 (data_loss: 0.886, reg_loss: 0.034), lr: 0.004999193 validation, acc: 0.873, loss: 0.761 \n",
            "epoch: 324, acc: 0.870, loss: 0.994 (data_loss: 0.960, reg_loss: 0.034), lr: 0.004999190 validation, acc: 0.890, loss: 0.722 \n",
            "epoch: 325, acc: 0.881, loss: 0.950 (data_loss: 0.916, reg_loss: 0.034), lr: 0.004999188 validation, acc: 0.902, loss: 0.674 \n",
            "epoch: 326, acc: 0.720, loss: 2.197 (data_loss: 2.163, reg_loss: 0.034), lr: 0.004999185 validation, acc: 0.781, loss: 1.373 \n",
            "epoch: 327, acc: 0.766, loss: 1.796 (data_loss: 1.762, reg_loss: 0.034), lr: 0.004999183 validation, acc: 0.807, loss: 1.179 \n",
            "epoch: 328, acc: 0.798, loss: 1.543 (data_loss: 1.509, reg_loss: 0.034), lr: 0.004999180 validation, acc: 0.781, loss: 1.127 \n",
            "epoch: 329, acc: 0.767, loss: 1.454 (data_loss: 1.420, reg_loss: 0.034), lr: 0.004999178 validation, acc: 0.776, loss: 1.071 \n",
            "epoch: 330, acc: 0.765, loss: 1.354 (data_loss: 1.320, reg_loss: 0.034), lr: 0.004999175 validation, acc: 0.819, loss: 0.894 \n",
            "epoch: 331, acc: 0.806, loss: 0.642 (data_loss: 0.608, reg_loss: 0.034), lr: 0.004999173 validation, acc: 0.909, loss: 0.333 \n",
            "epoch: 332, acc: 0.903, loss: 0.402 (data_loss: 0.368, reg_loss: 0.034), lr: 0.004999170 validation, acc: 0.917, loss: 0.249 \n",
            "epoch: 333, acc: 0.908, loss: 0.301 (data_loss: 0.268, reg_loss: 0.033), lr: 0.004999168 validation, acc: 0.935, loss: 0.212 \n",
            "epoch: 334, acc: 0.926, loss: 0.232 (data_loss: 0.198, reg_loss: 0.033), lr: 0.004999165 validation, acc: 0.989, loss: 0.113 \n",
            "epoch: 335, acc: 0.984, loss: 0.125 (data_loss: 0.092, reg_loss: 0.033), lr: 0.004999163 validation, acc: 0.996, loss: 0.077 \n",
            "epoch: 336, acc: 0.997, loss: 0.083 (data_loss: 0.050, reg_loss: 0.033), lr: 0.004999160 validation, acc: 0.997, loss: 0.063 \n",
            "epoch: 337, acc: 0.996, loss: 0.073 (data_loss: 0.041, reg_loss: 0.033), lr: 0.004999158 validation, acc: 0.991, loss: 0.068 \n",
            "epoch: 338, acc: 0.988, loss: 0.101 (data_loss: 0.068, reg_loss: 0.033), lr: 0.004999155 validation, acc: 0.985, loss: 0.076 \n",
            "epoch: 339, acc: 0.982, loss: 0.109 (data_loss: 0.077, reg_loss: 0.032), lr: 0.004999153 validation, acc: 0.991, loss: 0.056 \n",
            "epoch: 340, acc: 0.990, loss: 0.081 (data_loss: 0.049, reg_loss: 0.032), lr: 0.004999150 validation, acc: 0.997, loss: 0.045 \n",
            "epoch: 341, acc: 0.996, loss: 0.060 (data_loss: 0.028, reg_loss: 0.032), lr: 0.004999148 validation, acc: 0.999, loss: 0.042 \n",
            "epoch: 342, acc: 0.999, loss: 0.056 (data_loss: 0.024, reg_loss: 0.031), lr: 0.004999145 validation, acc: 0.999, loss: 0.041 \n",
            "epoch: 343, acc: 0.999, loss: 0.056 (data_loss: 0.025, reg_loss: 0.031), lr: 0.004999143 validation, acc: 0.999, loss: 0.031 \n",
            "epoch: 344, acc: 0.999, loss: 0.048 (data_loss: 0.018, reg_loss: 0.031), lr: 0.004999140 validation, acc: 1.000, loss: 0.023 \n",
            "epoch: 345, acc: 1.000, loss: 0.042 (data_loss: 0.011, reg_loss: 0.030), lr: 0.004999138 validation, acc: 1.000, loss: 0.018 \n",
            "epoch: 346, acc: 1.000, loss: 0.038 (data_loss: 0.008, reg_loss: 0.030), lr: 0.004999135 validation, acc: 1.000, loss: 0.016 \n",
            "epoch: 347, acc: 1.000, loss: 0.036 (data_loss: 0.007, reg_loss: 0.030), lr: 0.004999133 validation, acc: 0.999, loss: 0.015 \n",
            "epoch: 348, acc: 1.000, loss: 0.035 (data_loss: 0.006, reg_loss: 0.029), lr: 0.004999130 validation, acc: 0.998, loss: 0.015 \n",
            "epoch: 349, acc: 1.000, loss: 0.035 (data_loss: 0.006, reg_loss: 0.029), lr: 0.004999128 validation, acc: 0.998, loss: 0.016 \n",
            "epoch: 350, acc: 1.000, loss: 0.034 (data_loss: 0.006, reg_loss: 0.028), lr: 0.004999125 validation, acc: 0.998, loss: 0.017 \n",
            "epoch: 351, acc: 0.999, loss: 0.044 (data_loss: 0.016, reg_loss: 0.028), lr: 0.004999123 validation, acc: 0.998, loss: 0.026 \n",
            "epoch: 352, acc: 0.999, loss: 0.040 (data_loss: 0.013, reg_loss: 0.027), lr: 0.004999120 validation, acc: 0.998, loss: 0.022 \n",
            "epoch: 353, acc: 1.000, loss: 0.037 (data_loss: 0.010, reg_loss: 0.027), lr: 0.004999118 validation, acc: 0.998, loss: 0.019 \n",
            "epoch: 354, acc: 1.000, loss: 0.035 (data_loss: 0.008, reg_loss: 0.026), lr: 0.004999115 validation, acc: 0.998, loss: 0.018 \n",
            "epoch: 355, acc: 1.000, loss: 0.033 (data_loss: 0.007, reg_loss: 0.026), lr: 0.004999113 validation, acc: 0.998, loss: 0.016 \n",
            "epoch: 356, acc: 1.000, loss: 0.032 (data_loss: 0.007, reg_loss: 0.025), lr: 0.004999110 validation, acc: 0.999, loss: 0.014 \n",
            "epoch: 357, acc: 1.000, loss: 0.031 (data_loss: 0.006, reg_loss: 0.025), lr: 0.004999108 validation, acc: 1.000, loss: 0.013 \n",
            "epoch: 358, acc: 1.000, loss: 0.030 (data_loss: 0.005, reg_loss: 0.025), lr: 0.004999105 validation, acc: 0.999, loss: 0.011 \n",
            "epoch: 359, acc: 1.000, loss: 0.028 (data_loss: 0.004, reg_loss: 0.024), lr: 0.004999103 validation, acc: 1.000, loss: 0.010 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNnDUP_U8Xn",
        "outputId": "75199b99-1f6c-4cd7-d5b7-ded8173b4ff3"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7802159722222222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbXMisqQKqF",
        "outputId": "98f583c4-baa3-4701-f26b-fc673aeb73d6"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 15 epochs\n",
            "Model hit 85% validation accuracy in 15 epochs\n",
            "Model hit 90% validation accuracy in 20 epochs\n",
            "Model hit 95% validation accuracy in 24 epochs\n",
            "Model hit 97.5% validation accuracy in 24 epochs\n",
            "Model hit 100% validation accuracy in 344 epochs\n",
            "Max accuracy was 100.0% at epoch 344.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwSXsZVU8Xo",
        "outputId": "3468f1bb-dca7-4486-90f2-7a0c4c644053"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.000, loss: 0.010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k0Ve2M0bPG3"
      },
      "source": [
        "training_diff = []\n",
        "testing_diff = []\n",
        "combined_diff = []"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByL_RwvlIx3"
      },
      "source": [
        "Individual Training Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTOnqnDXa0ME",
        "outputId": "9c1a81bc-127c-4f6e-8908-845bce1d67a7"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x, sorted_y)):\n",
        "  accuracy = Accuracy_Categorical()\n",
        "\n",
        "  y = sorted_y[y_sorted_lists]\n",
        "  X = sorted_x[X_sorted_lists]\n",
        "  accuracy.init(y)\n",
        "\n",
        "  dense1.forward(X)\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "  train_train_mean = activation1.output\n",
        "\n",
        "  dropout1.infrence(activation1.output,y)\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, y)\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, y)\n",
        "  print(f'{number_mnist_labels[classes]} Train Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Train Accuracy: 1.000, loss: 0.014\n",
            "1 Train Accuracy: 1.000, loss: 0.000\n",
            "2 Train Accuracy: 1.000, loss: 0.006\n",
            "3 Train Accuracy: 1.000, loss: 0.010\n",
            "4 Train Accuracy: 1.000, loss: 0.001\n",
            "5 Train Accuracy: 1.000, loss: 0.000\n",
            "6 Train Accuracy: 1.000, loss: 0.039\n",
            "7 Train Accuracy: 1.000, loss: 0.001\n",
            "8 Train Accuracy: 1.000, loss: 0.014\n",
            "9 Train Accuracy: 0.999, loss: 0.008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scjb7Wh_sn6b",
        "outputId": "e2809b1c-e235-4496-af83-ed8514c6dba4"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x_val, sorted_y_val)):\n",
        "  accuracy.init(y_sorted_lists)\n",
        "  #print(sorted_y[y_sorted_lists].shape)\n",
        "  #print(sorted_x[X_sorted_lists].shape)\n",
        "  dense1.forward(sorted_x_val[X_sorted_lists])\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  testmean = np.mean(activation1.output, axis=0)\n",
        "  testing_diff.append(testmean)\n",
        "  dropout1.infrence(activation1.output,sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  print(f'{number_mnist_labels[classes]} Test Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Test Accuracy: 1.000, loss: 0.014\n",
            "1 Test Accuracy: 1.000, loss: 0.000\n",
            "2 Test Accuracy: 1.000, loss: 0.006\n",
            "3 Test Accuracy: 1.000, loss: 0.011\n",
            "4 Test Accuracy: 1.000, loss: 0.001\n",
            "5 Test Accuracy: 1.000, loss: 0.000\n",
            "6 Test Accuracy: 1.000, loss: 0.041\n",
            "7 Test Accuracy: 1.000, loss: 0.002\n",
            "8 Test Accuracy: 1.000, loss: 0.013\n",
            "9 Test Accuracy: 1.000, loss: 0.007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u-O8oNZ0qA"
      },
      "source": [
        "# Full mnist test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbD4KrLMnTcR"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMfBGUHeZ4L5",
        "outputId": "0b216eab-5763-4c68-8c62-321706b25861"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(label)\n",
        "\n",
        "dense1.forward(input)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,label)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, label)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, label)\n",
        "\n",
        "print(f'Found {input.shape[0]} images belonging to {len(set(label))} unique classes')\n",
        "print(f'Full Training Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 60000 images belonging to 10 unique classes\n",
            "Full Training Accuracy: 1.000, loss: 0.009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aat-uVF6nYu7"
      },
      "source": [
        "Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hyU1tBDna8x",
        "outputId": "e8718d8e-a2e4-452f-d507-3c3a6050a1d5"
      },
      "source": [
        "X_val = (X_val.reshape(X_val.shape[0], -1).astype(np.float32) - 127.5) / 127.5 # Reshape X_val if cell below was already ran\n",
        "\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_val)\n",
        "\n",
        "dense1.forward(X_val)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,y_val)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_val)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_val)\n",
        "\n",
        "print(f'Found {X_val.shape[0]} images belonging to {len(set(y_val))} unique classes')\n",
        "print(f'Full Testing Accuracy: {testaccuracy:.5f}, loss: {loss:.3f}')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 10 unique classes\n",
            "Full Testing Accuracy: 1.00000, loss: 0.023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0deRpPodLm04"
      },
      "source": [
        "Change idex to get confidence of different samples of testing data. Index values 0-1600 were refrenced in training. Anything past was never seen during training. Lowest confidence is at index 2118 when trained with 359 epochs and numpy seed set to 22."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "DdktRypGH2Gw",
        "outputId": "ce5e09ef-4b93-470a-85c9-e6cf4181b1a0"
      },
      "source": [
        "index = 2118\n",
        "\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {number_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {number_mnist_labels[y_val[index]]}')\n",
        "\n",
        "X_val.resize(X_val.shape[0],28,28)\n",
        "image = X_val[index]\n",
        "fig = plt.figure\n",
        "plt.title(f'{number_mnist_labels[y_val[index]]}')\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97.257% Confident True is 3. True is actually 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5qoOPZkiDnu",
        "outputId": "18424771-907f-430f-cd73-44996ecea26a"
      },
      "source": [
        "confidence_list = []\n",
        "for index in range(10000):\n",
        "  confidence_list.append(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0])\n",
        "\n",
        "print(confidence_list.index(min(confidence_list)))\n",
        "\n",
        "a = confidence_list[:] \n",
        "a.sort()\n",
        "print(confidence_list.index(a[0]))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2118\n",
            "2118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "h5c5xUTNXk2v",
        "outputId": "88222e0d-6e31-4ce6-a378-3ee357b054fc"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}