{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist numbers implementation of New_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_numbers_implementation_of_New_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOLv3jSjsxV"
      },
      "source": [
        "np.random.seed(seed=22)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX886WxClQ07",
        "outputId": "688419a7-2c8d-4687-fcc7-30950980af75"
      },
      "source": [
        "print(np.random.random(size=3))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20846054 0.48168106 0.42053804]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Og9ZrbKtV"
      },
      "source": [
        "# New Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXBWsHDIUSfh"
      },
      "source": [
        "class Layer_BinaryNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = 1 - rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, inputs, val_inputs):\n",
        "        self.inputs = inputs\n",
        "        self.val_inputs = val_inputs\n",
        "        nummask = round(len(self.inputs[0]) * self.rate)\n",
        "        \n",
        "        #Averaging Values\n",
        "        self.meanarray1 = np.mean(inputs, axis=0)\n",
        "        self.meanarray2 = np.mean(val_inputs, axis=0)\n",
        "\n",
        "        if self.iterations != 0:\n",
        "            # Calculating value\n",
        "            self.difference = self.meanarray1 - self.meanarray2\n",
        "            ind = np.argpartition(self.difference, -nummask)[-nummask:]\n",
        "            mask = np.ones(self.meanarray1.shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            self.difference[~mask] = 1\n",
        "            self.difference[mask] = 0. \n",
        "            self.binary_mask = self.difference / self.rate\n",
        "\n",
        "        else:\n",
        "            self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                                  size=inputs.shape) / self.rate\n",
        "        self.output = inputs * self.binary_mask\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiuCzwWxbRl0"
      },
      "source": [
        "class Layer_CatagoricalNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, X_test, y_test, X, y):        \n",
        "        if self.iterations != 0:\n",
        "          #Sorting data into classes\n",
        "          idx = np.argsort(y_test)\n",
        "          X_test_sorted = X_test[idx]\n",
        "          y_test_sorted = y_test[idx]\n",
        "\n",
        "          idx2 = np.argsort(y)\n",
        "          X_train_sorted = X[idx2]\n",
        "          y_train_sorted = y[idx2]\n",
        "\n",
        "          #Adding sorted data into dictionaries \n",
        "          sorted_x = {}\n",
        "          sorted_y = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x[\"class_{0}\".format(classes)] = X[y == classes]\n",
        "            sorted_y[\"label_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "          sorted_x_test = {}\n",
        "          sorted_y_test = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x_test[\"class_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "            sorted_y_test[\"label_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "          #Averaging sorted data from each class then finding the difference between the averaged train and test inputs\n",
        "          differnce_classes = {}\n",
        "          for i, classes, test_classes in zip(range(len(set(y))), sorted_x, sorted_x_test):\n",
        "            differnce_classes[\"diff_{0}\".format(i)] = np.mean(sorted_x[classes], axis=0) - np.mean(sorted_x_test[classes], axis=0)\n",
        "\n",
        "          #Masking the data taking the high values(greatest difference between train and test) and setting their values to 0\n",
        "          self.diff_mask = {}\n",
        "          for i, classes, test_classes, diff in zip(range(len(set(y))), sorted_x, sorted_x_test, differnce_classes):\n",
        "            ind = np.argpartition(differnce_classes[diff], -round(len(X[0]) * self.rate))[-round(len(X[0]) * self.rate):]\n",
        "            mask = np.ones(np.mean(sorted_x[classes],axis=0).shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            differnce_classes[diff][~mask] = 0.\n",
        "            differnce_classes[diff][mask] = 1\n",
        "            self.diff_mask[\"mask_{0}\".format(i)] = differnce_classes[diff]\n",
        "\n",
        "          #Goes through each input values and applies the apprioprite mask based on what the true output should be.\n",
        "          binary_mask = np.empty(shape=X.shape)\n",
        "          for i, input, label in zip(range(len(X)), X, y):\n",
        "            for true, diff in zip(range(len(set(y))),self.diff_mask):\n",
        "              if label == true:\n",
        "                self.binary_mask[i] = self.diff_mask[diff]\n",
        "        else:\n",
        "          self.binary_mask = np.random.binomial(1, (1-self.rate), size=X.shape)\n",
        "        \n",
        "        self.output = (self.binary_mask/(1-self.rate)) * X\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def infrence(self, input, label):\n",
        "        self.input = input\n",
        "        self.label = label\n",
        "        idx = np.argsort(self.label)\n",
        "        input_sorted = input[idx]\n",
        "        label_sorted = label[idx]\n",
        "        self.ingrence_binary_mask = np.empty(shape=self.input.shape)\n",
        "        for i, input, label in zip(range(len(self.input)), self.input, self.label):\n",
        "          for true, diff in zip(range(len(set(self.label))),self.diff_mask):\n",
        "            if label == true:\n",
        "              self.ingrence_binary_mask[i] = self.diff_mask[diff]\n",
        "\n",
        "        self.output = self.ingrence_binary_mask * self.input\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "0c78f565-0f2e-46b3-d6ff-49ea7afbb8e9"
      },
      "source": [
        "#(X, y), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "# load dataset\n",
        "(X, y), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "'''\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "'''\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: '0',\n",
        "    1: '1',\n",
        "    2: '2',\n",
        "    3: '3',\n",
        "    4: '4',\n",
        "    5: '5',\n",
        "    6: '6',\n",
        "    7: '7',\n",
        "    8: '8',\n",
        "    9: '9'\n",
        "}\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "\n",
        "X = X[:8000,:,:]\n",
        "X_test = X_test[:1600,:,:]\n",
        "y = y[:8000]\n",
        "y_test  = y_test[:1600]\n",
        "\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 784)\n",
            "(8000,)\n",
            "(1600, 784)\n",
            "(1600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nW7mqGTnem"
      },
      "source": [
        "Sorting Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtvA9y81TpGf",
        "outputId": "c92659d7-1f48-4a3b-900a-bb289ba9a7a1"
      },
      "source": [
        "idx = np.argsort(y)\n",
        "X_sorted = X[idx]\n",
        "y_sorted = y[idx]\n",
        "\n",
        "sorted_x = {}\n",
        "sorted_y = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x[\"X_{0}\".format(classes)] = X[y == classes]\n",
        "  sorted_y[\"y_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x[sorted_lists].shape[0]}')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_0: 818\n",
            "Number of Samples for X_1: 866\n",
            "Number of Samples for X_2: 808\n",
            "Number of Samples for X_3: 774\n",
            "Number of Samples for X_4: 768\n",
            "Number of Samples for X_5: 702\n",
            "Number of Samples for X_6: 735\n",
            "Number of Samples for X_7: 891\n",
            "Number of Samples for X_8: 820\n",
            "Number of Samples for X_9: 818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaNaUO3kP2G"
      },
      "source": [
        "Sorting Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBLFeGAUkSOs",
        "outputId": "6a1dc61e-6be8-4231-9b79-3c29d849d708"
      },
      "source": [
        "idx = np.argsort(y_test)\n",
        "X_test_sorted = X_test[idx]\n",
        "y_test_sorted = y_test[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_test = {}\n",
        "sorted_y_test = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_test[\"X_test_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "  sorted_y_test[\"y_test_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_test:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_test[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_test[sorted_lists].shape[0])\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_test_0: 137\n",
            "Number of Samples for X_test_1: 185\n",
            "Number of Samples for X_test_2: 180\n",
            "Number of Samples for X_test_3: 162\n",
            "Number of Samples for X_test_4: 181\n",
            "Number of Samples for X_test_5: 142\n",
            "Number of Samples for X_test_6: 139\n",
            "Number of Samples for X_test_7: 165\n",
            "Number of Samples for X_test_8: 154\n",
            "Number of Samples for X_test_9: 155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbnq4TJp1cl",
        "outputId": "bd0d5ba6-d939-46e8-dcd5-3aa47f0ee171"
      },
      "source": [
        "print(f'Found {X.shape[0]} images belonging to {len(set(y))} unique classes')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 10 unique classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_CatagoricalNSDropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,len(set(y)))\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7,learning_rate=0.005)\n",
        "#optimizer = Optimizer_SGD(learning_rate=0.01)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "8cf4fb07-d67a-4599-dd0e-dc54e59caee0"
      },
      "source": [
        "epochs = 500\n",
        "dips = 0\n",
        "accuracy_count = 0\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    if epoch != 0:\n",
        "      cached_val_inputs = cached_val_inputs\n",
        "      cached_train_inputs = activation1.output\n",
        "    else:\n",
        "      cached_val_inputs = np.random.random(size=128) #Never used just needed to pass to dropout\n",
        "      cached_train_inputs = activation1.output\n",
        "\n",
        "    #print(activation1.output.shape)\n",
        "\n",
        "    #dropout1.forward(inputs=activation1.output, val_inputs=cached_val_inputs)\n",
        "    dropout1.forward(X=activation1.output, y=y, X_test=cached_val_inputs, y_test=y_test)\n",
        "    #print(dropout1.output.shape)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "    dropout1.post_update_params()\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    if epoch == 0:\n",
        "      dense2.forward(activation1.output)\n",
        "    else:\n",
        "      dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "      dense2.forward(dropout1.output)\n",
        "    \n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        "\n",
        "    if valaccuracy >= .999 and train_accuracy >= .998:\n",
        "      break\n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.096, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.005000000 validation, acc: 0.104, loss: 2.302 \n",
            "epoch: 1, acc: 0.111, loss: 2.304 (data_loss: 2.302, reg_loss: 0.002), lr: 0.004999998 validation, acc: 0.103, loss: 2.299 \n",
            "epoch: 2, acc: 0.111, loss: 2.300 (data_loss: 2.299, reg_loss: 0.001), lr: 0.004999995 validation, acc: 0.103, loss: 2.275 \n",
            "epoch: 3, acc: 0.111, loss: 2.274 (data_loss: 2.274, reg_loss: 0.001), lr: 0.004999993 validation, acc: 0.103, loss: 2.203 \n",
            "epoch: 4, acc: 0.111, loss: 2.220 (data_loss: 2.219, reg_loss: 0.001), lr: 0.004999990 validation, acc: 0.103, loss: 2.131 \n",
            "epoch: 5, acc: 0.111, loss: 2.136 (data_loss: 2.135, reg_loss: 0.002), lr: 0.004999988 validation, acc: 0.339, loss: 2.118 \n",
            "epoch: 6, acc: 0.363, loss: 2.107 (data_loss: 2.104, reg_loss: 0.003), lr: 0.004999985 validation, acc: 0.346, loss: 1.995 \n",
            "epoch: 7, acc: 0.301, loss: 2.082 (data_loss: 2.078, reg_loss: 0.003), lr: 0.004999983 validation, acc: 0.319, loss: 1.903 \n",
            "epoch: 8, acc: 0.299, loss: 1.922 (data_loss: 1.918, reg_loss: 0.004), lr: 0.004999980 validation, acc: 0.296, loss: 1.903 \n",
            "epoch: 9, acc: 0.298, loss: 1.913 (data_loss: 1.908, reg_loss: 0.005), lr: 0.004999978 validation, acc: 0.301, loss: 1.745 \n",
            "epoch: 10, acc: 0.311, loss: 1.729 (data_loss: 1.724, reg_loss: 0.006), lr: 0.004999975 validation, acc: 0.433, loss: 1.608 \n",
            "epoch: 11, acc: 0.395, loss: 1.624 (data_loss: 1.618, reg_loss: 0.006), lr: 0.004999973 validation, acc: 0.279, loss: 1.612 \n",
            "epoch: 12, acc: 0.368, loss: 1.627 (data_loss: 1.620, reg_loss: 0.007), lr: 0.004999970 validation, acc: 0.519, loss: 1.523 \n",
            "epoch: 13, acc: 0.416, loss: 1.530 (data_loss: 1.522, reg_loss: 0.008), lr: 0.004999968 validation, acc: 0.427, loss: 1.451 \n",
            "epoch: 14, acc: 0.381, loss: 1.538 (data_loss: 1.529, reg_loss: 0.008), lr: 0.004999965 validation, acc: 0.426, loss: 1.305 \n",
            "epoch: 15, acc: 0.423, loss: 1.511 (data_loss: 1.502, reg_loss: 0.009), lr: 0.004999963 validation, acc: 0.603, loss: 1.197 \n",
            "epoch: 16, acc: 0.597, loss: 1.171 (data_loss: 1.162, reg_loss: 0.010), lr: 0.004999960 validation, acc: 0.420, loss: 1.232 \n",
            "epoch: 17, acc: 0.392, loss: 1.377 (data_loss: 1.367, reg_loss: 0.010), lr: 0.004999958 validation, acc: 0.636, loss: 1.129 \n",
            "epoch: 18, acc: 0.624, loss: 1.111 (data_loss: 1.100, reg_loss: 0.011), lr: 0.004999955 validation, acc: 0.656, loss: 1.135 \n",
            "epoch: 19, acc: 0.662, loss: 1.127 (data_loss: 1.115, reg_loss: 0.011), lr: 0.004999953 validation, acc: 0.704, loss: 1.000 \n",
            "epoch: 20, acc: 0.684, loss: 1.030 (data_loss: 1.018, reg_loss: 0.012), lr: 0.004999950 validation, acc: 0.673, loss: 0.910 \n",
            "epoch: 21, acc: 0.690, loss: 0.921 (data_loss: 0.908, reg_loss: 0.013), lr: 0.004999948 validation, acc: 0.703, loss: 0.844 \n",
            "epoch: 22, acc: 0.743, loss: 0.777 (data_loss: 0.764, reg_loss: 0.013), lr: 0.004999945 validation, acc: 0.821, loss: 0.691 \n",
            "epoch: 23, acc: 0.832, loss: 0.625 (data_loss: 0.611, reg_loss: 0.014), lr: 0.004999943 validation, acc: 0.875, loss: 0.567 \n",
            "epoch: 24, acc: 0.833, loss: 0.570 (data_loss: 0.555, reg_loss: 0.015), lr: 0.004999940 validation, acc: 0.928, loss: 0.514 \n",
            "epoch: 25, acc: 0.757, loss: 0.647 (data_loss: 0.631, reg_loss: 0.015), lr: 0.004999938 validation, acc: 0.774, loss: 0.662 \n",
            "epoch: 26, acc: 0.723, loss: 0.828 (data_loss: 0.813, reg_loss: 0.016), lr: 0.004999935 validation, acc: 0.793, loss: 0.566 \n",
            "epoch: 27, acc: 0.758, loss: 0.786 (data_loss: 0.770, reg_loss: 0.017), lr: 0.004999933 validation, acc: 0.770, loss: 0.715 \n",
            "epoch: 28, acc: 0.717, loss: 0.968 (data_loss: 0.951, reg_loss: 0.017), lr: 0.004999930 validation, acc: 0.821, loss: 0.750 \n",
            "epoch: 29, acc: 0.733, loss: 1.473 (data_loss: 1.455, reg_loss: 0.018), lr: 0.004999928 validation, acc: 0.706, loss: 1.110 \n",
            "epoch: 30, acc: 0.660, loss: 1.582 (data_loss: 1.563, reg_loss: 0.018), lr: 0.004999925 validation, acc: 0.748, loss: 1.598 \n",
            "epoch: 31, acc: 0.748, loss: 1.983 (data_loss: 1.964, reg_loss: 0.019), lr: 0.004999923 validation, acc: 0.745, loss: 1.350 \n",
            "epoch: 32, acc: 0.746, loss: 2.379 (data_loss: 2.359, reg_loss: 0.019), lr: 0.004999920 validation, acc: 0.686, loss: 2.106 \n",
            "epoch: 33, acc: 0.674, loss: 2.505 (data_loss: 2.485, reg_loss: 0.020), lr: 0.004999918 validation, acc: 0.787, loss: 2.091 \n",
            "epoch: 34, acc: 0.750, loss: 2.397 (data_loss: 2.377, reg_loss: 0.020), lr: 0.004999915 validation, acc: 0.690, loss: 1.950 \n",
            "epoch: 35, acc: 0.648, loss: 2.286 (data_loss: 2.266, reg_loss: 0.021), lr: 0.004999913 validation, acc: 0.674, loss: 1.634 \n",
            "epoch: 36, acc: 0.702, loss: 1.815 (data_loss: 1.794, reg_loss: 0.021), lr: 0.004999910 validation, acc: 0.734, loss: 1.802 \n",
            "epoch: 37, acc: 0.714, loss: 1.883 (data_loss: 1.861, reg_loss: 0.021), lr: 0.004999908 validation, acc: 0.720, loss: 1.800 \n",
            "epoch: 38, acc: 0.734, loss: 1.972 (data_loss: 1.950, reg_loss: 0.022), lr: 0.004999905 validation, acc: 0.709, loss: 1.774 \n",
            "epoch: 39, acc: 0.700, loss: 2.030 (data_loss: 2.008, reg_loss: 0.022), lr: 0.004999903 validation, acc: 0.752, loss: 1.451 \n",
            "epoch: 40, acc: 0.627, loss: 1.659 (data_loss: 1.637, reg_loss: 0.022), lr: 0.004999900 validation, acc: 0.657, loss: 1.312 \n",
            "epoch: 41, acc: 0.665, loss: 1.421 (data_loss: 1.398, reg_loss: 0.023), lr: 0.004999898 validation, acc: 0.684, loss: 1.115 \n",
            "epoch: 42, acc: 0.674, loss: 1.093 (data_loss: 1.070, reg_loss: 0.023), lr: 0.004999895 validation, acc: 0.662, loss: 1.079 \n",
            "epoch: 43, acc: 0.617, loss: 1.457 (data_loss: 1.434, reg_loss: 0.023), lr: 0.004999893 validation, acc: 0.817, loss: 1.061 \n",
            "epoch: 44, acc: 0.735, loss: 1.303 (data_loss: 1.280, reg_loss: 0.023), lr: 0.004999890 validation, acc: 0.642, loss: 1.433 \n",
            "epoch: 45, acc: 0.652, loss: 1.526 (data_loss: 1.502, reg_loss: 0.024), lr: 0.004999888 validation, acc: 0.729, loss: 1.433 \n",
            "epoch: 46, acc: 0.743, loss: 1.473 (data_loss: 1.449, reg_loss: 0.024), lr: 0.004999885 validation, acc: 0.703, loss: 1.421 \n",
            "epoch: 47, acc: 0.700, loss: 1.461 (data_loss: 1.437, reg_loss: 0.024), lr: 0.004999883 validation, acc: 0.643, loss: 1.379 \n",
            "epoch: 48, acc: 0.574, loss: 1.830 (data_loss: 1.806, reg_loss: 0.025), lr: 0.004999880 validation, acc: 0.604, loss: 1.532 \n",
            "epoch: 49, acc: 0.624, loss: 1.732 (data_loss: 1.707, reg_loss: 0.025), lr: 0.004999878 validation, acc: 0.665, loss: 1.275 \n",
            "epoch: 50, acc: 0.660, loss: 1.452 (data_loss: 1.427, reg_loss: 0.025), lr: 0.004999875 validation, acc: 0.651, loss: 1.068 \n",
            "epoch: 51, acc: 0.656, loss: 1.393 (data_loss: 1.367, reg_loss: 0.025), lr: 0.004999873 validation, acc: 0.658, loss: 1.275 \n",
            "epoch: 52, acc: 0.663, loss: 1.433 (data_loss: 1.408, reg_loss: 0.025), lr: 0.004999870 validation, acc: 0.677, loss: 1.173 \n",
            "epoch: 53, acc: 0.679, loss: 1.183 (data_loss: 1.158, reg_loss: 0.026), lr: 0.004999868 validation, acc: 0.693, loss: 0.891 \n",
            "epoch: 54, acc: 0.693, loss: 0.985 (data_loss: 0.959, reg_loss: 0.026), lr: 0.004999865 validation, acc: 0.716, loss: 0.845 \n",
            "epoch: 55, acc: 0.710, loss: 1.270 (data_loss: 1.245, reg_loss: 0.026), lr: 0.004999863 validation, acc: 0.769, loss: 1.054 \n",
            "epoch: 56, acc: 0.754, loss: 1.163 (data_loss: 1.137, reg_loss: 0.026), lr: 0.004999860 validation, acc: 0.776, loss: 0.940 \n",
            "epoch: 57, acc: 0.758, loss: 1.257 (data_loss: 1.231, reg_loss: 0.026), lr: 0.004999858 validation, acc: 0.778, loss: 1.010 \n",
            "epoch: 58, acc: 0.763, loss: 1.160 (data_loss: 1.134, reg_loss: 0.026), lr: 0.004999855 validation, acc: 0.792, loss: 0.942 \n",
            "epoch: 59, acc: 0.745, loss: 1.481 (data_loss: 1.455, reg_loss: 0.026), lr: 0.004999853 validation, acc: 0.767, loss: 1.202 \n",
            "epoch: 60, acc: 0.770, loss: 1.396 (data_loss: 1.370, reg_loss: 0.026), lr: 0.004999850 validation, acc: 0.787, loss: 1.123 \n",
            "epoch: 61, acc: 0.766, loss: 1.382 (data_loss: 1.356, reg_loss: 0.026), lr: 0.004999848 validation, acc: 0.782, loss: 1.100 \n",
            "epoch: 62, acc: 0.772, loss: 1.459 (data_loss: 1.433, reg_loss: 0.026), lr: 0.004999845 validation, acc: 0.787, loss: 1.171 \n",
            "epoch: 63, acc: 0.775, loss: 1.416 (data_loss: 1.390, reg_loss: 0.026), lr: 0.004999843 validation, acc: 0.792, loss: 1.130 \n",
            "epoch: 64, acc: 0.782, loss: 1.086 (data_loss: 1.061, reg_loss: 0.025), lr: 0.004999840 validation, acc: 0.797, loss: 0.878 \n",
            "epoch: 65, acc: 0.785, loss: 1.045 (data_loss: 1.020, reg_loss: 0.025), lr: 0.004999838 validation, acc: 0.799, loss: 0.854 \n",
            "epoch: 66, acc: 0.783, loss: 1.016 (data_loss: 0.991, reg_loss: 0.025), lr: 0.004999835 validation, acc: 0.800, loss: 0.809 \n",
            "epoch: 67, acc: 0.786, loss: 0.970 (data_loss: 0.946, reg_loss: 0.025), lr: 0.004999833 validation, acc: 0.803, loss: 0.769 \n",
            "epoch: 68, acc: 0.789, loss: 1.144 (data_loss: 1.119, reg_loss: 0.025), lr: 0.004999830 validation, acc: 0.803, loss: 0.919 \n",
            "epoch: 69, acc: 0.792, loss: 1.114 (data_loss: 1.089, reg_loss: 0.025), lr: 0.004999828 validation, acc: 0.806, loss: 0.890 \n",
            "epoch: 70, acc: 0.729, loss: 1.201 (data_loss: 1.177, reg_loss: 0.024), lr: 0.004999825 validation, acc: 0.792, loss: 1.133 \n",
            "epoch: 71, acc: 0.759, loss: 1.313 (data_loss: 1.289, reg_loss: 0.024), lr: 0.004999823 validation, acc: 0.726, loss: 1.456 \n",
            "epoch: 72, acc: 0.724, loss: 1.796 (data_loss: 1.772, reg_loss: 0.024), lr: 0.004999820 validation, acc: 0.796, loss: 1.149 \n",
            "epoch: 73, acc: 0.767, loss: 1.372 (data_loss: 1.348, reg_loss: 0.024), lr: 0.004999818 validation, acc: 0.758, loss: 1.048 \n",
            "epoch: 74, acc: 0.648, loss: 1.472 (data_loss: 1.448, reg_loss: 0.023), lr: 0.004999815 validation, acc: 0.752, loss: 1.123 \n",
            "epoch: 75, acc: 0.747, loss: 1.039 (data_loss: 1.016, reg_loss: 0.023), lr: 0.004999813 validation, acc: 0.760, loss: 1.011 \n",
            "epoch: 76, acc: 0.715, loss: 1.216 (data_loss: 1.193, reg_loss: 0.023), lr: 0.004999810 validation, acc: 0.799, loss: 0.804 \n",
            "epoch: 77, acc: 0.762, loss: 0.979 (data_loss: 0.956, reg_loss: 0.023), lr: 0.004999808 validation, acc: 0.772, loss: 0.784 \n",
            "epoch: 78, acc: 0.760, loss: 0.846 (data_loss: 0.824, reg_loss: 0.023), lr: 0.004999805 validation, acc: 0.749, loss: 0.765 \n",
            "epoch: 79, acc: 0.688, loss: 0.881 (data_loss: 0.859, reg_loss: 0.023), lr: 0.004999803 validation, acc: 0.766, loss: 0.747 \n",
            "epoch: 80, acc: 0.797, loss: 0.721 (data_loss: 0.699, reg_loss: 0.022), lr: 0.004999800 validation, acc: 0.868, loss: 0.677 \n",
            "epoch: 81, acc: 0.860, loss: 0.655 (data_loss: 0.633, reg_loss: 0.022), lr: 0.004999798 validation, acc: 0.880, loss: 0.651 \n",
            "epoch: 82, acc: 0.783, loss: 0.755 (data_loss: 0.733, reg_loss: 0.022), lr: 0.004999795 validation, acc: 0.883, loss: 0.653 \n",
            "epoch: 83, acc: 0.878, loss: 0.604 (data_loss: 0.581, reg_loss: 0.022), lr: 0.004999793 validation, acc: 0.864, loss: 0.585 \n",
            "epoch: 84, acc: 0.857, loss: 0.577 (data_loss: 0.555, reg_loss: 0.022), lr: 0.004999790 validation, acc: 0.876, loss: 0.503 \n",
            "epoch: 85, acc: 0.874, loss: 0.526 (data_loss: 0.504, reg_loss: 0.022), lr: 0.004999788 validation, acc: 0.884, loss: 0.456 \n",
            "epoch: 86, acc: 0.880, loss: 0.538 (data_loss: 0.515, reg_loss: 0.022), lr: 0.004999785 validation, acc: 0.887, loss: 0.515 \n",
            "epoch: 87, acc: 0.876, loss: 0.535 (data_loss: 0.513, reg_loss: 0.023), lr: 0.004999783 validation, acc: 0.888, loss: 0.442 \n",
            "epoch: 88, acc: 0.872, loss: 0.469 (data_loss: 0.446, reg_loss: 0.023), lr: 0.004999780 validation, acc: 0.886, loss: 0.368 \n",
            "epoch: 89, acc: 0.877, loss: 0.370 (data_loss: 0.347, reg_loss: 0.023), lr: 0.004999778 validation, acc: 0.943, loss: 0.316 \n",
            "epoch: 90, acc: 0.942, loss: 0.296 (data_loss: 0.273, reg_loss: 0.023), lr: 0.004999775 validation, acc: 0.961, loss: 0.242 \n",
            "epoch: 91, acc: 0.954, loss: 0.244 (data_loss: 0.221, reg_loss: 0.023), lr: 0.004999773 validation, acc: 0.980, loss: 0.182 \n",
            "epoch: 92, acc: 0.982, loss: 0.184 (data_loss: 0.160, reg_loss: 0.023), lr: 0.004999770 validation, acc: 0.991, loss: 0.156 \n",
            "epoch: 93, acc: 0.950, loss: 0.262 (data_loss: 0.239, reg_loss: 0.023), lr: 0.004999768 validation, acc: 0.967, loss: 0.223 \n",
            "epoch: 94, acc: 0.973, loss: 0.191 (data_loss: 0.167, reg_loss: 0.024), lr: 0.004999765 validation, acc: 0.979, loss: 0.161 \n",
            "epoch: 95, acc: 0.983, loss: 0.135 (data_loss: 0.112, reg_loss: 0.024), lr: 0.004999763 validation, acc: 0.989, loss: 0.115 \n",
            "epoch: 96, acc: 0.987, loss: 0.108 (data_loss: 0.084, reg_loss: 0.024), lr: 0.004999760 validation, acc: 0.988, loss: 0.106 \n",
            "epoch: 97, acc: 0.962, loss: 0.177 (data_loss: 0.154, reg_loss: 0.024), lr: 0.004999758 validation, acc: 0.988, loss: 0.120 \n",
            "epoch: 98, acc: 0.955, loss: 0.180 (data_loss: 0.156, reg_loss: 0.024), lr: 0.004999755 validation, acc: 0.966, loss: 0.153 \n",
            "epoch: 99, acc: 0.875, loss: 0.915 (data_loss: 0.891, reg_loss: 0.024), lr: 0.004999753 validation, acc: 0.869, loss: 0.889 \n",
            "epoch: 100, acc: 0.877, loss: 0.949 (data_loss: 0.925, reg_loss: 0.024), lr: 0.004999750 validation, acc: 0.876, loss: 0.930 \n",
            "epoch: 101, acc: 0.613, loss: 1.618 (data_loss: 1.594, reg_loss: 0.024), lr: 0.004999748 validation, acc: 0.771, loss: 1.251 \n",
            "epoch: 102, acc: 0.677, loss: 1.976 (data_loss: 1.952, reg_loss: 0.024), lr: 0.004999745 validation, acc: 0.676, loss: 1.912 \n",
            "epoch: 103, acc: 0.697, loss: 2.442 (data_loss: 2.418, reg_loss: 0.024), lr: 0.004999743 validation, acc: 0.684, loss: 2.369 \n",
            "epoch: 104, acc: 0.650, loss: 3.031 (data_loss: 3.007, reg_loss: 0.024), lr: 0.004999740 validation, acc: 0.680, loss: 2.688 \n",
            "epoch: 105, acc: 0.595, loss: 3.246 (data_loss: 3.223, reg_loss: 0.024), lr: 0.004999738 validation, acc: 0.594, loss: 2.851 \n",
            "epoch: 106, acc: 0.617, loss: 3.264 (data_loss: 3.241, reg_loss: 0.024), lr: 0.004999735 validation, acc: 0.577, loss: 2.744 \n",
            "epoch: 107, acc: 0.607, loss: 3.050 (data_loss: 3.026, reg_loss: 0.024), lr: 0.004999733 validation, acc: 0.620, loss: 2.612 \n",
            "epoch: 108, acc: 0.668, loss: 2.776 (data_loss: 2.752, reg_loss: 0.024), lr: 0.004999730 validation, acc: 0.679, loss: 2.190 \n",
            "epoch: 109, acc: 0.698, loss: 2.463 (data_loss: 2.439, reg_loss: 0.024), lr: 0.004999728 validation, acc: 0.755, loss: 1.882 \n",
            "epoch: 110, acc: 0.723, loss: 2.245 (data_loss: 2.221, reg_loss: 0.024), lr: 0.004999725 validation, acc: 0.762, loss: 1.785 \n",
            "epoch: 111, acc: 0.779, loss: 1.933 (data_loss: 1.910, reg_loss: 0.024), lr: 0.004999723 validation, acc: 0.772, loss: 1.672 \n",
            "epoch: 112, acc: 0.789, loss: 1.888 (data_loss: 1.864, reg_loss: 0.024), lr: 0.004999720 validation, acc: 0.762, loss: 1.645 \n",
            "epoch: 113, acc: 0.781, loss: 1.787 (data_loss: 1.763, reg_loss: 0.024), lr: 0.004999718 validation, acc: 0.774, loss: 1.439 \n",
            "epoch: 114, acc: 0.877, loss: 1.404 (data_loss: 1.380, reg_loss: 0.024), lr: 0.004999715 validation, acc: 0.884, loss: 1.230 \n",
            "epoch: 115, acc: 0.883, loss: 1.331 (data_loss: 1.307, reg_loss: 0.024), lr: 0.004999713 validation, acc: 0.884, loss: 1.214 \n",
            "epoch: 116, acc: 0.829, loss: 1.410 (data_loss: 1.385, reg_loss: 0.024), lr: 0.004999710 validation, acc: 0.864, loss: 1.250 \n",
            "epoch: 117, acc: 0.858, loss: 1.491 (data_loss: 1.467, reg_loss: 0.024), lr: 0.004999708 validation, acc: 0.869, loss: 1.352 \n",
            "epoch: 118, acc: 0.776, loss: 1.630 (data_loss: 1.606, reg_loss: 0.024), lr: 0.004999705 validation, acc: 0.868, loss: 1.377 \n",
            "epoch: 119, acc: 0.793, loss: 1.876 (data_loss: 1.852, reg_loss: 0.024), lr: 0.004999703 validation, acc: 0.790, loss: 1.584 \n",
            "epoch: 120, acc: 0.793, loss: 1.690 (data_loss: 1.666, reg_loss: 0.024), lr: 0.004999700 validation, acc: 0.805, loss: 1.386 \n",
            "epoch: 121, acc: 0.824, loss: 1.438 (data_loss: 1.414, reg_loss: 0.024), lr: 0.004999698 validation, acc: 0.850, loss: 1.281 \n",
            "epoch: 122, acc: 0.667, loss: 2.450 (data_loss: 2.426, reg_loss: 0.025), lr: 0.004999695 validation, acc: 0.718, loss: 1.971 \n",
            "epoch: 123, acc: 0.668, loss: 2.369 (data_loss: 2.344, reg_loss: 0.025), lr: 0.004999693 validation, acc: 0.672, loss: 2.003 \n",
            "epoch: 124, acc: 0.697, loss: 2.692 (data_loss: 2.667, reg_loss: 0.025), lr: 0.004999690 validation, acc: 0.681, loss: 2.087 \n",
            "epoch: 125, acc: 0.704, loss: 2.299 (data_loss: 2.274, reg_loss: 0.025), lr: 0.004999688 validation, acc: 0.789, loss: 1.678 \n",
            "epoch: 126, acc: 0.598, loss: 2.450 (data_loss: 2.424, reg_loss: 0.025), lr: 0.004999685 validation, acc: 0.610, loss: 2.161 \n",
            "epoch: 127, acc: 0.603, loss: 2.519 (data_loss: 2.493, reg_loss: 0.026), lr: 0.004999683 validation, acc: 0.683, loss: 1.901 \n",
            "epoch: 128, acc: 0.689, loss: 2.975 (data_loss: 2.949, reg_loss: 0.026), lr: 0.004999680 validation, acc: 0.689, loss: 2.317 \n",
            "epoch: 129, acc: 0.597, loss: 3.176 (data_loss: 3.150, reg_loss: 0.026), lr: 0.004999678 validation, acc: 0.561, loss: 2.571 \n",
            "epoch: 130, acc: 0.578, loss: 3.168 (data_loss: 3.141, reg_loss: 0.027), lr: 0.004999675 validation, acc: 0.561, loss: 2.608 \n",
            "epoch: 131, acc: 0.574, loss: 3.109 (data_loss: 3.082, reg_loss: 0.027), lr: 0.004999673 validation, acc: 0.590, loss: 2.527 \n",
            "epoch: 132, acc: 0.602, loss: 2.871 (data_loss: 2.844, reg_loss: 0.027), lr: 0.004999670 validation, acc: 0.593, loss: 2.359 \n",
            "epoch: 133, acc: 0.599, loss: 2.922 (data_loss: 2.895, reg_loss: 0.027), lr: 0.004999668 validation, acc: 0.592, loss: 2.300 \n",
            "epoch: 134, acc: 0.591, loss: 2.774 (data_loss: 2.747, reg_loss: 0.027), lr: 0.004999665 validation, acc: 0.559, loss: 2.434 \n",
            "epoch: 135, acc: 0.564, loss: 2.677 (data_loss: 2.650, reg_loss: 0.027), lr: 0.004999663 validation, acc: 0.678, loss: 2.228 \n",
            "epoch: 136, acc: 0.617, loss: 2.882 (data_loss: 2.855, reg_loss: 0.027), lr: 0.004999660 validation, acc: 0.660, loss: 2.481 \n",
            "epoch: 137, acc: 0.568, loss: 3.260 (data_loss: 3.233, reg_loss: 0.027), lr: 0.004999658 validation, acc: 0.583, loss: 2.655 \n",
            "epoch: 138, acc: 0.521, loss: 3.188 (data_loss: 3.161, reg_loss: 0.027), lr: 0.004999655 validation, acc: 0.569, loss: 2.503 \n",
            "epoch: 139, acc: 0.580, loss: 3.091 (data_loss: 3.064, reg_loss: 0.027), lr: 0.004999653 validation, acc: 0.710, loss: 2.452 \n",
            "epoch: 140, acc: 0.616, loss: 2.922 (data_loss: 2.895, reg_loss: 0.027), lr: 0.004999650 validation, acc: 0.594, loss: 2.445 \n",
            "epoch: 141, acc: 0.607, loss: 2.969 (data_loss: 2.943, reg_loss: 0.026), lr: 0.004999648 validation, acc: 0.679, loss: 2.391 \n",
            "epoch: 142, acc: 0.680, loss: 2.333 (data_loss: 2.307, reg_loss: 0.026), lr: 0.004999645 validation, acc: 0.706, loss: 1.901 \n",
            "epoch: 143, acc: 0.702, loss: 2.224 (data_loss: 2.198, reg_loss: 0.026), lr: 0.004999643 validation, acc: 0.739, loss: 1.953 \n",
            "epoch: 144, acc: 0.702, loss: 2.182 (data_loss: 2.156, reg_loss: 0.026), lr: 0.004999640 validation, acc: 0.754, loss: 1.830 \n",
            "epoch: 145, acc: 0.761, loss: 1.987 (data_loss: 1.961, reg_loss: 0.026), lr: 0.004999638 validation, acc: 0.759, loss: 1.727 \n",
            "epoch: 146, acc: 0.622, loss: 2.178 (data_loss: 2.152, reg_loss: 0.026), lr: 0.004999635 validation, acc: 0.657, loss: 1.869 \n",
            "epoch: 147, acc: 0.663, loss: 2.047 (data_loss: 2.021, reg_loss: 0.026), lr: 0.004999633 validation, acc: 0.655, loss: 1.838 \n",
            "epoch: 148, acc: 0.660, loss: 2.057 (data_loss: 2.031, reg_loss: 0.026), lr: 0.004999630 validation, acc: 0.713, loss: 1.654 \n",
            "epoch: 149, acc: 0.711, loss: 1.791 (data_loss: 1.765, reg_loss: 0.026), lr: 0.004999628 validation, acc: 0.786, loss: 1.517 \n",
            "epoch: 150, acc: 0.788, loss: 1.679 (data_loss: 1.653, reg_loss: 0.026), lr: 0.004999625 validation, acc: 0.775, loss: 1.478 \n",
            "epoch: 151, acc: 0.671, loss: 2.004 (data_loss: 1.978, reg_loss: 0.026), lr: 0.004999623 validation, acc: 0.740, loss: 1.556 \n",
            "epoch: 152, acc: 0.719, loss: 1.839 (data_loss: 1.813, reg_loss: 0.026), lr: 0.004999620 validation, acc: 0.797, loss: 1.544 \n",
            "epoch: 153, acc: 0.794, loss: 1.691 (data_loss: 1.666, reg_loss: 0.026), lr: 0.004999618 validation, acc: 0.735, loss: 1.760 \n",
            "epoch: 154, acc: 0.734, loss: 2.217 (data_loss: 2.191, reg_loss: 0.026), lr: 0.004999615 validation, acc: 0.759, loss: 2.024 \n",
            "epoch: 155, acc: 0.755, loss: 2.129 (data_loss: 2.103, reg_loss: 0.026), lr: 0.004999613 validation, acc: 0.770, loss: 1.924 \n",
            "epoch: 156, acc: 0.779, loss: 1.996 (data_loss: 1.970, reg_loss: 0.026), lr: 0.004999610 validation, acc: 0.769, loss: 1.845 \n",
            "epoch: 157, acc: 0.774, loss: 1.932 (data_loss: 1.906, reg_loss: 0.026), lr: 0.004999608 validation, acc: 0.788, loss: 1.745 \n",
            "epoch: 158, acc: 0.799, loss: 1.826 (data_loss: 1.800, reg_loss: 0.026), lr: 0.004999605 validation, acc: 0.790, loss: 1.753 \n",
            "epoch: 159, acc: 0.803, loss: 1.782 (data_loss: 1.756, reg_loss: 0.026), lr: 0.004999603 validation, acc: 0.845, loss: 1.787 \n",
            "epoch: 160, acc: 0.848, loss: 1.742 (data_loss: 1.716, reg_loss: 0.026), lr: 0.004999600 validation, acc: 0.870, loss: 1.846 \n",
            "epoch: 161, acc: 0.888, loss: 1.697 (data_loss: 1.671, reg_loss: 0.026), lr: 0.004999598 validation, acc: 0.881, loss: 1.892 \n",
            "epoch: 162, acc: 0.897, loss: 1.668 (data_loss: 1.641, reg_loss: 0.026), lr: 0.004999595 validation, acc: 0.884, loss: 1.883 \n",
            "epoch: 163, acc: 0.900, loss: 1.643 (data_loss: 1.616, reg_loss: 0.026), lr: 0.004999593 validation, acc: 0.884, loss: 1.893 \n",
            "epoch: 164, acc: 0.900, loss: 1.625 (data_loss: 1.599, reg_loss: 0.026), lr: 0.004999590 validation, acc: 0.883, loss: 1.882 \n",
            "epoch: 165, acc: 0.898, loss: 1.616 (data_loss: 1.590, reg_loss: 0.026), lr: 0.004999588 validation, acc: 0.887, loss: 1.857 \n",
            "epoch: 166, acc: 0.882, loss: 1.637 (data_loss: 1.611, reg_loss: 0.026), lr: 0.004999585 validation, acc: 0.885, loss: 1.747 \n",
            "epoch: 167, acc: 0.901, loss: 1.594 (data_loss: 1.569, reg_loss: 0.026), lr: 0.004999583 validation, acc: 0.883, loss: 1.767 \n",
            "epoch: 168, acc: 0.897, loss: 1.623 (data_loss: 1.597, reg_loss: 0.025), lr: 0.004999580 validation, acc: 0.873, loss: 1.783 \n",
            "epoch: 169, acc: 0.890, loss: 1.650 (data_loss: 1.625, reg_loss: 0.025), lr: 0.004999578 validation, acc: 0.881, loss: 1.711 \n",
            "epoch: 170, acc: 0.896, loss: 1.623 (data_loss: 1.598, reg_loss: 0.025), lr: 0.004999575 validation, acc: 0.882, loss: 1.693 \n",
            "epoch: 171, acc: 0.900, loss: 1.603 (data_loss: 1.579, reg_loss: 0.024), lr: 0.004999573 validation, acc: 0.882, loss: 1.774 \n",
            "epoch: 172, acc: 0.899, loss: 1.613 (data_loss: 1.589, reg_loss: 0.024), lr: 0.004999570 validation, acc: 0.882, loss: 1.831 \n",
            "epoch: 173, acc: 0.899, loss: 1.601 (data_loss: 1.577, reg_loss: 0.024), lr: 0.004999568 validation, acc: 0.880, loss: 1.868 \n",
            "epoch: 174, acc: 0.898, loss: 1.599 (data_loss: 1.576, reg_loss: 0.023), lr: 0.004999565 validation, acc: 0.882, loss: 1.860 \n",
            "epoch: 175, acc: 0.900, loss: 1.593 (data_loss: 1.569, reg_loss: 0.023), lr: 0.004999563 validation, acc: 0.884, loss: 1.841 \n",
            "epoch: 176, acc: 0.895, loss: 1.609 (data_loss: 1.587, reg_loss: 0.023), lr: 0.004999560 validation, acc: 0.879, loss: 1.828 \n",
            "epoch: 177, acc: 0.846, loss: 1.755 (data_loss: 1.733, reg_loss: 0.023), lr: 0.004999558 validation, acc: 0.876, loss: 1.867 \n",
            "epoch: 178, acc: 0.889, loss: 1.645 (data_loss: 1.623, reg_loss: 0.022), lr: 0.004999555 validation, acc: 0.819, loss: 2.000 \n",
            "epoch: 179, acc: 0.838, loss: 1.739 (data_loss: 1.717, reg_loss: 0.022), lr: 0.004999553 validation, acc: 0.861, loss: 1.824 \n",
            "epoch: 180, acc: 0.719, loss: 2.337 (data_loss: 2.315, reg_loss: 0.022), lr: 0.004999550 validation, acc: 0.723, loss: 2.172 \n",
            "epoch: 181, acc: 0.698, loss: 2.349 (data_loss: 2.328, reg_loss: 0.022), lr: 0.004999548 validation, acc: 0.798, loss: 1.891 \n",
            "epoch: 182, acc: 0.702, loss: 1.597 (data_loss: 1.575, reg_loss: 0.021), lr: 0.004999545 validation, acc: 0.861, loss: 1.169 \n",
            "epoch: 183, acc: 0.792, loss: 1.424 (data_loss: 1.403, reg_loss: 0.021), lr: 0.004999543 validation, acc: 0.797, loss: 1.386 \n",
            "epoch: 184, acc: 0.762, loss: 1.521 (data_loss: 1.500, reg_loss: 0.021), lr: 0.004999540 validation, acc: 0.841, loss: 1.410 \n",
            "epoch: 185, acc: 0.729, loss: 1.390 (data_loss: 1.369, reg_loss: 0.021), lr: 0.004999538 validation, acc: 0.823, loss: 1.129 \n",
            "epoch: 186, acc: 0.751, loss: 1.240 (data_loss: 1.218, reg_loss: 0.022), lr: 0.004999535 validation, acc: 0.773, loss: 1.017 \n",
            "epoch: 187, acc: 0.767, loss: 1.057 (data_loss: 1.035, reg_loss: 0.022), lr: 0.004999533 validation, acc: 0.759, loss: 0.957 \n",
            "epoch: 188, acc: 0.697, loss: 1.332 (data_loss: 1.310, reg_loss: 0.022), lr: 0.004999530 validation, acc: 0.706, loss: 1.055 \n",
            "epoch: 189, acc: 0.712, loss: 1.128 (data_loss: 1.106, reg_loss: 0.022), lr: 0.004999528 validation, acc: 0.756, loss: 0.878 \n",
            "epoch: 190, acc: 0.745, loss: 1.243 (data_loss: 1.221, reg_loss: 0.022), lr: 0.004999525 validation, acc: 0.772, loss: 1.047 \n",
            "epoch: 191, acc: 0.775, loss: 1.143 (data_loss: 1.121, reg_loss: 0.023), lr: 0.004999523 validation, acc: 0.780, loss: 0.956 \n",
            "epoch: 192, acc: 0.790, loss: 1.034 (data_loss: 1.011, reg_loss: 0.023), lr: 0.004999520 validation, acc: 0.782, loss: 0.932 \n",
            "epoch: 193, acc: 0.792, loss: 0.595 (data_loss: 0.572, reg_loss: 0.023), lr: 0.004999518 validation, acc: 0.859, loss: 0.472 \n",
            "epoch: 194, acc: 0.721, loss: 0.921 (data_loss: 0.898, reg_loss: 0.023), lr: 0.004999515 validation, acc: 0.757, loss: 0.808 \n",
            "epoch: 195, acc: 0.722, loss: 1.008 (data_loss: 0.984, reg_loss: 0.024), lr: 0.004999513 validation, acc: 0.684, loss: 0.900 \n",
            "epoch: 196, acc: 0.680, loss: 0.976 (data_loss: 0.952, reg_loss: 0.024), lr: 0.004999510 validation, acc: 0.692, loss: 0.848 \n",
            "epoch: 197, acc: 0.707, loss: 0.905 (data_loss: 0.880, reg_loss: 0.024), lr: 0.004999508 validation, acc: 0.753, loss: 0.764 \n",
            "epoch: 198, acc: 0.756, loss: 0.806 (data_loss: 0.781, reg_loss: 0.024), lr: 0.004999505 validation, acc: 0.857, loss: 0.652 \n",
            "epoch: 199, acc: 0.746, loss: 0.985 (data_loss: 0.961, reg_loss: 0.024), lr: 0.004999503 validation, acc: 0.792, loss: 0.826 \n",
            "epoch: 200, acc: 0.685, loss: 1.098 (data_loss: 1.073, reg_loss: 0.024), lr: 0.004999500 validation, acc: 0.766, loss: 0.837 \n",
            "epoch: 201, acc: 0.652, loss: 1.470 (data_loss: 1.445, reg_loss: 0.025), lr: 0.004999498 validation, acc: 0.664, loss: 1.192 \n",
            "epoch: 202, acc: 0.605, loss: 1.460 (data_loss: 1.435, reg_loss: 0.025), lr: 0.004999495 validation, acc: 0.634, loss: 1.039 \n",
            "epoch: 203, acc: 0.627, loss: 1.094 (data_loss: 1.069, reg_loss: 0.025), lr: 0.004999493 validation, acc: 0.783, loss: 0.744 \n",
            "epoch: 204, acc: 0.683, loss: 0.873 (data_loss: 0.848, reg_loss: 0.025), lr: 0.004999490 validation, acc: 0.818, loss: 0.650 \n",
            "epoch: 205, acc: 0.792, loss: 0.881 (data_loss: 0.856, reg_loss: 0.025), lr: 0.004999488 validation, acc: 0.873, loss: 0.625 \n",
            "epoch: 206, acc: 0.867, loss: 0.668 (data_loss: 0.643, reg_loss: 0.025), lr: 0.004999485 validation, acc: 0.888, loss: 0.496 \n",
            "epoch: 207, acc: 0.864, loss: 0.508 (data_loss: 0.483, reg_loss: 0.025), lr: 0.004999483 validation, acc: 0.906, loss: 0.423 \n",
            "epoch: 208, acc: 0.904, loss: 0.372 (data_loss: 0.347, reg_loss: 0.025), lr: 0.004999480 validation, acc: 0.972, loss: 0.307 \n",
            "epoch: 209, acc: 0.976, loss: 0.264 (data_loss: 0.239, reg_loss: 0.026), lr: 0.004999478 validation, acc: 0.973, loss: 0.258 \n",
            "epoch: 210, acc: 0.823, loss: 0.520 (data_loss: 0.494, reg_loss: 0.026), lr: 0.004999475 validation, acc: 0.822, loss: 0.451 \n",
            "epoch: 211, acc: 0.812, loss: 0.467 (data_loss: 0.441, reg_loss: 0.026), lr: 0.004999473 validation, acc: 0.892, loss: 0.366 \n",
            "epoch: 212, acc: 0.895, loss: 0.370 (data_loss: 0.344, reg_loss: 0.026), lr: 0.004999470 validation, acc: 0.861, loss: 0.403 \n",
            "epoch: 213, acc: 0.870, loss: 0.405 (data_loss: 0.379, reg_loss: 0.026), lr: 0.004999468 validation, acc: 0.876, loss: 0.338 \n",
            "epoch: 214, acc: 0.800, loss: 0.491 (data_loss: 0.465, reg_loss: 0.026), lr: 0.004999465 validation, acc: 0.880, loss: 0.312 \n",
            "epoch: 215, acc: 0.876, loss: 0.315 (data_loss: 0.288, reg_loss: 0.026), lr: 0.004999463 validation, acc: 0.959, loss: 0.186 \n",
            "epoch: 216, acc: 0.914, loss: 0.265 (data_loss: 0.238, reg_loss: 0.026), lr: 0.004999460 validation, acc: 0.966, loss: 0.175 \n",
            "epoch: 217, acc: 0.866, loss: 0.294 (data_loss: 0.268, reg_loss: 0.027), lr: 0.004999458 validation, acc: 0.946, loss: 0.214 \n",
            "epoch: 218, acc: 0.851, loss: 0.655 (data_loss: 0.628, reg_loss: 0.027), lr: 0.004999455 validation, acc: 0.877, loss: 0.607 \n",
            "epoch: 219, acc: 0.856, loss: 0.753 (data_loss: 0.726, reg_loss: 0.027), lr: 0.004999453 validation, acc: 0.881, loss: 0.826 \n",
            "epoch: 220, acc: 0.859, loss: 1.040 (data_loss: 1.013, reg_loss: 0.027), lr: 0.004999450 validation, acc: 0.863, loss: 0.979 \n",
            "epoch: 221, acc: 0.824, loss: 1.280 (data_loss: 1.253, reg_loss: 0.027), lr: 0.004999448 validation, acc: 0.882, loss: 0.663 \n",
            "epoch: 222, acc: 0.880, loss: 0.833 (data_loss: 0.806, reg_loss: 0.027), lr: 0.004999445 validation, acc: 0.868, loss: 0.610 \n",
            "epoch: 223, acc: 0.874, loss: 0.758 (data_loss: 0.731, reg_loss: 0.027), lr: 0.004999443 validation, acc: 0.885, loss: 0.557 \n",
            "epoch: 224, acc: 0.886, loss: 0.703 (data_loss: 0.676, reg_loss: 0.027), lr: 0.004999440 validation, acc: 0.891, loss: 0.620 \n",
            "epoch: 225, acc: 0.875, loss: 0.810 (data_loss: 0.784, reg_loss: 0.027), lr: 0.004999438 validation, acc: 0.880, loss: 0.695 \n",
            "epoch: 226, acc: 0.872, loss: 0.889 (data_loss: 0.862, reg_loss: 0.027), lr: 0.004999435 validation, acc: 0.897, loss: 0.564 \n",
            "epoch: 227, acc: 0.890, loss: 0.732 (data_loss: 0.705, reg_loss: 0.026), lr: 0.004999433 validation, acc: 0.893, loss: 0.522 \n",
            "epoch: 228, acc: 0.889, loss: 0.667 (data_loss: 0.641, reg_loss: 0.026), lr: 0.004999430 validation, acc: 0.887, loss: 0.560 \n",
            "epoch: 229, acc: 0.864, loss: 0.762 (data_loss: 0.736, reg_loss: 0.026), lr: 0.004999428 validation, acc: 0.885, loss: 0.562 \n",
            "epoch: 230, acc: 0.888, loss: 0.675 (data_loss: 0.650, reg_loss: 0.025), lr: 0.004999425 validation, acc: 0.893, loss: 0.600 \n",
            "epoch: 231, acc: 0.889, loss: 0.748 (data_loss: 0.723, reg_loss: 0.025), lr: 0.004999423 validation, acc: 0.881, loss: 0.591 \n",
            "epoch: 232, acc: 0.831, loss: 0.832 (data_loss: 0.807, reg_loss: 0.025), lr: 0.004999420 validation, acc: 0.901, loss: 0.549 \n",
            "epoch: 233, acc: 0.892, loss: 0.708 (data_loss: 0.683, reg_loss: 0.025), lr: 0.004999418 validation, acc: 0.901, loss: 0.504 \n",
            "epoch: 234, acc: 0.894, loss: 0.625 (data_loss: 0.601, reg_loss: 0.024), lr: 0.004999415 validation, acc: 0.900, loss: 0.434 \n",
            "epoch: 235, acc: 0.893, loss: 0.536 (data_loss: 0.512, reg_loss: 0.024), lr: 0.004999413 validation, acc: 0.898, loss: 0.395 \n",
            "epoch: 236, acc: 0.891, loss: 0.699 (data_loss: 0.675, reg_loss: 0.024), lr: 0.004999410 validation, acc: 0.901, loss: 0.555 \n",
            "epoch: 237, acc: 0.894, loss: 0.700 (data_loss: 0.677, reg_loss: 0.024), lr: 0.004999408 validation, acc: 0.902, loss: 0.528 \n",
            "epoch: 238, acc: 0.894, loss: 0.674 (data_loss: 0.650, reg_loss: 0.023), lr: 0.004999405 validation, acc: 0.902, loss: 0.481 \n",
            "epoch: 239, acc: 0.893, loss: 0.606 (data_loss: 0.583, reg_loss: 0.023), lr: 0.004999403 validation, acc: 0.902, loss: 0.447 \n",
            "epoch: 240, acc: 0.894, loss: 0.558 (data_loss: 0.535, reg_loss: 0.023), lr: 0.004999400 validation, acc: 0.902, loss: 0.415 \n",
            "epoch: 241, acc: 0.896, loss: 0.511 (data_loss: 0.488, reg_loss: 0.023), lr: 0.004999398 validation, acc: 0.902, loss: 0.388 \n",
            "epoch: 242, acc: 0.869, loss: 0.525 (data_loss: 0.503, reg_loss: 0.022), lr: 0.004999395 validation, acc: 0.901, loss: 0.382 \n",
            "epoch: 243, acc: 0.895, loss: 0.456 (data_loss: 0.434, reg_loss: 0.022), lr: 0.004999393 validation, acc: 0.899, loss: 0.369 \n",
            "epoch: 244, acc: 0.876, loss: 0.488 (data_loss: 0.466, reg_loss: 0.021), lr: 0.004999390 validation, acc: 0.901, loss: 0.339 \n",
            "epoch: 245, acc: 0.895, loss: 0.386 (data_loss: 0.365, reg_loss: 0.021), lr: 0.004999388 validation, acc: 0.902, loss: 0.266 \n",
            "epoch: 246, acc: 0.894, loss: 0.315 (data_loss: 0.294, reg_loss: 0.021), lr: 0.004999385 validation, acc: 0.942, loss: 0.227 \n",
            "epoch: 247, acc: 0.934, loss: 0.233 (data_loss: 0.213, reg_loss: 0.020), lr: 0.004999383 validation, acc: 0.990, loss: 0.194 \n",
            "epoch: 248, acc: 0.990, loss: 0.172 (data_loss: 0.152, reg_loss: 0.020), lr: 0.004999380 validation, acc: 0.988, loss: 0.155 \n",
            "epoch: 249, acc: 0.997, loss: 0.119 (data_loss: 0.099, reg_loss: 0.020), lr: 0.004999378 validation, acc: 0.994, loss: 0.115 \n",
            "epoch: 250, acc: 0.995, loss: 0.095 (data_loss: 0.076, reg_loss: 0.019), lr: 0.004999375 validation, acc: 0.999, loss: 0.086 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNnDUP_U8Xn",
        "outputId": "60fc59ba-0851-4ece-ee64-2674c2c414bc"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7426463293650793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbXMisqQKqF",
        "outputId": "92fb6791-50e6-478f-e6f4-a181cad44936"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 22 epochs\n",
            "Model hit 85% validation accuracy in 23 epochs\n",
            "Model hit 90% validation accuracy in 24 epochs\n",
            "Model hit 95% validation accuracy in 90 epochs\n",
            "Model hit 97.5% validation accuracy in 91 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwSXsZVU8Xo",
        "outputId": "65cf6b0a-84db-497d-f54c-3d9c9ec04a02"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "test_train_mean = activation1.output\n",
        "print(test_train_mean.shape)\n",
        "\n",
        "dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'training, acc: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600, 128)\n",
            "training, acc: 0.999, loss: 0.065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmp-61GRtNcD",
        "outputId": "13e77e47-7b98-4f2d-cf35-23f4c4454e13"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)\n",
        "\n",
        "dense1.forward(X)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,y)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y)\n",
        "\n",
        "print(f'training, acc: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training, acc: 0.999, loss: 0.072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k0Ve2M0bPG3"
      },
      "source": [
        "training_diff = []\n",
        "testing_diff = []\n",
        "combined_diff = []"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByL_RwvlIx3"
      },
      "source": [
        "Individual Training Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTOnqnDXa0ME",
        "outputId": "6ec7e3c4-2b2c-4d48-cd12-c42822b8b7fc"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x, sorted_y)):\n",
        "  accuracy = Accuracy_Categorical()\n",
        "\n",
        "  y = sorted_y[y_sorted_lists]\n",
        "  X = sorted_x[X_sorted_lists]\n",
        "  accuracy.init(y)\n",
        "\n",
        "  dense1.forward(X)\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "  train_train_mean = activation1.output\n",
        "\n",
        "  dropout1.infrence(activation1.output,y)\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, y)\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, y)\n",
        "  print(f'{fashion_mnist_labels[classes]} Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 1.000, loss: 0.024\n",
            "1 Accuracy: 0.000, loss: 15.364\n",
            "2 Accuracy: 0.038, loss: 15.160\n",
            "3 Accuracy: 0.000, loss: 16.102\n",
            "4 Accuracy: 0.000, loss: 16.118\n",
            "5 Accuracy: 0.717, loss: 3.644\n",
            "6 Accuracy: 0.253, loss: 6.175\n",
            "7 Accuracy: 0.019, loss: 10.877\n",
            "8 Accuracy: 0.000, loss: 16.118\n",
            "9 Accuracy: 0.000, loss: 16.118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXDmVgdrlNit"
      },
      "source": [
        "Individual Testing Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd-bplUflP1o",
        "outputId": "1a7d7716-4a86-4867-d13a-7be9e920524a"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x_test, sorted_y_test)):\n",
        "  accuracy.init(y_sorted_lists)\n",
        "  #print(sorted_y[y_sorted_lists].shape)\n",
        "  #print(sorted_x[X_sorted_lists].shape)\n",
        "  dense1.forward(sorted_x_test[X_sorted_lists])\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  testmean = np.mean(activation1.output, axis=0)\n",
        "  testing_diff.append(testmean)\n",
        "  dropout1.infrence(activation1.output,sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  print(f'{fashion_mnist_labels[classes]} Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 1.000, loss: 0.024\n",
            "1 Accuracy: 0.000, loss: 2.508\n",
            "2 Accuracy: 0.000, loss: 16.118\n",
            "3 Accuracy: 0.049, loss: nan\n",
            "4 Accuracy: 0.000, loss: nan\n",
            "5 Accuracy: 0.007, loss: 16.005\n",
            "6 Accuracy: 0.000, loss: 16.118\n",
            "7 Accuracy: 0.030, loss: nan\n",
            "8 Accuracy: 0.513, loss: 4.198\n",
            "9 Accuracy: 0.000, loss: 16.118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: RuntimeWarning: overflow encountered in multiply\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u-O8oNZ0qA"
      },
      "source": [
        "# Full mnist test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbD4KrLMnTcR"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMfBGUHeZ4L5",
        "outputId": "d1ec7882-041d-43ce-dd19-4f1b41282aa5"
      },
      "source": [
        "(input, label), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(input.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "input = input[keys]\n",
        "label = label[keys]\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "input = (input.reshape(input.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "\n",
        "\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(label)\n",
        "\n",
        "dense1.forward(input)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,label)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, label)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, label)\n",
        "\n",
        "print(f'training, acc: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training, acc: 0.999, loss: 0.071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aat-uVF6nYu7"
      },
      "source": [
        "Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hyU1tBDna8x",
        "outputId": "1480df86-da00-414d-91de-6ffa1d2953a2"
      },
      "source": [
        "(input, label), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(input.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "input = input[keys]\n",
        "label = label[keys]\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "input = (input.reshape(input.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "\n",
        "\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'training, acc: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training, acc: 0.999, loss: 0.069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "h5c5xUTNXk2v",
        "outputId": "f12f84ff-da58-4e38-fdd8-ee46c410396e"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}