{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " mnist implementation of Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_implementation_of_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOLv3jSjsxV"
      },
      "source": [
        "np.random.seed(seed=50)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX886WxClQ07",
        "outputId": "dcbded2a-30d7-4d9f-8923-56558c144c2e"
      },
      "source": [
        "print(np.random.random(size=3))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.49460165 0.2280831  0.25547392]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Og9ZrbKtV"
      },
      "source": [
        "# New Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiuCzwWxbRl0"
      },
      "source": [
        "class Layer_NewDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = 1 - rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, inputs, val_inputs):\n",
        "        self.inputs = inputs\n",
        "        self.val_inputs = val_inputs\n",
        "        #print(len(self.inputs[0]))\n",
        "        nummask = round(len(self.inputs[0]) * self.rate)\n",
        "        \n",
        "        #Averaging Values\n",
        "        self.meanarray1 = np.mean(inputs, axis=0)\n",
        "        self.meanarray2 = np.mean(val_inputs, axis=0)\n",
        "\n",
        "        #if self.iterations % 10 and self.iterations != 0:\n",
        "        if self.iterations != 0:\n",
        "            # Calculating value\n",
        "            #print(self.iterations)\n",
        "            self.difference = self.meanarray1 - self.meanarray2\n",
        "            #print(self.difference)\n",
        "            ind = np.argpartition(self.difference, -nummask)[-nummask:]\n",
        "            #print(ind)\n",
        "            mask = np.ones(self.meanarray1.shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            #print(ind)\n",
        "            self.difference[~mask] = 1\n",
        "            self.difference[mask] = np.random.random(1)[0]\n",
        "            #print(self.difference / self.rate)\n",
        "            self.binary_mask = self.difference #/ self.rate\n",
        "            #print(self.binary_mask)\n",
        "        else:\n",
        "            self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                                  size=inputs.shape) #/ self.rate\n",
        "            #print(self.binary_mask)\n",
        "        self.output = inputs * self.binary_mask\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUEiUOYDrL89"
      },
      "source": [
        "# Creating Spiral Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "0236a891-6336-417e-d76a-9a996f8eac40"
      },
      "source": [
        "(X, y), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n",
            "(10000, 784)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_Dropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,10)\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "edfb3e15-a708-4c68-8c58-50d8aacb34d7"
      },
      "source": [
        "epochs = 500\n",
        "dips = 0\n",
        "accuracy_count = 0\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    if epoch != 0:\n",
        "      cached_val_inputs = cached_val_inputs\n",
        "      cached_train_inputs = activation1.output\n",
        "    else:\n",
        "      cached_val_inputs = np.random.random(size=128) #Never used\n",
        "      cached_train_inputs = activation1.output\n",
        "\n",
        "    dropout1.forward(activation1.output)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        " \n",
        "\n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.100, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.020000000 validation, acc: 0.100, loss: 2.304 \n",
            "epoch: 1, acc: 0.100, loss: 2.312 (data_loss: 2.304, reg_loss: 0.008), lr: 0.019999990 validation, acc: 0.100, loss: 2.418 \n",
            "epoch: 2, acc: 0.100, loss: 2.436 (data_loss: 2.425, reg_loss: 0.011), lr: 0.019999980 validation, acc: 0.147, loss: 2.335 \n",
            "epoch: 3, acc: 0.134, loss: 2.348 (data_loss: 2.335, reg_loss: 0.013), lr: 0.019999970 validation, acc: 0.230, loss: 2.184 \n",
            "epoch: 4, acc: 0.204, loss: 2.202 (data_loss: 2.186, reg_loss: 0.016), lr: 0.019999960 validation, acc: 0.241, loss: 1.929 \n",
            "epoch: 5, acc: 0.231, loss: 1.967 (data_loss: 1.948, reg_loss: 0.019), lr: 0.019999950 validation, acc: 0.256, loss: 1.772 \n",
            "epoch: 6, acc: 0.255, loss: 1.830 (data_loss: 1.808, reg_loss: 0.022), lr: 0.019999940 validation, acc: 0.144, loss: 2.961 \n",
            "epoch: 7, acc: 0.148, loss: 3.014 (data_loss: 2.990, reg_loss: 0.024), lr: 0.019999930 validation, acc: 0.229, loss: 2.083 \n",
            "epoch: 8, acc: 0.227, loss: 2.136 (data_loss: 2.110, reg_loss: 0.026), lr: 0.019999920 validation, acc: 0.198, loss: 2.009 \n",
            "epoch: 9, acc: 0.191, loss: 2.058 (data_loss: 2.028, reg_loss: 0.029), lr: 0.019999910 validation, acc: 0.343, loss: 1.885 \n",
            "epoch: 10, acc: 0.324, loss: 1.933 (data_loss: 1.901, reg_loss: 0.033), lr: 0.019999900 validation, acc: 0.298, loss: 1.817 \n",
            "epoch: 11, acc: 0.298, loss: 1.882 (data_loss: 1.845, reg_loss: 0.037), lr: 0.019999890 validation, acc: 0.323, loss: 1.756 \n",
            "epoch: 12, acc: 0.320, loss: 1.839 (data_loss: 1.796, reg_loss: 0.042), lr: 0.019999880 validation, acc: 0.383, loss: 1.704 \n",
            "epoch: 13, acc: 0.363, loss: 1.772 (data_loss: 1.725, reg_loss: 0.047), lr: 0.019999870 validation, acc: 0.375, loss: 1.585 \n",
            "epoch: 14, acc: 0.368, loss: 1.704 (data_loss: 1.653, reg_loss: 0.051), lr: 0.019999860 validation, acc: 0.415, loss: 1.490 \n",
            "epoch: 15, acc: 0.397, loss: 1.587 (data_loss: 1.533, reg_loss: 0.054), lr: 0.019999850 validation, acc: 0.393, loss: 1.454 \n",
            "epoch: 16, acc: 0.385, loss: 1.554 (data_loss: 1.498, reg_loss: 0.057), lr: 0.019999840 validation, acc: 0.459, loss: 1.484 \n",
            "epoch: 17, acc: 0.435, loss: 1.577 (data_loss: 1.518, reg_loss: 0.058), lr: 0.019999830 validation, acc: 0.541, loss: 1.356 \n",
            "epoch: 18, acc: 0.521, loss: 1.445 (data_loss: 1.386, reg_loss: 0.059), lr: 0.019999820 validation, acc: 0.560, loss: 1.248 \n",
            "epoch: 19, acc: 0.538, loss: 1.352 (data_loss: 1.293, reg_loss: 0.059), lr: 0.019999810 validation, acc: 0.599, loss: 1.124 \n",
            "epoch: 20, acc: 0.583, loss: 1.227 (data_loss: 1.168, reg_loss: 0.059), lr: 0.019999800 validation, acc: 0.622, loss: 1.078 \n",
            "epoch: 21, acc: 0.585, loss: 1.184 (data_loss: 1.126, reg_loss: 0.059), lr: 0.019999790 validation, acc: 0.626, loss: 1.095 \n",
            "epoch: 22, acc: 0.608, loss: 1.198 (data_loss: 1.140, reg_loss: 0.058), lr: 0.019999780 validation, acc: 0.603, loss: 1.005 \n",
            "epoch: 23, acc: 0.597, loss: 1.098 (data_loss: 1.041, reg_loss: 0.058), lr: 0.019999770 validation, acc: 0.680, loss: 0.888 \n",
            "epoch: 24, acc: 0.664, loss: 0.991 (data_loss: 0.934, reg_loss: 0.057), lr: 0.019999760 validation, acc: 0.666, loss: 0.919 \n",
            "epoch: 25, acc: 0.646, loss: 1.024 (data_loss: 0.967, reg_loss: 0.057), lr: 0.019999750 validation, acc: 0.611, loss: 1.172 \n",
            "epoch: 26, acc: 0.593, loss: 1.300 (data_loss: 1.244, reg_loss: 0.056), lr: 0.019999740 validation, acc: 0.613, loss: 1.080 \n",
            "epoch: 27, acc: 0.600, loss: 1.188 (data_loss: 1.132, reg_loss: 0.056), lr: 0.019999730 validation, acc: 0.673, loss: 0.960 \n",
            "epoch: 28, acc: 0.661, loss: 1.054 (data_loss: 0.998, reg_loss: 0.057), lr: 0.019999720 validation, acc: 0.689, loss: 0.876 \n",
            "epoch: 29, acc: 0.680, loss: 0.955 (data_loss: 0.898, reg_loss: 0.057), lr: 0.019999710 validation, acc: 0.700, loss: 0.872 \n",
            "epoch: 30, acc: 0.685, loss: 0.963 (data_loss: 0.905, reg_loss: 0.057), lr: 0.019999700 validation, acc: 0.712, loss: 0.827 \n",
            "epoch: 31, acc: 0.690, loss: 0.930 (data_loss: 0.872, reg_loss: 0.058), lr: 0.019999690 validation, acc: 0.734, loss: 0.762 \n",
            "epoch: 32, acc: 0.720, loss: 0.861 (data_loss: 0.803, reg_loss: 0.058), lr: 0.019999680 validation, acc: 0.727, loss: 0.747 \n",
            "epoch: 33, acc: 0.719, loss: 0.835 (data_loss: 0.777, reg_loss: 0.058), lr: 0.019999670 validation, acc: 0.747, loss: 0.709 \n",
            "epoch: 34, acc: 0.738, loss: 0.798 (data_loss: 0.740, reg_loss: 0.058), lr: 0.019999660 validation, acc: 0.750, loss: 0.695 \n",
            "epoch: 35, acc: 0.741, loss: 0.789 (data_loss: 0.732, reg_loss: 0.057), lr: 0.019999650 validation, acc: 0.756, loss: 0.697 \n",
            "epoch: 36, acc: 0.742, loss: 0.790 (data_loss: 0.733, reg_loss: 0.056), lr: 0.019999640 validation, acc: 0.758, loss: 0.671 \n",
            "epoch: 37, acc: 0.752, loss: 0.751 (data_loss: 0.696, reg_loss: 0.055), lr: 0.019999630 validation, acc: 0.737, loss: 0.698 \n",
            "epoch: 38, acc: 0.729, loss: 0.779 (data_loss: 0.725, reg_loss: 0.054), lr: 0.019999620 validation, acc: 0.708, loss: 0.817 \n",
            "epoch: 39, acc: 0.709, loss: 0.894 (data_loss: 0.842, reg_loss: 0.052), lr: 0.019999610 validation, acc: 0.747, loss: 0.745 \n",
            "epoch: 40, acc: 0.736, loss: 0.835 (data_loss: 0.784, reg_loss: 0.052), lr: 0.019999600 validation, acc: 0.742, loss: 0.756 \n",
            "epoch: 41, acc: 0.733, loss: 0.844 (data_loss: 0.792, reg_loss: 0.052), lr: 0.019999590 validation, acc: 0.741, loss: 0.728 \n",
            "epoch: 42, acc: 0.731, loss: 0.810 (data_loss: 0.758, reg_loss: 0.052), lr: 0.019999580 validation, acc: 0.755, loss: 0.694 \n",
            "epoch: 43, acc: 0.743, loss: 0.792 (data_loss: 0.739, reg_loss: 0.052), lr: 0.019999570 validation, acc: 0.761, loss: 0.686 \n",
            "epoch: 44, acc: 0.753, loss: 0.775 (data_loss: 0.722, reg_loss: 0.052), lr: 0.019999560 validation, acc: 0.764, loss: 0.672 \n",
            "epoch: 45, acc: 0.753, loss: 0.751 (data_loss: 0.699, reg_loss: 0.052), lr: 0.019999550 validation, acc: 0.782, loss: 0.646 \n",
            "epoch: 46, acc: 0.773, loss: 0.721 (data_loss: 0.669, reg_loss: 0.051), lr: 0.019999540 validation, acc: 0.775, loss: 0.633 \n",
            "epoch: 47, acc: 0.769, loss: 0.714 (data_loss: 0.663, reg_loss: 0.051), lr: 0.019999530 validation, acc: 0.788, loss: 0.596 \n",
            "epoch: 48, acc: 0.779, loss: 0.676 (data_loss: 0.626, reg_loss: 0.050), lr: 0.019999520 validation, acc: 0.785, loss: 0.589 \n",
            "epoch: 49, acc: 0.783, loss: 0.667 (data_loss: 0.619, reg_loss: 0.048), lr: 0.019999510 validation, acc: 0.797, loss: 0.570 \n",
            "epoch: 50, acc: 0.791, loss: 0.646 (data_loss: 0.599, reg_loss: 0.047), lr: 0.019999500 validation, acc: 0.801, loss: 0.571 \n",
            "epoch: 51, acc: 0.793, loss: 0.637 (data_loss: 0.592, reg_loss: 0.045), lr: 0.019999490 validation, acc: 0.803, loss: 0.572 \n",
            "epoch: 52, acc: 0.798, loss: 0.636 (data_loss: 0.593, reg_loss: 0.044), lr: 0.019999480 validation, acc: 0.802, loss: 0.563 \n",
            "epoch: 53, acc: 0.797, loss: 0.628 (data_loss: 0.586, reg_loss: 0.042), lr: 0.019999470 validation, acc: 0.814, loss: 0.549 \n",
            "epoch: 54, acc: 0.804, loss: 0.609 (data_loss: 0.569, reg_loss: 0.040), lr: 0.019999460 validation, acc: 0.780, loss: 0.608 \n",
            "epoch: 55, acc: 0.779, loss: 0.662 (data_loss: 0.624, reg_loss: 0.038), lr: 0.019999450 validation, acc: 0.784, loss: 0.596 \n",
            "epoch: 56, acc: 0.782, loss: 0.644 (data_loss: 0.608, reg_loss: 0.036), lr: 0.019999440 validation, acc: 0.783, loss: 0.622 \n",
            "epoch: 57, acc: 0.778, loss: 0.671 (data_loss: 0.636, reg_loss: 0.035), lr: 0.019999430 validation, acc: 0.712, loss: 0.834 \n",
            "epoch: 58, acc: 0.711, loss: 0.900 (data_loss: 0.866, reg_loss: 0.034), lr: 0.019999420 validation, acc: 0.632, loss: 1.093 \n",
            "epoch: 59, acc: 0.628, loss: 1.167 (data_loss: 1.133, reg_loss: 0.034), lr: 0.019999410 validation, acc: 0.628, loss: 1.113 \n",
            "epoch: 60, acc: 0.626, loss: 1.185 (data_loss: 1.150, reg_loss: 0.035), lr: 0.019999400 validation, acc: 0.723, loss: 0.815 \n",
            "epoch: 61, acc: 0.711, loss: 0.911 (data_loss: 0.873, reg_loss: 0.038), lr: 0.019999390 validation, acc: 0.752, loss: 0.729 \n",
            "epoch: 62, acc: 0.736, loss: 0.838 (data_loss: 0.796, reg_loss: 0.042), lr: 0.019999380 validation, acc: 0.706, loss: 0.860 \n",
            "epoch: 63, acc: 0.653, loss: 1.097 (data_loss: 1.049, reg_loss: 0.047), lr: 0.019999370 validation, acc: 0.640, loss: 1.178 \n",
            "epoch: 64, acc: 0.638, loss: 1.311 (data_loss: 1.258, reg_loss: 0.053), lr: 0.019999360 validation, acc: 0.635, loss: 1.337 \n",
            "epoch: 65, acc: 0.625, loss: 1.536 (data_loss: 1.476, reg_loss: 0.059), lr: 0.019999350 validation, acc: 0.683, loss: 1.049 \n",
            "epoch: 66, acc: 0.666, loss: 1.220 (data_loss: 1.154, reg_loss: 0.066), lr: 0.019999340 validation, acc: 0.674, loss: 0.963 \n",
            "epoch: 67, acc: 0.654, loss: 1.181 (data_loss: 1.108, reg_loss: 0.074), lr: 0.019999330 validation, acc: 0.728, loss: 0.768 \n",
            "epoch: 68, acc: 0.708, loss: 0.920 (data_loss: 0.839, reg_loss: 0.080), lr: 0.019999320 validation, acc: 0.750, loss: 0.768 \n",
            "epoch: 69, acc: 0.724, loss: 0.923 (data_loss: 0.837, reg_loss: 0.086), lr: 0.019999310 validation, acc: 0.745, loss: 0.763 \n",
            "epoch: 70, acc: 0.726, loss: 0.924 (data_loss: 0.832, reg_loss: 0.091), lr: 0.019999300 validation, acc: 0.741, loss: 0.755 \n",
            "epoch: 71, acc: 0.724, loss: 0.918 (data_loss: 0.823, reg_loss: 0.095), lr: 0.019999290 validation, acc: 0.759, loss: 0.696 \n",
            "epoch: 72, acc: 0.738, loss: 0.860 (data_loss: 0.762, reg_loss: 0.098), lr: 0.019999280 validation, acc: 0.764, loss: 0.675 \n",
            "epoch: 73, acc: 0.747, loss: 0.832 (data_loss: 0.733, reg_loss: 0.099), lr: 0.019999270 validation, acc: 0.768, loss: 0.670 \n",
            "epoch: 74, acc: 0.753, loss: 0.816 (data_loss: 0.717, reg_loss: 0.099), lr: 0.019999260 validation, acc: 0.773, loss: 0.656 \n",
            "epoch: 75, acc: 0.757, loss: 0.803 (data_loss: 0.705, reg_loss: 0.098), lr: 0.019999250 validation, acc: 0.778, loss: 0.651 \n",
            "epoch: 76, acc: 0.767, loss: 0.780 (data_loss: 0.684, reg_loss: 0.096), lr: 0.019999240 validation, acc: 0.783, loss: 0.638 \n",
            "epoch: 77, acc: 0.776, loss: 0.761 (data_loss: 0.667, reg_loss: 0.094), lr: 0.019999230 validation, acc: 0.791, loss: 0.614 \n",
            "epoch: 78, acc: 0.781, loss: 0.732 (data_loss: 0.641, reg_loss: 0.091), lr: 0.019999220 validation, acc: 0.793, loss: 0.603 \n",
            "epoch: 79, acc: 0.780, loss: 0.719 (data_loss: 0.632, reg_loss: 0.087), lr: 0.019999210 validation, acc: 0.800, loss: 0.586 \n",
            "epoch: 80, acc: 0.791, loss: 0.694 (data_loss: 0.611, reg_loss: 0.083), lr: 0.019999200 validation, acc: 0.798, loss: 0.575 \n",
            "epoch: 81, acc: 0.794, loss: 0.675 (data_loss: 0.597, reg_loss: 0.079), lr: 0.019999190 validation, acc: 0.805, loss: 0.564 \n",
            "epoch: 82, acc: 0.800, loss: 0.657 (data_loss: 0.583, reg_loss: 0.074), lr: 0.019999180 validation, acc: 0.809, loss: 0.548 \n",
            "epoch: 83, acc: 0.805, loss: 0.636 (data_loss: 0.566, reg_loss: 0.070), lr: 0.019999170 validation, acc: 0.808, loss: 0.543 \n",
            "epoch: 84, acc: 0.806, loss: 0.628 (data_loss: 0.562, reg_loss: 0.066), lr: 0.019999160 validation, acc: 0.809, loss: 0.550 \n",
            "epoch: 85, acc: 0.805, loss: 0.630 (data_loss: 0.568, reg_loss: 0.062), lr: 0.019999150 validation, acc: 0.805, loss: 0.558 \n",
            "epoch: 86, acc: 0.804, loss: 0.632 (data_loss: 0.574, reg_loss: 0.058), lr: 0.019999140 validation, acc: 0.809, loss: 0.538 \n",
            "epoch: 87, acc: 0.811, loss: 0.607 (data_loss: 0.553, reg_loss: 0.055), lr: 0.019999130 validation, acc: 0.814, loss: 0.533 \n",
            "epoch: 88, acc: 0.813, loss: 0.594 (data_loss: 0.543, reg_loss: 0.051), lr: 0.019999120 validation, acc: 0.809, loss: 0.552 \n",
            "epoch: 89, acc: 0.805, loss: 0.615 (data_loss: 0.566, reg_loss: 0.049), lr: 0.019999110 validation, acc: 0.794, loss: 0.574 \n",
            "epoch: 90, acc: 0.797, loss: 0.628 (data_loss: 0.582, reg_loss: 0.046), lr: 0.019999100 validation, acc: 0.813, loss: 0.537 \n",
            "epoch: 91, acc: 0.811, loss: 0.595 (data_loss: 0.551, reg_loss: 0.044), lr: 0.019999090 validation, acc: 0.811, loss: 0.552 \n",
            "epoch: 92, acc: 0.808, loss: 0.613 (data_loss: 0.571, reg_loss: 0.043), lr: 0.019999080 validation, acc: 0.812, loss: 0.538 \n",
            "epoch: 93, acc: 0.811, loss: 0.597 (data_loss: 0.555, reg_loss: 0.041), lr: 0.019999070 validation, acc: 0.814, loss: 0.524 \n",
            "epoch: 94, acc: 0.811, loss: 0.577 (data_loss: 0.537, reg_loss: 0.041), lr: 0.019999060 validation, acc: 0.818, loss: 0.530 \n",
            "epoch: 95, acc: 0.814, loss: 0.582 (data_loss: 0.542, reg_loss: 0.040), lr: 0.019999050 validation, acc: 0.813, loss: 0.524 \n",
            "epoch: 96, acc: 0.813, loss: 0.578 (data_loss: 0.538, reg_loss: 0.039), lr: 0.019999040 validation, acc: 0.818, loss: 0.537 \n",
            "epoch: 97, acc: 0.812, loss: 0.591 (data_loss: 0.552, reg_loss: 0.039), lr: 0.019999030 validation, acc: 0.817, loss: 0.531 \n",
            "epoch: 98, acc: 0.813, loss: 0.577 (data_loss: 0.539, reg_loss: 0.038), lr: 0.019999020 validation, acc: 0.785, loss: 0.593 \n",
            "epoch: 99, acc: 0.785, loss: 0.645 (data_loss: 0.607, reg_loss: 0.038), lr: 0.019999010 validation, acc: 0.778, loss: 0.619 \n",
            "epoch: 100, acc: 0.770, loss: 0.688 (data_loss: 0.651, reg_loss: 0.037), lr: 0.019999000 validation, acc: 0.727, loss: 0.928 \n",
            "epoch: 101, acc: 0.713, loss: 0.990 (data_loss: 0.953, reg_loss: 0.037), lr: 0.019998990 validation, acc: 0.711, loss: 0.805 \n",
            "epoch: 102, acc: 0.700, loss: 0.887 (data_loss: 0.849, reg_loss: 0.037), lr: 0.019998980 validation, acc: 0.726, loss: 0.766 \n",
            "epoch: 103, acc: 0.709, loss: 0.860 (data_loss: 0.821, reg_loss: 0.039), lr: 0.019998970 validation, acc: 0.777, loss: 0.673 \n",
            "epoch: 104, acc: 0.753, loss: 0.773 (data_loss: 0.732, reg_loss: 0.041), lr: 0.019998960 validation, acc: 0.785, loss: 0.619 \n",
            "epoch: 105, acc: 0.761, loss: 0.745 (data_loss: 0.701, reg_loss: 0.044), lr: 0.019998950 validation, acc: 0.778, loss: 0.644 \n",
            "epoch: 106, acc: 0.754, loss: 0.773 (data_loss: 0.726, reg_loss: 0.047), lr: 0.019998940 validation, acc: 0.801, loss: 0.573 \n",
            "epoch: 107, acc: 0.773, loss: 0.719 (data_loss: 0.669, reg_loss: 0.050), lr: 0.019998930 validation, acc: 0.779, loss: 0.643 \n",
            "epoch: 108, acc: 0.763, loss: 0.763 (data_loss: 0.710, reg_loss: 0.053), lr: 0.019998920 validation, acc: 0.800, loss: 0.577 \n",
            "epoch: 109, acc: 0.782, loss: 0.698 (data_loss: 0.642, reg_loss: 0.056), lr: 0.019998910 validation, acc: 0.809, loss: 0.561 \n",
            "epoch: 110, acc: 0.790, loss: 0.675 (data_loss: 0.617, reg_loss: 0.058), lr: 0.019998900 validation, acc: 0.806, loss: 0.572 \n",
            "epoch: 111, acc: 0.800, loss: 0.656 (data_loss: 0.596, reg_loss: 0.060), lr: 0.019998890 validation, acc: 0.808, loss: 0.576 \n",
            "epoch: 112, acc: 0.801, loss: 0.656 (data_loss: 0.595, reg_loss: 0.061), lr: 0.019998880 validation, acc: 0.807, loss: 0.564 \n",
            "epoch: 113, acc: 0.800, loss: 0.650 (data_loss: 0.588, reg_loss: 0.062), lr: 0.019998870 validation, acc: 0.816, loss: 0.538 \n",
            "epoch: 114, acc: 0.810, loss: 0.620 (data_loss: 0.557, reg_loss: 0.063), lr: 0.019998860 validation, acc: 0.815, loss: 0.540 \n",
            "epoch: 115, acc: 0.810, loss: 0.622 (data_loss: 0.559, reg_loss: 0.063), lr: 0.019998850 validation, acc: 0.818, loss: 0.528 \n",
            "epoch: 116, acc: 0.813, loss: 0.609 (data_loss: 0.546, reg_loss: 0.063), lr: 0.019998840 validation, acc: 0.820, loss: 0.527 \n",
            "epoch: 117, acc: 0.813, loss: 0.607 (data_loss: 0.545, reg_loss: 0.062), lr: 0.019998830 validation, acc: 0.823, loss: 0.530 \n",
            "epoch: 118, acc: 0.818, loss: 0.602 (data_loss: 0.541, reg_loss: 0.061), lr: 0.019998820 validation, acc: 0.818, loss: 0.542 \n",
            "epoch: 119, acc: 0.814, loss: 0.610 (data_loss: 0.551, reg_loss: 0.059), lr: 0.019998810 validation, acc: 0.832, loss: 0.500 \n",
            "epoch: 120, acc: 0.826, loss: 0.568 (data_loss: 0.510, reg_loss: 0.058), lr: 0.019998800 validation, acc: 0.829, loss: 0.510 \n",
            "epoch: 121, acc: 0.820, loss: 0.580 (data_loss: 0.524, reg_loss: 0.056), lr: 0.019998790 validation, acc: 0.819, loss: 0.520 \n",
            "epoch: 122, acc: 0.816, loss: 0.583 (data_loss: 0.530, reg_loss: 0.054), lr: 0.019998780 validation, acc: 0.832, loss: 0.493 \n",
            "epoch: 123, acc: 0.830, loss: 0.553 (data_loss: 0.501, reg_loss: 0.052), lr: 0.019998770 validation, acc: 0.824, loss: 0.496 \n",
            "epoch: 124, acc: 0.826, loss: 0.551 (data_loss: 0.502, reg_loss: 0.049), lr: 0.019998760 validation, acc: 0.832, loss: 0.488 \n",
            "epoch: 125, acc: 0.830, loss: 0.546 (data_loss: 0.499, reg_loss: 0.047), lr: 0.019998750 validation, acc: 0.830, loss: 0.487 \n",
            "epoch: 126, acc: 0.830, loss: 0.536 (data_loss: 0.492, reg_loss: 0.045), lr: 0.019998740 validation, acc: 0.834, loss: 0.467 \n",
            "epoch: 127, acc: 0.837, loss: 0.517 (data_loss: 0.474, reg_loss: 0.043), lr: 0.019998730 validation, acc: 0.839, loss: 0.462 \n",
            "epoch: 128, acc: 0.838, loss: 0.508 (data_loss: 0.467, reg_loss: 0.041), lr: 0.019998720 validation, acc: 0.837, loss: 0.471 \n",
            "epoch: 129, acc: 0.837, loss: 0.511 (data_loss: 0.472, reg_loss: 0.039), lr: 0.019998710 validation, acc: 0.833, loss: 0.476 \n",
            "epoch: 130, acc: 0.831, loss: 0.519 (data_loss: 0.482, reg_loss: 0.037), lr: 0.019998700 validation, acc: 0.808, loss: 0.537 \n",
            "epoch: 131, acc: 0.806, loss: 0.578 (data_loss: 0.544, reg_loss: 0.035), lr: 0.019998690 validation, acc: 0.720, loss: 0.779 \n",
            "epoch: 132, acc: 0.723, loss: 0.833 (data_loss: 0.799, reg_loss: 0.034), lr: 0.019998680 validation, acc: 0.733, loss: 0.844 \n",
            "epoch: 133, acc: 0.729, loss: 0.909 (data_loss: 0.875, reg_loss: 0.034), lr: 0.019998670 validation, acc: 0.649, loss: 1.108 \n",
            "epoch: 134, acc: 0.620, loss: 1.343 (data_loss: 1.307, reg_loss: 0.036), lr: 0.019998660 validation, acc: 0.457, loss: 2.611 \n",
            "epoch: 135, acc: 0.455, loss: 2.842 (data_loss: 2.803, reg_loss: 0.039), lr: 0.019998650 validation, acc: 0.552, loss: 1.896 \n",
            "epoch: 136, acc: 0.519, loss: 2.262 (data_loss: 2.217, reg_loss: 0.045), lr: 0.019998640 validation, acc: 0.626, loss: 1.583 \n",
            "epoch: 137, acc: 0.563, loss: 2.015 (data_loss: 1.961, reg_loss: 0.054), lr: 0.019998630 validation, acc: 0.644, loss: 1.259 \n",
            "epoch: 138, acc: 0.606, loss: 1.546 (data_loss: 1.482, reg_loss: 0.064), lr: 0.019998620 validation, acc: 0.681, loss: 1.244 \n",
            "epoch: 139, acc: 0.654, loss: 1.442 (data_loss: 1.367, reg_loss: 0.075), lr: 0.019998610 validation, acc: 0.720, loss: 0.961 \n",
            "epoch: 140, acc: 0.686, loss: 1.192 (data_loss: 1.106, reg_loss: 0.086), lr: 0.019998600 validation, acc: 0.707, loss: 0.915 \n",
            "epoch: 141, acc: 0.675, loss: 1.158 (data_loss: 1.063, reg_loss: 0.096), lr: 0.019998590 validation, acc: 0.750, loss: 0.756 \n",
            "epoch: 142, acc: 0.723, loss: 0.952 (data_loss: 0.848, reg_loss: 0.104), lr: 0.019998580 validation, acc: 0.747, loss: 0.739 \n",
            "epoch: 143, acc: 0.722, loss: 0.940 (data_loss: 0.828, reg_loss: 0.112), lr: 0.019998570 validation, acc: 0.748, loss: 0.709 \n",
            "epoch: 144, acc: 0.732, loss: 0.907 (data_loss: 0.790, reg_loss: 0.117), lr: 0.019998560 validation, acc: 0.763, loss: 0.695 \n",
            "epoch: 145, acc: 0.741, loss: 0.882 (data_loss: 0.760, reg_loss: 0.122), lr: 0.019998550 validation, acc: 0.768, loss: 0.694 \n",
            "epoch: 146, acc: 0.747, loss: 0.873 (data_loss: 0.749, reg_loss: 0.125), lr: 0.019998540 validation, acc: 0.776, loss: 0.655 \n",
            "epoch: 147, acc: 0.756, loss: 0.830 (data_loss: 0.704, reg_loss: 0.126), lr: 0.019998530 validation, acc: 0.783, loss: 0.634 \n",
            "epoch: 148, acc: 0.764, loss: 0.807 (data_loss: 0.682, reg_loss: 0.126), lr: 0.019998520 validation, acc: 0.788, loss: 0.627 \n",
            "epoch: 149, acc: 0.769, loss: 0.792 (data_loss: 0.668, reg_loss: 0.124), lr: 0.019998510 validation, acc: 0.792, loss: 0.615 \n",
            "epoch: 150, acc: 0.776, loss: 0.775 (data_loss: 0.654, reg_loss: 0.122), lr: 0.019998500 validation, acc: 0.794, loss: 0.603 \n",
            "epoch: 151, acc: 0.784, loss: 0.755 (data_loss: 0.637, reg_loss: 0.118), lr: 0.019998490 validation, acc: 0.797, loss: 0.597 \n",
            "epoch: 152, acc: 0.786, loss: 0.738 (data_loss: 0.624, reg_loss: 0.114), lr: 0.019998480 validation, acc: 0.803, loss: 0.580 \n",
            "epoch: 153, acc: 0.795, loss: 0.714 (data_loss: 0.605, reg_loss: 0.110), lr: 0.019998470 validation, acc: 0.806, loss: 0.568 \n",
            "epoch: 154, acc: 0.796, loss: 0.700 (data_loss: 0.596, reg_loss: 0.105), lr: 0.019998460 validation, acc: 0.807, loss: 0.558 \n",
            "epoch: 155, acc: 0.799, loss: 0.682 (data_loss: 0.583, reg_loss: 0.100), lr: 0.019998450 validation, acc: 0.811, loss: 0.549 \n",
            "epoch: 156, acc: 0.803, loss: 0.667 (data_loss: 0.573, reg_loss: 0.094), lr: 0.019998440 validation, acc: 0.818, loss: 0.543 \n",
            "epoch: 157, acc: 0.809, loss: 0.651 (data_loss: 0.562, reg_loss: 0.089), lr: 0.019998430 validation, acc: 0.819, loss: 0.537 \n",
            "epoch: 158, acc: 0.812, loss: 0.635 (data_loss: 0.551, reg_loss: 0.084), lr: 0.019998420 validation, acc: 0.819, loss: 0.530 \n",
            "epoch: 159, acc: 0.816, loss: 0.619 (data_loss: 0.540, reg_loss: 0.079), lr: 0.019998410 validation, acc: 0.826, loss: 0.525 \n",
            "epoch: 160, acc: 0.819, loss: 0.610 (data_loss: 0.536, reg_loss: 0.074), lr: 0.019998400 validation, acc: 0.825, loss: 0.516 \n",
            "epoch: 161, acc: 0.823, loss: 0.597 (data_loss: 0.528, reg_loss: 0.069), lr: 0.019998390 validation, acc: 0.828, loss: 0.509 \n",
            "epoch: 162, acc: 0.824, loss: 0.584 (data_loss: 0.519, reg_loss: 0.065), lr: 0.019998380 validation, acc: 0.819, loss: 0.539 \n",
            "epoch: 163, acc: 0.818, loss: 0.607 (data_loss: 0.546, reg_loss: 0.061), lr: 0.019998370 validation, acc: 0.800, loss: 0.587 \n",
            "epoch: 164, acc: 0.798, loss: 0.659 (data_loss: 0.602, reg_loss: 0.057), lr: 0.019998360 validation, acc: 0.800, loss: 0.586 \n",
            "epoch: 165, acc: 0.794, loss: 0.668 (data_loss: 0.614, reg_loss: 0.054), lr: 0.019998350 validation, acc: 0.788, loss: 0.629 \n",
            "epoch: 166, acc: 0.782, loss: 0.711 (data_loss: 0.659, reg_loss: 0.052), lr: 0.019998340 validation, acc: 0.810, loss: 0.580 \n",
            "epoch: 167, acc: 0.802, loss: 0.654 (data_loss: 0.603, reg_loss: 0.051), lr: 0.019998330 validation, acc: 0.816, loss: 0.564 \n",
            "epoch: 168, acc: 0.803, loss: 0.642 (data_loss: 0.592, reg_loss: 0.050), lr: 0.019998320 validation, acc: 0.809, loss: 0.558 \n",
            "epoch: 169, acc: 0.800, loss: 0.645 (data_loss: 0.595, reg_loss: 0.050), lr: 0.019998310 validation, acc: 0.804, loss: 0.568 \n",
            "epoch: 170, acc: 0.793, loss: 0.653 (data_loss: 0.604, reg_loss: 0.050), lr: 0.019998300 validation, acc: 0.818, loss: 0.537 \n",
            "epoch: 171, acc: 0.807, loss: 0.618 (data_loss: 0.569, reg_loss: 0.049), lr: 0.019998290 validation, acc: 0.824, loss: 0.536 \n",
            "epoch: 172, acc: 0.815, loss: 0.609 (data_loss: 0.561, reg_loss: 0.049), lr: 0.019998280 validation, acc: 0.822, loss: 0.526 \n",
            "epoch: 173, acc: 0.815, loss: 0.593 (data_loss: 0.545, reg_loss: 0.048), lr: 0.019998270 validation, acc: 0.824, loss: 0.513 \n",
            "epoch: 174, acc: 0.818, loss: 0.576 (data_loss: 0.529, reg_loss: 0.047), lr: 0.019998260 validation, acc: 0.827, loss: 0.505 \n",
            "epoch: 175, acc: 0.824, loss: 0.561 (data_loss: 0.514, reg_loss: 0.046), lr: 0.019998250 validation, acc: 0.829, loss: 0.500 \n",
            "epoch: 176, acc: 0.828, loss: 0.552 (data_loss: 0.507, reg_loss: 0.045), lr: 0.019998240 validation, acc: 0.834, loss: 0.489 \n",
            "epoch: 177, acc: 0.831, loss: 0.542 (data_loss: 0.498, reg_loss: 0.044), lr: 0.019998230 validation, acc: 0.831, loss: 0.496 \n",
            "epoch: 178, acc: 0.827, loss: 0.545 (data_loss: 0.502, reg_loss: 0.042), lr: 0.019998220 validation, acc: 0.834, loss: 0.493 \n",
            "epoch: 179, acc: 0.830, loss: 0.539 (data_loss: 0.499, reg_loss: 0.041), lr: 0.019998210 validation, acc: 0.827, loss: 0.504 \n",
            "epoch: 180, acc: 0.823, loss: 0.557 (data_loss: 0.518, reg_loss: 0.039), lr: 0.019998200 validation, acc: 0.824, loss: 0.509 \n",
            "epoch: 181, acc: 0.823, loss: 0.554 (data_loss: 0.517, reg_loss: 0.037), lr: 0.019998190 validation, acc: 0.793, loss: 0.603 \n",
            "epoch: 182, acc: 0.787, loss: 0.661 (data_loss: 0.625, reg_loss: 0.036), lr: 0.019998180 validation, acc: 0.816, loss: 0.540 \n",
            "epoch: 183, acc: 0.813, loss: 0.585 (data_loss: 0.551, reg_loss: 0.034), lr: 0.019998170 validation, acc: 0.784, loss: 0.629 \n",
            "epoch: 184, acc: 0.776, loss: 0.694 (data_loss: 0.660, reg_loss: 0.033), lr: 0.019998160 validation, acc: 0.792, loss: 0.596 \n",
            "epoch: 185, acc: 0.785, loss: 0.666 (data_loss: 0.633, reg_loss: 0.033), lr: 0.019998150 validation, acc: 0.796, loss: 0.583 \n",
            "epoch: 186, acc: 0.788, loss: 0.641 (data_loss: 0.608, reg_loss: 0.033), lr: 0.019998140 validation, acc: 0.811, loss: 0.559 \n",
            "epoch: 187, acc: 0.803, loss: 0.617 (data_loss: 0.584, reg_loss: 0.033), lr: 0.019998130 validation, acc: 0.773, loss: 0.634 \n",
            "epoch: 188, acc: 0.769, loss: 0.684 (data_loss: 0.651, reg_loss: 0.033), lr: 0.019998120 validation, acc: 0.802, loss: 0.564 \n",
            "epoch: 189, acc: 0.793, loss: 0.624 (data_loss: 0.590, reg_loss: 0.034), lr: 0.019998110 validation, acc: 0.810, loss: 0.542 \n",
            "epoch: 190, acc: 0.803, loss: 0.598 (data_loss: 0.564, reg_loss: 0.034), lr: 0.019998100 validation, acc: 0.820, loss: 0.515 \n",
            "epoch: 191, acc: 0.813, loss: 0.572 (data_loss: 0.537, reg_loss: 0.035), lr: 0.019998090 validation, acc: 0.814, loss: 0.538 \n",
            "epoch: 192, acc: 0.809, loss: 0.589 (data_loss: 0.554, reg_loss: 0.036), lr: 0.019998080 validation, acc: 0.810, loss: 0.539 \n",
            "epoch: 193, acc: 0.800, loss: 0.601 (data_loss: 0.565, reg_loss: 0.036), lr: 0.019998070 validation, acc: 0.787, loss: 0.603 \n",
            "epoch: 194, acc: 0.780, loss: 0.653 (data_loss: 0.616, reg_loss: 0.036), lr: 0.019998060 validation, acc: 0.735, loss: 0.799 \n",
            "epoch: 195, acc: 0.726, loss: 0.887 (data_loss: 0.850, reg_loss: 0.037), lr: 0.019998050 validation, acc: 0.687, loss: 0.936 \n",
            "epoch: 196, acc: 0.678, loss: 1.015 (data_loss: 0.976, reg_loss: 0.039), lr: 0.019998040 validation, acc: 0.757, loss: 0.690 \n",
            "epoch: 197, acc: 0.739, loss: 0.789 (data_loss: 0.749, reg_loss: 0.040), lr: 0.019998030 validation, acc: 0.776, loss: 0.707 \n",
            "epoch: 198, acc: 0.749, loss: 0.827 (data_loss: 0.784, reg_loss: 0.042), lr: 0.019998020 validation, acc: 0.781, loss: 0.654 \n",
            "epoch: 199, acc: 0.758, loss: 0.769 (data_loss: 0.724, reg_loss: 0.045), lr: 0.019998010 validation, acc: 0.787, loss: 0.621 \n",
            "epoch: 200, acc: 0.756, loss: 0.752 (data_loss: 0.706, reg_loss: 0.046), lr: 0.019998000 validation, acc: 0.795, loss: 0.573 \n",
            "epoch: 201, acc: 0.776, loss: 0.675 (data_loss: 0.626, reg_loss: 0.048), lr: 0.019997990 validation, acc: 0.784, loss: 0.597 \n",
            "epoch: 202, acc: 0.776, loss: 0.690 (data_loss: 0.640, reg_loss: 0.050), lr: 0.019997980 validation, acc: 0.805, loss: 0.563 \n",
            "epoch: 203, acc: 0.786, loss: 0.664 (data_loss: 0.613, reg_loss: 0.050), lr: 0.019997970 validation, acc: 0.809, loss: 0.542 \n",
            "epoch: 204, acc: 0.794, loss: 0.641 (data_loss: 0.590, reg_loss: 0.051), lr: 0.019997960 validation, acc: 0.816, loss: 0.525 \n",
            "epoch: 205, acc: 0.802, loss: 0.611 (data_loss: 0.560, reg_loss: 0.051), lr: 0.019997950 validation, acc: 0.818, loss: 0.528 \n",
            "epoch: 206, acc: 0.803, loss: 0.611 (data_loss: 0.561, reg_loss: 0.051), lr: 0.019997940 validation, acc: 0.819, loss: 0.525 \n",
            "epoch: 207, acc: 0.810, loss: 0.598 (data_loss: 0.548, reg_loss: 0.050), lr: 0.019997930 validation, acc: 0.824, loss: 0.514 \n",
            "epoch: 208, acc: 0.818, loss: 0.583 (data_loss: 0.533, reg_loss: 0.049), lr: 0.019997920 validation, acc: 0.828, loss: 0.504 \n",
            "epoch: 209, acc: 0.821, loss: 0.573 (data_loss: 0.524, reg_loss: 0.048), lr: 0.019997910 validation, acc: 0.829, loss: 0.499 \n",
            "epoch: 210, acc: 0.824, loss: 0.565 (data_loss: 0.518, reg_loss: 0.047), lr: 0.019997900 validation, acc: 0.831, loss: 0.492 \n",
            "epoch: 211, acc: 0.827, loss: 0.552 (data_loss: 0.506, reg_loss: 0.046), lr: 0.019997890 validation, acc: 0.831, loss: 0.482 \n",
            "epoch: 212, acc: 0.830, loss: 0.538 (data_loss: 0.493, reg_loss: 0.045), lr: 0.019997880 validation, acc: 0.833, loss: 0.478 \n",
            "epoch: 213, acc: 0.830, loss: 0.536 (data_loss: 0.492, reg_loss: 0.044), lr: 0.019997870 validation, acc: 0.833, loss: 0.477 \n",
            "epoch: 214, acc: 0.827, loss: 0.536 (data_loss: 0.494, reg_loss: 0.042), lr: 0.019997860 validation, acc: 0.829, loss: 0.498 \n",
            "epoch: 215, acc: 0.823, loss: 0.556 (data_loss: 0.516, reg_loss: 0.041), lr: 0.019997850 validation, acc: 0.823, loss: 0.507 \n",
            "epoch: 216, acc: 0.816, loss: 0.565 (data_loss: 0.526, reg_loss: 0.039), lr: 0.019997840 validation, acc: 0.832, loss: 0.491 \n",
            "epoch: 217, acc: 0.829, loss: 0.541 (data_loss: 0.503, reg_loss: 0.038), lr: 0.019997830 validation, acc: 0.835, loss: 0.473 \n",
            "epoch: 218, acc: 0.835, loss: 0.516 (data_loss: 0.479, reg_loss: 0.037), lr: 0.019997820 validation, acc: 0.830, loss: 0.482 \n",
            "epoch: 219, acc: 0.827, loss: 0.527 (data_loss: 0.491, reg_loss: 0.036), lr: 0.019997810 validation, acc: 0.831, loss: 0.498 \n",
            "epoch: 220, acc: 0.830, loss: 0.536 (data_loss: 0.500, reg_loss: 0.036), lr: 0.019997800 validation, acc: 0.823, loss: 0.496 \n",
            "epoch: 221, acc: 0.821, loss: 0.547 (data_loss: 0.512, reg_loss: 0.035), lr: 0.019997790 validation, acc: 0.837, loss: 0.467 \n",
            "epoch: 222, acc: 0.833, loss: 0.517 (data_loss: 0.482, reg_loss: 0.035), lr: 0.019997780 validation, acc: 0.831, loss: 0.491 \n",
            "epoch: 223, acc: 0.826, loss: 0.535 (data_loss: 0.501, reg_loss: 0.034), lr: 0.019997770 validation, acc: 0.832, loss: 0.484 \n",
            "epoch: 224, acc: 0.825, loss: 0.529 (data_loss: 0.495, reg_loss: 0.033), lr: 0.019997760 validation, acc: 0.819, loss: 0.503 \n",
            "epoch: 225, acc: 0.817, loss: 0.550 (data_loss: 0.517, reg_loss: 0.033), lr: 0.019997750 validation, acc: 0.829, loss: 0.502 \n",
            "epoch: 226, acc: 0.824, loss: 0.546 (data_loss: 0.514, reg_loss: 0.032), lr: 0.019997740 validation, acc: 0.815, loss: 0.525 \n",
            "epoch: 227, acc: 0.812, loss: 0.574 (data_loss: 0.542, reg_loss: 0.032), lr: 0.019997730 validation, acc: 0.786, loss: 0.731 \n",
            "epoch: 228, acc: 0.775, loss: 0.799 (data_loss: 0.766, reg_loss: 0.032), lr: 0.019997720 validation, acc: 0.745, loss: 0.739 \n",
            "epoch: 229, acc: 0.728, loss: 0.852 (data_loss: 0.819, reg_loss: 0.033), lr: 0.019997710 validation, acc: 0.749, loss: 0.756 \n",
            "epoch: 230, acc: 0.742, loss: 0.842 (data_loss: 0.808, reg_loss: 0.034), lr: 0.019997700 validation, acc: 0.696, loss: 0.864 \n",
            "epoch: 231, acc: 0.685, loss: 1.021 (data_loss: 0.985, reg_loss: 0.036), lr: 0.019997690 validation, acc: 0.626, loss: 1.229 \n",
            "epoch: 232, acc: 0.608, loss: 1.358 (data_loss: 1.319, reg_loss: 0.039), lr: 0.019997680 validation, acc: 0.698, loss: 0.913 \n",
            "epoch: 233, acc: 0.684, loss: 1.036 (data_loss: 0.993, reg_loss: 0.043), lr: 0.019997670 validation, acc: 0.742, loss: 0.753 \n",
            "epoch: 234, acc: 0.715, loss: 0.910 (data_loss: 0.862, reg_loss: 0.048), lr: 0.019997660 validation, acc: 0.754, loss: 0.742 \n",
            "epoch: 235, acc: 0.716, loss: 0.922 (data_loss: 0.869, reg_loss: 0.053), lr: 0.019997650 validation, acc: 0.738, loss: 0.704 \n",
            "epoch: 236, acc: 0.716, loss: 0.848 (data_loss: 0.789, reg_loss: 0.059), lr: 0.019997640 validation, acc: 0.759, loss: 0.685 \n",
            "epoch: 237, acc: 0.729, loss: 0.835 (data_loss: 0.771, reg_loss: 0.064), lr: 0.019997630 validation, acc: 0.772, loss: 0.649 \n",
            "epoch: 238, acc: 0.746, loss: 0.792 (data_loss: 0.723, reg_loss: 0.069), lr: 0.019997620 validation, acc: 0.778, loss: 0.639 \n",
            "epoch: 239, acc: 0.753, loss: 0.777 (data_loss: 0.704, reg_loss: 0.073), lr: 0.019997610 validation, acc: 0.796, loss: 0.599 \n",
            "epoch: 240, acc: 0.773, loss: 0.737 (data_loss: 0.661, reg_loss: 0.076), lr: 0.019997600 validation, acc: 0.792, loss: 0.591 \n",
            "epoch: 241, acc: 0.778, loss: 0.718 (data_loss: 0.640, reg_loss: 0.078), lr: 0.019997590 validation, acc: 0.790, loss: 0.598 \n",
            "epoch: 242, acc: 0.780, loss: 0.711 (data_loss: 0.632, reg_loss: 0.080), lr: 0.019997580 validation, acc: 0.795, loss: 0.594 \n",
            "epoch: 243, acc: 0.785, loss: 0.703 (data_loss: 0.623, reg_loss: 0.080), lr: 0.019997570 validation, acc: 0.804, loss: 0.582 \n",
            "epoch: 244, acc: 0.793, loss: 0.691 (data_loss: 0.610, reg_loss: 0.080), lr: 0.019997560 validation, acc: 0.813, loss: 0.565 \n",
            "epoch: 245, acc: 0.800, loss: 0.668 (data_loss: 0.588, reg_loss: 0.080), lr: 0.019997550 validation, acc: 0.812, loss: 0.563 \n",
            "epoch: 246, acc: 0.800, loss: 0.664 (data_loss: 0.586, reg_loss: 0.078), lr: 0.019997540 validation, acc: 0.818, loss: 0.542 \n",
            "epoch: 247, acc: 0.810, loss: 0.636 (data_loss: 0.559, reg_loss: 0.077), lr: 0.019997530 validation, acc: 0.817, loss: 0.527 \n",
            "epoch: 248, acc: 0.812, loss: 0.617 (data_loss: 0.543, reg_loss: 0.075), lr: 0.019997520 validation, acc: 0.820, loss: 0.520 \n",
            "epoch: 249, acc: 0.812, loss: 0.609 (data_loss: 0.537, reg_loss: 0.072), lr: 0.019997510 validation, acc: 0.824, loss: 0.516 \n",
            "epoch: 250, acc: 0.814, loss: 0.604 (data_loss: 0.534, reg_loss: 0.070), lr: 0.019997500 validation, acc: 0.821, loss: 0.511 \n",
            "epoch: 251, acc: 0.813, loss: 0.600 (data_loss: 0.533, reg_loss: 0.067), lr: 0.019997490 validation, acc: 0.826, loss: 0.506 \n",
            "epoch: 252, acc: 0.818, loss: 0.586 (data_loss: 0.521, reg_loss: 0.064), lr: 0.019997480 validation, acc: 0.830, loss: 0.497 \n",
            "epoch: 253, acc: 0.820, loss: 0.577 (data_loss: 0.515, reg_loss: 0.061), lr: 0.019997470 validation, acc: 0.830, loss: 0.490 \n",
            "epoch: 254, acc: 0.825, loss: 0.565 (data_loss: 0.506, reg_loss: 0.059), lr: 0.019997460 validation, acc: 0.828, loss: 0.486 \n",
            "epoch: 255, acc: 0.827, loss: 0.553 (data_loss: 0.497, reg_loss: 0.056), lr: 0.019997450 validation, acc: 0.832, loss: 0.482 \n",
            "epoch: 256, acc: 0.829, loss: 0.546 (data_loss: 0.492, reg_loss: 0.053), lr: 0.019997440 validation, acc: 0.829, loss: 0.497 \n",
            "epoch: 257, acc: 0.822, loss: 0.565 (data_loss: 0.514, reg_loss: 0.051), lr: 0.019997430 validation, acc: 0.816, loss: 0.522 \n",
            "epoch: 258, acc: 0.811, loss: 0.583 (data_loss: 0.535, reg_loss: 0.049), lr: 0.019997420 validation, acc: 0.792, loss: 0.584 \n",
            "epoch: 259, acc: 0.786, loss: 0.654 (data_loss: 0.607, reg_loss: 0.047), lr: 0.019997410 validation, acc: 0.819, loss: 0.523 \n",
            "epoch: 260, acc: 0.811, loss: 0.598 (data_loss: 0.552, reg_loss: 0.046), lr: 0.019997400 validation, acc: 0.809, loss: 0.546 \n",
            "epoch: 261, acc: 0.800, loss: 0.624 (data_loss: 0.579, reg_loss: 0.045), lr: 0.019997390 validation, acc: 0.810, loss: 0.538 \n",
            "epoch: 262, acc: 0.801, loss: 0.615 (data_loss: 0.570, reg_loss: 0.045), lr: 0.019997380 validation, acc: 0.819, loss: 0.531 \n",
            "epoch: 263, acc: 0.806, loss: 0.608 (data_loss: 0.563, reg_loss: 0.045), lr: 0.019997370 validation, acc: 0.816, loss: 0.529 \n",
            "epoch: 264, acc: 0.798, loss: 0.629 (data_loss: 0.584, reg_loss: 0.046), lr: 0.019997360 validation, acc: 0.802, loss: 0.600 \n",
            "epoch: 265, acc: 0.793, loss: 0.672 (data_loss: 0.626, reg_loss: 0.046), lr: 0.019997350 validation, acc: 0.815, loss: 0.525 \n",
            "epoch: 266, acc: 0.804, loss: 0.604 (data_loss: 0.559, reg_loss: 0.046), lr: 0.019997340 validation, acc: 0.809, loss: 0.545 \n",
            "epoch: 267, acc: 0.802, loss: 0.628 (data_loss: 0.582, reg_loss: 0.046), lr: 0.019997330 validation, acc: 0.812, loss: 0.553 \n",
            "epoch: 268, acc: 0.802, loss: 0.629 (data_loss: 0.583, reg_loss: 0.046), lr: 0.019997320 validation, acc: 0.820, loss: 0.519 \n",
            "epoch: 269, acc: 0.813, loss: 0.596 (data_loss: 0.550, reg_loss: 0.046), lr: 0.019997310 validation, acc: 0.822, loss: 0.515 \n",
            "epoch: 270, acc: 0.811, loss: 0.601 (data_loss: 0.555, reg_loss: 0.046), lr: 0.019997300 validation, acc: 0.825, loss: 0.501 \n",
            "epoch: 271, acc: 0.819, loss: 0.578 (data_loss: 0.531, reg_loss: 0.046), lr: 0.019997290 validation, acc: 0.830, loss: 0.494 \n",
            "epoch: 272, acc: 0.823, loss: 0.561 (data_loss: 0.515, reg_loss: 0.046), lr: 0.019997280 validation, acc: 0.830, loss: 0.492 \n",
            "epoch: 273, acc: 0.826, loss: 0.552 (data_loss: 0.507, reg_loss: 0.046), lr: 0.019997270 validation, acc: 0.828, loss: 0.503 \n",
            "epoch: 274, acc: 0.821, loss: 0.571 (data_loss: 0.526, reg_loss: 0.045), lr: 0.019997260 validation, acc: 0.814, loss: 0.525 \n",
            "epoch: 275, acc: 0.807, loss: 0.591 (data_loss: 0.547, reg_loss: 0.044), lr: 0.019997250 validation, acc: 0.789, loss: 0.605 \n",
            "epoch: 276, acc: 0.785, loss: 0.665 (data_loss: 0.622, reg_loss: 0.043), lr: 0.019997240 validation, acc: 0.818, loss: 0.526 \n",
            "epoch: 277, acc: 0.815, loss: 0.595 (data_loss: 0.552, reg_loss: 0.043), lr: 0.019997230 validation, acc: 0.821, loss: 0.532 \n",
            "epoch: 278, acc: 0.808, loss: 0.603 (data_loss: 0.560, reg_loss: 0.043), lr: 0.019997220 validation, acc: 0.812, loss: 0.529 \n",
            "epoch: 279, acc: 0.807, loss: 0.592 (data_loss: 0.549, reg_loss: 0.043), lr: 0.019997210 validation, acc: 0.832, loss: 0.492 \n",
            "epoch: 280, acc: 0.826, loss: 0.549 (data_loss: 0.506, reg_loss: 0.044), lr: 0.019997200 validation, acc: 0.814, loss: 0.530 \n",
            "epoch: 281, acc: 0.809, loss: 0.602 (data_loss: 0.559, reg_loss: 0.044), lr: 0.019997190 validation, acc: 0.808, loss: 0.536 \n",
            "epoch: 282, acc: 0.804, loss: 0.600 (data_loss: 0.557, reg_loss: 0.044), lr: 0.019997180 validation, acc: 0.796, loss: 0.599 \n",
            "epoch: 283, acc: 0.789, loss: 0.664 (data_loss: 0.621, reg_loss: 0.043), lr: 0.019997170 validation, acc: 0.823, loss: 0.511 \n",
            "epoch: 284, acc: 0.815, loss: 0.577 (data_loss: 0.534, reg_loss: 0.043), lr: 0.019997160 validation, acc: 0.803, loss: 0.574 \n",
            "epoch: 285, acc: 0.792, loss: 0.655 (data_loss: 0.613, reg_loss: 0.042), lr: 0.019997150 validation, acc: 0.763, loss: 0.802 \n",
            "epoch: 286, acc: 0.746, loss: 0.913 (data_loss: 0.872, reg_loss: 0.042), lr: 0.019997140 validation, acc: 0.786, loss: 0.644 \n",
            "epoch: 287, acc: 0.768, loss: 0.746 (data_loss: 0.704, reg_loss: 0.042), lr: 0.019997130 validation, acc: 0.795, loss: 0.587 \n",
            "epoch: 288, acc: 0.773, loss: 0.696 (data_loss: 0.655, reg_loss: 0.042), lr: 0.019997120 validation, acc: 0.803, loss: 0.569 \n",
            "epoch: 289, acc: 0.780, loss: 0.675 (data_loss: 0.633, reg_loss: 0.042), lr: 0.019997110 validation, acc: 0.810, loss: 0.539 \n",
            "epoch: 290, acc: 0.799, loss: 0.618 (data_loss: 0.576, reg_loss: 0.042), lr: 0.019997100 validation, acc: 0.819, loss: 0.530 \n",
            "epoch: 291, acc: 0.809, loss: 0.603 (data_loss: 0.560, reg_loss: 0.043), lr: 0.019997090 validation, acc: 0.821, loss: 0.514 \n",
            "epoch: 292, acc: 0.813, loss: 0.585 (data_loss: 0.542, reg_loss: 0.043), lr: 0.019997080 validation, acc: 0.827, loss: 0.511 \n",
            "epoch: 293, acc: 0.816, loss: 0.585 (data_loss: 0.542, reg_loss: 0.043), lr: 0.019997070 validation, acc: 0.818, loss: 0.506 \n",
            "epoch: 294, acc: 0.813, loss: 0.567 (data_loss: 0.523, reg_loss: 0.043), lr: 0.019997060 validation, acc: 0.816, loss: 0.532 \n",
            "epoch: 295, acc: 0.807, loss: 0.592 (data_loss: 0.549, reg_loss: 0.043), lr: 0.019997050 validation, acc: 0.808, loss: 0.539 \n",
            "epoch: 296, acc: 0.801, loss: 0.598 (data_loss: 0.554, reg_loss: 0.043), lr: 0.019997040 validation, acc: 0.821, loss: 0.514 \n",
            "epoch: 297, acc: 0.815, loss: 0.579 (data_loss: 0.535, reg_loss: 0.043), lr: 0.019997030 validation, acc: 0.818, loss: 0.522 \n",
            "epoch: 298, acc: 0.812, loss: 0.581 (data_loss: 0.537, reg_loss: 0.043), lr: 0.019997020 validation, acc: 0.825, loss: 0.496 \n",
            "epoch: 299, acc: 0.816, loss: 0.573 (data_loss: 0.530, reg_loss: 0.043), lr: 0.019997010 validation, acc: 0.831, loss: 0.483 \n",
            "epoch: 300, acc: 0.822, loss: 0.555 (data_loss: 0.511, reg_loss: 0.044), lr: 0.019997000 validation, acc: 0.829, loss: 0.499 \n",
            "epoch: 301, acc: 0.822, loss: 0.558 (data_loss: 0.515, reg_loss: 0.043), lr: 0.019996990 validation, acc: 0.831, loss: 0.484 \n",
            "epoch: 302, acc: 0.823, loss: 0.551 (data_loss: 0.508, reg_loss: 0.043), lr: 0.019996980 validation, acc: 0.836, loss: 0.471 \n",
            "epoch: 303, acc: 0.826, loss: 0.539 (data_loss: 0.497, reg_loss: 0.043), lr: 0.019996970 validation, acc: 0.838, loss: 0.481 \n",
            "epoch: 304, acc: 0.829, loss: 0.537 (data_loss: 0.495, reg_loss: 0.042), lr: 0.019996960 validation, acc: 0.835, loss: 0.485 \n",
            "epoch: 305, acc: 0.828, loss: 0.538 (data_loss: 0.497, reg_loss: 0.041), lr: 0.019996950 validation, acc: 0.835, loss: 0.476 \n",
            "epoch: 306, acc: 0.834, loss: 0.523 (data_loss: 0.483, reg_loss: 0.040), lr: 0.019996940 validation, acc: 0.837, loss: 0.469 \n",
            "epoch: 307, acc: 0.837, loss: 0.511 (data_loss: 0.472, reg_loss: 0.039), lr: 0.019996930 validation, acc: 0.840, loss: 0.462 \n",
            "epoch: 308, acc: 0.839, loss: 0.502 (data_loss: 0.465, reg_loss: 0.038), lr: 0.019996920 validation, acc: 0.842, loss: 0.456 \n",
            "epoch: 309, acc: 0.842, loss: 0.495 (data_loss: 0.459, reg_loss: 0.036), lr: 0.019996910 validation, acc: 0.845, loss: 0.448 \n",
            "epoch: 310, acc: 0.844, loss: 0.488 (data_loss: 0.453, reg_loss: 0.035), lr: 0.019996900 validation, acc: 0.845, loss: 0.451 \n",
            "epoch: 311, acc: 0.844, loss: 0.483 (data_loss: 0.449, reg_loss: 0.033), lr: 0.019996890 validation, acc: 0.849, loss: 0.439 \n",
            "epoch: 312, acc: 0.844, loss: 0.480 (data_loss: 0.448, reg_loss: 0.032), lr: 0.019996880 validation, acc: 0.849, loss: 0.438 \n",
            "epoch: 313, acc: 0.846, loss: 0.472 (data_loss: 0.442, reg_loss: 0.031), lr: 0.019996870 validation, acc: 0.841, loss: 0.451 \n",
            "epoch: 314, acc: 0.844, loss: 0.478 (data_loss: 0.449, reg_loss: 0.029), lr: 0.019996860 validation, acc: 0.837, loss: 0.468 \n",
            "epoch: 315, acc: 0.833, loss: 0.510 (data_loss: 0.483, reg_loss: 0.028), lr: 0.019996850 validation, acc: 0.810, loss: 0.524 \n",
            "epoch: 316, acc: 0.809, loss: 0.558 (data_loss: 0.532, reg_loss: 0.027), lr: 0.019996840 validation, acc: 0.734, loss: 0.738 \n",
            "epoch: 317, acc: 0.733, loss: 0.783 (data_loss: 0.758, reg_loss: 0.026), lr: 0.019996831 validation, acc: 0.782, loss: 0.623 \n",
            "epoch: 318, acc: 0.770, loss: 0.690 (data_loss: 0.664, reg_loss: 0.026), lr: 0.019996821 validation, acc: 0.707, loss: 0.861 \n",
            "epoch: 319, acc: 0.685, loss: 0.985 (data_loss: 0.959, reg_loss: 0.027), lr: 0.019996811 validation, acc: 0.583, loss: 1.525 \n",
            "epoch: 320, acc: 0.562, loss: 1.698 (data_loss: 1.670, reg_loss: 0.029), lr: 0.019996801 validation, acc: 0.632, loss: 1.322 \n",
            "epoch: 321, acc: 0.600, loss: 1.533 (data_loss: 1.501, reg_loss: 0.032), lr: 0.019996791 validation, acc: 0.657, loss: 1.116 \n",
            "epoch: 322, acc: 0.626, loss: 1.270 (data_loss: 1.234, reg_loss: 0.037), lr: 0.019996781 validation, acc: 0.655, loss: 1.072 \n",
            "epoch: 323, acc: 0.609, loss: 1.261 (data_loss: 1.219, reg_loss: 0.042), lr: 0.019996771 validation, acc: 0.667, loss: 0.981 \n",
            "epoch: 324, acc: 0.649, loss: 1.135 (data_loss: 1.088, reg_loss: 0.048), lr: 0.019996761 validation, acc: 0.725, loss: 0.819 \n",
            "epoch: 325, acc: 0.677, loss: 1.017 (data_loss: 0.963, reg_loss: 0.054), lr: 0.019996751 validation, acc: 0.734, loss: 0.727 \n",
            "epoch: 326, acc: 0.707, loss: 0.901 (data_loss: 0.842, reg_loss: 0.059), lr: 0.019996741 validation, acc: 0.776, loss: 0.674 \n",
            "epoch: 327, acc: 0.743, loss: 0.834 (data_loss: 0.769, reg_loss: 0.065), lr: 0.019996731 validation, acc: 0.782, loss: 0.660 \n",
            "epoch: 328, acc: 0.745, loss: 0.828 (data_loss: 0.757, reg_loss: 0.071), lr: 0.019996721 validation, acc: 0.779, loss: 0.669 \n",
            "epoch: 329, acc: 0.752, loss: 0.835 (data_loss: 0.759, reg_loss: 0.076), lr: 0.019996711 validation, acc: 0.787, loss: 0.631 \n",
            "epoch: 330, acc: 0.759, loss: 0.800 (data_loss: 0.721, reg_loss: 0.080), lr: 0.019996701 validation, acc: 0.794, loss: 0.601 \n",
            "epoch: 331, acc: 0.765, loss: 0.771 (data_loss: 0.689, reg_loss: 0.082), lr: 0.019996691 validation, acc: 0.800, loss: 0.584 \n",
            "epoch: 332, acc: 0.776, loss: 0.744 (data_loss: 0.659, reg_loss: 0.084), lr: 0.019996681 validation, acc: 0.798, loss: 0.589 \n",
            "epoch: 333, acc: 0.779, loss: 0.734 (data_loss: 0.649, reg_loss: 0.085), lr: 0.019996671 validation, acc: 0.805, loss: 0.573 \n",
            "epoch: 334, acc: 0.788, loss: 0.714 (data_loss: 0.628, reg_loss: 0.086), lr: 0.019996661 validation, acc: 0.807, loss: 0.564 \n",
            "epoch: 335, acc: 0.791, loss: 0.693 (data_loss: 0.608, reg_loss: 0.085), lr: 0.019996651 validation, acc: 0.816, loss: 0.548 \n",
            "epoch: 336, acc: 0.797, loss: 0.671 (data_loss: 0.587, reg_loss: 0.084), lr: 0.019996641 validation, acc: 0.818, loss: 0.536 \n",
            "epoch: 337, acc: 0.803, loss: 0.654 (data_loss: 0.571, reg_loss: 0.082), lr: 0.019996631 validation, acc: 0.820, loss: 0.535 \n",
            "epoch: 338, acc: 0.803, loss: 0.648 (data_loss: 0.568, reg_loss: 0.080), lr: 0.019996621 validation, acc: 0.826, loss: 0.525 \n",
            "epoch: 339, acc: 0.808, loss: 0.632 (data_loss: 0.555, reg_loss: 0.078), lr: 0.019996611 validation, acc: 0.825, loss: 0.520 \n",
            "epoch: 340, acc: 0.809, loss: 0.624 (data_loss: 0.549, reg_loss: 0.075), lr: 0.019996601 validation, acc: 0.822, loss: 0.522 \n",
            "epoch: 341, acc: 0.812, loss: 0.616 (data_loss: 0.544, reg_loss: 0.071), lr: 0.019996591 validation, acc: 0.817, loss: 0.531 \n",
            "epoch: 342, acc: 0.806, loss: 0.630 (data_loss: 0.562, reg_loss: 0.068), lr: 0.019996581 validation, acc: 0.827, loss: 0.508 \n",
            "epoch: 343, acc: 0.814, loss: 0.598 (data_loss: 0.533, reg_loss: 0.065), lr: 0.019996571 validation, acc: 0.829, loss: 0.499 \n",
            "epoch: 344, acc: 0.820, loss: 0.581 (data_loss: 0.518, reg_loss: 0.062), lr: 0.019996561 validation, acc: 0.830, loss: 0.509 \n",
            "epoch: 345, acc: 0.819, loss: 0.589 (data_loss: 0.530, reg_loss: 0.060), lr: 0.019996551 validation, acc: 0.832, loss: 0.489 \n",
            "epoch: 346, acc: 0.824, loss: 0.567 (data_loss: 0.510, reg_loss: 0.057), lr: 0.019996541 validation, acc: 0.836, loss: 0.477 \n",
            "epoch: 347, acc: 0.827, loss: 0.553 (data_loss: 0.499, reg_loss: 0.054), lr: 0.019996531 validation, acc: 0.837, loss: 0.475 \n",
            "epoch: 348, acc: 0.828, loss: 0.550 (data_loss: 0.498, reg_loss: 0.052), lr: 0.019996521 validation, acc: 0.834, loss: 0.483 \n",
            "epoch: 349, acc: 0.826, loss: 0.554 (data_loss: 0.505, reg_loss: 0.050), lr: 0.019996511 validation, acc: 0.832, loss: 0.485 \n",
            "epoch: 350, acc: 0.822, loss: 0.557 (data_loss: 0.510, reg_loss: 0.047), lr: 0.019996501 validation, acc: 0.826, loss: 0.497 \n",
            "epoch: 351, acc: 0.815, loss: 0.564 (data_loss: 0.519, reg_loss: 0.045), lr: 0.019996491 validation, acc: 0.814, loss: 0.527 \n",
            "epoch: 352, acc: 0.812, loss: 0.583 (data_loss: 0.539, reg_loss: 0.044), lr: 0.019996481 validation, acc: 0.803, loss: 0.565 \n",
            "epoch: 353, acc: 0.790, loss: 0.654 (data_loss: 0.611, reg_loss: 0.042), lr: 0.019996471 validation, acc: 0.817, loss: 0.515 \n",
            "epoch: 354, acc: 0.808, loss: 0.582 (data_loss: 0.541, reg_loss: 0.042), lr: 0.019996461 validation, acc: 0.806, loss: 0.556 \n",
            "epoch: 355, acc: 0.802, loss: 0.628 (data_loss: 0.586, reg_loss: 0.041), lr: 0.019996451 validation, acc: 0.815, loss: 0.549 \n",
            "epoch: 356, acc: 0.798, loss: 0.638 (data_loss: 0.596, reg_loss: 0.042), lr: 0.019996441 validation, acc: 0.800, loss: 0.568 \n",
            "epoch: 357, acc: 0.795, loss: 0.639 (data_loss: 0.596, reg_loss: 0.042), lr: 0.019996431 validation, acc: 0.803, loss: 0.555 \n",
            "epoch: 358, acc: 0.797, loss: 0.629 (data_loss: 0.586, reg_loss: 0.043), lr: 0.019996421 validation, acc: 0.811, loss: 0.535 \n",
            "epoch: 359, acc: 0.804, loss: 0.617 (data_loss: 0.573, reg_loss: 0.044), lr: 0.019996411 validation, acc: 0.826, loss: 0.515 \n",
            "epoch: 360, acc: 0.812, loss: 0.592 (data_loss: 0.548, reg_loss: 0.044), lr: 0.019996401 validation, acc: 0.820, loss: 0.517 \n",
            "epoch: 361, acc: 0.806, loss: 0.613 (data_loss: 0.568, reg_loss: 0.045), lr: 0.019996391 validation, acc: 0.829, loss: 0.498 \n",
            "epoch: 362, acc: 0.816, loss: 0.577 (data_loss: 0.532, reg_loss: 0.045), lr: 0.019996381 validation, acc: 0.823, loss: 0.514 \n",
            "epoch: 363, acc: 0.816, loss: 0.582 (data_loss: 0.537, reg_loss: 0.045), lr: 0.019996371 validation, acc: 0.836, loss: 0.486 \n",
            "epoch: 364, acc: 0.825, loss: 0.552 (data_loss: 0.507, reg_loss: 0.045), lr: 0.019996361 validation, acc: 0.834, loss: 0.489 \n",
            "epoch: 365, acc: 0.823, loss: 0.555 (data_loss: 0.510, reg_loss: 0.044), lr: 0.019996351 validation, acc: 0.834, loss: 0.482 \n",
            "epoch: 366, acc: 0.827, loss: 0.546 (data_loss: 0.502, reg_loss: 0.044), lr: 0.019996341 validation, acc: 0.840, loss: 0.474 \n",
            "epoch: 367, acc: 0.831, loss: 0.535 (data_loss: 0.491, reg_loss: 0.043), lr: 0.019996331 validation, acc: 0.837, loss: 0.473 \n",
            "epoch: 368, acc: 0.828, loss: 0.532 (data_loss: 0.489, reg_loss: 0.042), lr: 0.019996321 validation, acc: 0.835, loss: 0.474 \n",
            "epoch: 369, acc: 0.833, loss: 0.530 (data_loss: 0.489, reg_loss: 0.041), lr: 0.019996311 validation, acc: 0.841, loss: 0.461 \n",
            "epoch: 370, acc: 0.837, loss: 0.517 (data_loss: 0.478, reg_loss: 0.040), lr: 0.019996301 validation, acc: 0.842, loss: 0.460 \n",
            "epoch: 371, acc: 0.839, loss: 0.506 (data_loss: 0.468, reg_loss: 0.038), lr: 0.019996291 validation, acc: 0.842, loss: 0.463 \n",
            "epoch: 372, acc: 0.840, loss: 0.504 (data_loss: 0.467, reg_loss: 0.036), lr: 0.019996281 validation, acc: 0.844, loss: 0.450 \n",
            "epoch: 373, acc: 0.841, loss: 0.495 (data_loss: 0.460, reg_loss: 0.035), lr: 0.019996271 validation, acc: 0.846, loss: 0.451 \n",
            "epoch: 374, acc: 0.841, loss: 0.494 (data_loss: 0.462, reg_loss: 0.033), lr: 0.019996261 validation, acc: 0.839, loss: 0.463 \n",
            "epoch: 375, acc: 0.839, loss: 0.500 (data_loss: 0.469, reg_loss: 0.031), lr: 0.019996251 validation, acc: 0.810, loss: 0.543 \n",
            "epoch: 376, acc: 0.804, loss: 0.588 (data_loss: 0.559, reg_loss: 0.030), lr: 0.019996241 validation, acc: 0.781, loss: 0.631 \n",
            "epoch: 377, acc: 0.765, loss: 0.704 (data_loss: 0.675, reg_loss: 0.028), lr: 0.019996231 validation, acc: 0.739, loss: 0.701 \n",
            "epoch: 378, acc: 0.736, loss: 0.766 (data_loss: 0.738, reg_loss: 0.028), lr: 0.019996221 validation, acc: 0.678, loss: 0.900 \n",
            "epoch: 379, acc: 0.645, loss: 1.038 (data_loss: 1.009, reg_loss: 0.029), lr: 0.019996211 validation, acc: 0.705, loss: 0.828 \n",
            "epoch: 380, acc: 0.683, loss: 0.984 (data_loss: 0.953, reg_loss: 0.031), lr: 0.019996201 validation, acc: 0.729, loss: 0.780 \n",
            "epoch: 381, acc: 0.705, loss: 0.934 (data_loss: 0.900, reg_loss: 0.035), lr: 0.019996191 validation, acc: 0.760, loss: 0.732 \n",
            "epoch: 382, acc: 0.716, loss: 0.948 (data_loss: 0.908, reg_loss: 0.040), lr: 0.019996181 validation, acc: 0.713, loss: 1.037 \n",
            "epoch: 383, acc: 0.694, loss: 1.219 (data_loss: 1.174, reg_loss: 0.045), lr: 0.019996171 validation, acc: 0.735, loss: 0.918 \n",
            "epoch: 384, acc: 0.699, loss: 1.143 (data_loss: 1.092, reg_loss: 0.050), lr: 0.019996161 validation, acc: 0.762, loss: 0.716 \n",
            "epoch: 385, acc: 0.720, loss: 0.918 (data_loss: 0.862, reg_loss: 0.056), lr: 0.019996151 validation, acc: 0.767, loss: 0.686 \n",
            "epoch: 386, acc: 0.733, loss: 0.851 (data_loss: 0.790, reg_loss: 0.061), lr: 0.019996141 validation, acc: 0.772, loss: 0.644 \n",
            "epoch: 387, acc: 0.745, loss: 0.817 (data_loss: 0.751, reg_loss: 0.066), lr: 0.019996131 validation, acc: 0.786, loss: 0.617 \n",
            "epoch: 388, acc: 0.765, loss: 0.763 (data_loss: 0.693, reg_loss: 0.070), lr: 0.019996121 validation, acc: 0.790, loss: 0.596 \n",
            "epoch: 389, acc: 0.770, loss: 0.737 (data_loss: 0.663, reg_loss: 0.073), lr: 0.019996111 validation, acc: 0.791, loss: 0.590 \n",
            "epoch: 390, acc: 0.773, loss: 0.723 (data_loss: 0.647, reg_loss: 0.076), lr: 0.019996101 validation, acc: 0.795, loss: 0.583 \n",
            "epoch: 391, acc: 0.780, loss: 0.711 (data_loss: 0.634, reg_loss: 0.077), lr: 0.019996091 validation, acc: 0.808, loss: 0.559 \n",
            "epoch: 392, acc: 0.785, loss: 0.696 (data_loss: 0.618, reg_loss: 0.078), lr: 0.019996081 validation, acc: 0.815, loss: 0.544 \n",
            "epoch: 393, acc: 0.797, loss: 0.662 (data_loss: 0.585, reg_loss: 0.077), lr: 0.019996071 validation, acc: 0.811, loss: 0.545 \n",
            "epoch: 394, acc: 0.798, loss: 0.654 (data_loss: 0.578, reg_loss: 0.076), lr: 0.019996061 validation, acc: 0.810, loss: 0.540 \n",
            "epoch: 395, acc: 0.803, loss: 0.641 (data_loss: 0.567, reg_loss: 0.074), lr: 0.019996051 validation, acc: 0.818, loss: 0.528 \n",
            "epoch: 396, acc: 0.807, loss: 0.622 (data_loss: 0.551, reg_loss: 0.071), lr: 0.019996041 validation, acc: 0.819, loss: 0.524 \n",
            "epoch: 397, acc: 0.809, loss: 0.617 (data_loss: 0.548, reg_loss: 0.069), lr: 0.019996031 validation, acc: 0.824, loss: 0.513 \n",
            "epoch: 398, acc: 0.814, loss: 0.604 (data_loss: 0.539, reg_loss: 0.066), lr: 0.019996021 validation, acc: 0.825, loss: 0.503 \n",
            "epoch: 399, acc: 0.815, loss: 0.592 (data_loss: 0.529, reg_loss: 0.063), lr: 0.019996011 validation, acc: 0.826, loss: 0.499 \n",
            "epoch: 400, acc: 0.819, loss: 0.578 (data_loss: 0.518, reg_loss: 0.059), lr: 0.019996001 validation, acc: 0.829, loss: 0.492 \n",
            "epoch: 401, acc: 0.825, loss: 0.561 (data_loss: 0.504, reg_loss: 0.056), lr: 0.019995991 validation, acc: 0.830, loss: 0.492 \n",
            "epoch: 402, acc: 0.825, loss: 0.553 (data_loss: 0.500, reg_loss: 0.053), lr: 0.019995981 validation, acc: 0.829, loss: 0.490 \n",
            "epoch: 403, acc: 0.825, loss: 0.554 (data_loss: 0.503, reg_loss: 0.051), lr: 0.019995971 validation, acc: 0.832, loss: 0.496 \n",
            "epoch: 404, acc: 0.829, loss: 0.546 (data_loss: 0.498, reg_loss: 0.048), lr: 0.019995961 validation, acc: 0.833, loss: 0.476 \n",
            "epoch: 405, acc: 0.826, loss: 0.539 (data_loss: 0.493, reg_loss: 0.045), lr: 0.019995951 validation, acc: 0.834, loss: 0.475 \n",
            "epoch: 406, acc: 0.828, loss: 0.531 (data_loss: 0.488, reg_loss: 0.043), lr: 0.019995941 validation, acc: 0.832, loss: 0.480 \n",
            "epoch: 407, acc: 0.830, loss: 0.528 (data_loss: 0.487, reg_loss: 0.041), lr: 0.019995931 validation, acc: 0.837, loss: 0.470 \n",
            "epoch: 408, acc: 0.830, loss: 0.532 (data_loss: 0.493, reg_loss: 0.038), lr: 0.019995921 validation, acc: 0.834, loss: 0.485 \n",
            "epoch: 409, acc: 0.833, loss: 0.526 (data_loss: 0.490, reg_loss: 0.037), lr: 0.019995911 validation, acc: 0.829, loss: 0.497 \n",
            "epoch: 410, acc: 0.825, loss: 0.544 (data_loss: 0.509, reg_loss: 0.035), lr: 0.019995901 validation, acc: 0.809, loss: 0.539 \n",
            "epoch: 411, acc: 0.798, loss: 0.602 (data_loss: 0.569, reg_loss: 0.034), lr: 0.019995891 validation, acc: 0.793, loss: 0.564 \n",
            "epoch: 412, acc: 0.785, loss: 0.629 (data_loss: 0.596, reg_loss: 0.033), lr: 0.019995881 validation, acc: 0.808, loss: 0.561 \n",
            "epoch: 413, acc: 0.797, loss: 0.617 (data_loss: 0.584, reg_loss: 0.033), lr: 0.019995871 validation, acc: 0.826, loss: 0.513 \n",
            "epoch: 414, acc: 0.813, loss: 0.579 (data_loss: 0.545, reg_loss: 0.034), lr: 0.019995861 validation, acc: 0.825, loss: 0.510 \n",
            "epoch: 415, acc: 0.810, loss: 0.571 (data_loss: 0.536, reg_loss: 0.035), lr: 0.019995851 validation, acc: 0.819, loss: 0.534 \n",
            "epoch: 416, acc: 0.809, loss: 0.595 (data_loss: 0.559, reg_loss: 0.036), lr: 0.019995841 validation, acc: 0.800, loss: 0.570 \n",
            "epoch: 417, acc: 0.792, loss: 0.626 (data_loss: 0.588, reg_loss: 0.038), lr: 0.019995831 validation, acc: 0.814, loss: 0.530 \n",
            "epoch: 418, acc: 0.800, loss: 0.613 (data_loss: 0.574, reg_loss: 0.039), lr: 0.019995821 validation, acc: 0.802, loss: 0.558 \n",
            "epoch: 419, acc: 0.794, loss: 0.631 (data_loss: 0.590, reg_loss: 0.041), lr: 0.019995811 validation, acc: 0.814, loss: 0.536 \n",
            "epoch: 420, acc: 0.803, loss: 0.606 (data_loss: 0.565, reg_loss: 0.042), lr: 0.019995801 validation, acc: 0.828, loss: 0.523 \n",
            "epoch: 421, acc: 0.814, loss: 0.597 (data_loss: 0.553, reg_loss: 0.043), lr: 0.019995791 validation, acc: 0.830, loss: 0.506 \n",
            "epoch: 422, acc: 0.820, loss: 0.570 (data_loss: 0.526, reg_loss: 0.044), lr: 0.019995781 validation, acc: 0.830, loss: 0.497 \n",
            "epoch: 423, acc: 0.822, loss: 0.560 (data_loss: 0.515, reg_loss: 0.045), lr: 0.019995771 validation, acc: 0.833, loss: 0.483 \n",
            "epoch: 424, acc: 0.822, loss: 0.552 (data_loss: 0.507, reg_loss: 0.045), lr: 0.019995761 validation, acc: 0.839, loss: 0.477 \n",
            "epoch: 425, acc: 0.830, loss: 0.542 (data_loss: 0.498, reg_loss: 0.045), lr: 0.019995751 validation, acc: 0.842, loss: 0.483 \n",
            "epoch: 426, acc: 0.832, loss: 0.538 (data_loss: 0.494, reg_loss: 0.044), lr: 0.019995741 validation, acc: 0.841, loss: 0.479 \n",
            "epoch: 427, acc: 0.831, loss: 0.535 (data_loss: 0.492, reg_loss: 0.043), lr: 0.019995731 validation, acc: 0.829, loss: 0.500 \n",
            "epoch: 428, acc: 0.823, loss: 0.555 (data_loss: 0.513, reg_loss: 0.042), lr: 0.019995721 validation, acc: 0.788, loss: 0.594 \n",
            "epoch: 429, acc: 0.781, loss: 0.667 (data_loss: 0.626, reg_loss: 0.041), lr: 0.019995711 validation, acc: 0.817, loss: 0.544 \n",
            "epoch: 430, acc: 0.796, loss: 0.637 (data_loss: 0.597, reg_loss: 0.040), lr: 0.019995701 validation, acc: 0.798, loss: 0.570 \n",
            "epoch: 431, acc: 0.777, loss: 0.672 (data_loss: 0.633, reg_loss: 0.039), lr: 0.019995691 validation, acc: 0.806, loss: 0.543 \n",
            "epoch: 432, acc: 0.785, loss: 0.638 (data_loss: 0.600, reg_loss: 0.038), lr: 0.019995681 validation, acc: 0.790, loss: 0.612 \n",
            "epoch: 433, acc: 0.774, loss: 0.709 (data_loss: 0.671, reg_loss: 0.038), lr: 0.019995671 validation, acc: 0.792, loss: 0.602 \n",
            "epoch: 434, acc: 0.767, loss: 0.727 (data_loss: 0.689, reg_loss: 0.038), lr: 0.019995661 validation, acc: 0.741, loss: 0.745 \n",
            "epoch: 435, acc: 0.729, loss: 0.835 (data_loss: 0.796, reg_loss: 0.039), lr: 0.019995651 validation, acc: 0.797, loss: 0.602 \n",
            "epoch: 436, acc: 0.769, loss: 0.724 (data_loss: 0.683, reg_loss: 0.041), lr: 0.019995641 validation, acc: 0.794, loss: 0.597 \n",
            "epoch: 437, acc: 0.772, loss: 0.698 (data_loss: 0.654, reg_loss: 0.043), lr: 0.019995631 validation, acc: 0.802, loss: 0.580 \n",
            "epoch: 438, acc: 0.786, loss: 0.665 (data_loss: 0.619, reg_loss: 0.046), lr: 0.019995621 validation, acc: 0.818, loss: 0.532 \n",
            "epoch: 439, acc: 0.796, loss: 0.628 (data_loss: 0.579, reg_loss: 0.048), lr: 0.019995611 validation, acc: 0.819, loss: 0.534 \n",
            "epoch: 440, acc: 0.802, loss: 0.627 (data_loss: 0.577, reg_loss: 0.051), lr: 0.019995601 validation, acc: 0.822, loss: 0.520 \n",
            "epoch: 441, acc: 0.808, loss: 0.605 (data_loss: 0.552, reg_loss: 0.053), lr: 0.019995591 validation, acc: 0.820, loss: 0.528 \n",
            "epoch: 442, acc: 0.804, loss: 0.617 (data_loss: 0.562, reg_loss: 0.054), lr: 0.019995581 validation, acc: 0.827, loss: 0.499 \n",
            "epoch: 443, acc: 0.809, loss: 0.598 (data_loss: 0.542, reg_loss: 0.055), lr: 0.019995571 validation, acc: 0.827, loss: 0.505 \n",
            "epoch: 444, acc: 0.813, loss: 0.589 (data_loss: 0.533, reg_loss: 0.056), lr: 0.019995561 validation, acc: 0.828, loss: 0.504 \n",
            "epoch: 445, acc: 0.819, loss: 0.579 (data_loss: 0.523, reg_loss: 0.056), lr: 0.019995551 validation, acc: 0.832, loss: 0.483 \n",
            "epoch: 446, acc: 0.820, loss: 0.569 (data_loss: 0.514, reg_loss: 0.055), lr: 0.019995541 validation, acc: 0.831, loss: 0.488 \n",
            "epoch: 447, acc: 0.822, loss: 0.567 (data_loss: 0.513, reg_loss: 0.054), lr: 0.019995531 validation, acc: 0.832, loss: 0.497 \n",
            "epoch: 448, acc: 0.823, loss: 0.561 (data_loss: 0.509, reg_loss: 0.052), lr: 0.019995521 validation, acc: 0.823, loss: 0.516 \n",
            "epoch: 449, acc: 0.810, loss: 0.602 (data_loss: 0.552, reg_loss: 0.050), lr: 0.019995511 validation, acc: 0.824, loss: 0.498 \n",
            "epoch: 450, acc: 0.817, loss: 0.569 (data_loss: 0.520, reg_loss: 0.049), lr: 0.019995501 validation, acc: 0.837, loss: 0.479 \n",
            "epoch: 451, acc: 0.825, loss: 0.551 (data_loss: 0.504, reg_loss: 0.047), lr: 0.019995491 validation, acc: 0.826, loss: 0.516 \n",
            "epoch: 452, acc: 0.813, loss: 0.589 (data_loss: 0.543, reg_loss: 0.045), lr: 0.019995481 validation, acc: 0.825, loss: 0.516 \n",
            "epoch: 453, acc: 0.818, loss: 0.572 (data_loss: 0.528, reg_loss: 0.044), lr: 0.019995471 validation, acc: 0.835, loss: 0.479 \n",
            "epoch: 454, acc: 0.827, loss: 0.544 (data_loss: 0.501, reg_loss: 0.043), lr: 0.019995461 validation, acc: 0.836, loss: 0.480 \n",
            "epoch: 455, acc: 0.828, loss: 0.543 (data_loss: 0.501, reg_loss: 0.042), lr: 0.019995451 validation, acc: 0.830, loss: 0.494 \n",
            "epoch: 456, acc: 0.825, loss: 0.551 (data_loss: 0.509, reg_loss: 0.042), lr: 0.019995441 validation, acc: 0.832, loss: 0.484 \n",
            "epoch: 457, acc: 0.828, loss: 0.540 (data_loss: 0.498, reg_loss: 0.042), lr: 0.019995431 validation, acc: 0.829, loss: 0.487 \n",
            "epoch: 458, acc: 0.825, loss: 0.546 (data_loss: 0.504, reg_loss: 0.042), lr: 0.019995421 validation, acc: 0.842, loss: 0.468 \n",
            "epoch: 459, acc: 0.839, loss: 0.515 (data_loss: 0.473, reg_loss: 0.042), lr: 0.019995411 validation, acc: 0.839, loss: 0.478 \n",
            "epoch: 460, acc: 0.836, loss: 0.524 (data_loss: 0.482, reg_loss: 0.042), lr: 0.019995401 validation, acc: 0.839, loss: 0.469 \n",
            "epoch: 461, acc: 0.831, loss: 0.533 (data_loss: 0.490, reg_loss: 0.042), lr: 0.019995391 validation, acc: 0.831, loss: 0.485 \n",
            "epoch: 462, acc: 0.828, loss: 0.534 (data_loss: 0.491, reg_loss: 0.042), lr: 0.019995381 validation, acc: 0.835, loss: 0.490 \n",
            "epoch: 463, acc: 0.824, loss: 0.545 (data_loss: 0.503, reg_loss: 0.042), lr: 0.019995371 validation, acc: 0.787, loss: 0.607 \n",
            "epoch: 464, acc: 0.781, loss: 0.665 (data_loss: 0.624, reg_loss: 0.042), lr: 0.019995361 validation, acc: 0.691, loss: 0.939 \n",
            "epoch: 465, acc: 0.688, loss: 1.050 (data_loss: 1.009, reg_loss: 0.041), lr: 0.019995351 validation, acc: 0.705, loss: 1.331 \n",
            "epoch: 466, acc: 0.681, loss: 1.511 (data_loss: 1.469, reg_loss: 0.042), lr: 0.019995341 validation, acc: 0.674, loss: 1.162 \n",
            "epoch: 467, acc: 0.652, loss: 1.384 (data_loss: 1.339, reg_loss: 0.045), lr: 0.019995331 validation, acc: 0.636, loss: 1.257 \n",
            "epoch: 468, acc: 0.607, loss: 1.593 (data_loss: 1.544, reg_loss: 0.049), lr: 0.019995321 validation, acc: 0.736, loss: 0.771 \n",
            "epoch: 469, acc: 0.689, loss: 0.983 (data_loss: 0.929, reg_loss: 0.054), lr: 0.019995311 validation, acc: 0.715, loss: 0.861 \n",
            "epoch: 470, acc: 0.675, loss: 1.035 (data_loss: 0.976, reg_loss: 0.059), lr: 0.019995301 validation, acc: 0.765, loss: 0.710 \n",
            "epoch: 471, acc: 0.716, loss: 0.926 (data_loss: 0.862, reg_loss: 0.064), lr: 0.019995291 validation, acc: 0.771, loss: 0.669 \n",
            "epoch: 472, acc: 0.726, loss: 0.910 (data_loss: 0.841, reg_loss: 0.069), lr: 0.019995281 validation, acc: 0.794, loss: 0.610 \n",
            "epoch: 473, acc: 0.751, loss: 0.803 (data_loss: 0.730, reg_loss: 0.074), lr: 0.019995271 validation, acc: 0.790, loss: 0.605 \n",
            "epoch: 474, acc: 0.760, loss: 0.775 (data_loss: 0.697, reg_loss: 0.078), lr: 0.019995261 validation, acc: 0.787, loss: 0.621 \n",
            "epoch: 475, acc: 0.763, loss: 0.777 (data_loss: 0.696, reg_loss: 0.081), lr: 0.019995251 validation, acc: 0.798, loss: 0.597 \n",
            "epoch: 476, acc: 0.777, loss: 0.737 (data_loss: 0.654, reg_loss: 0.083), lr: 0.019995241 validation, acc: 0.799, loss: 0.602 \n",
            "epoch: 477, acc: 0.779, loss: 0.736 (data_loss: 0.652, reg_loss: 0.084), lr: 0.019995231 validation, acc: 0.808, loss: 0.581 \n",
            "epoch: 478, acc: 0.792, loss: 0.696 (data_loss: 0.612, reg_loss: 0.084), lr: 0.019995221 validation, acc: 0.802, loss: 0.582 \n",
            "epoch: 479, acc: 0.792, loss: 0.692 (data_loss: 0.608, reg_loss: 0.084), lr: 0.019995211 validation, acc: 0.812, loss: 0.567 \n",
            "epoch: 480, acc: 0.801, loss: 0.671 (data_loss: 0.589, reg_loss: 0.082), lr: 0.019995201 validation, acc: 0.817, loss: 0.560 \n",
            "epoch: 481, acc: 0.804, loss: 0.661 (data_loss: 0.580, reg_loss: 0.081), lr: 0.019995191 validation, acc: 0.821, loss: 0.546 \n",
            "epoch: 482, acc: 0.808, loss: 0.650 (data_loss: 0.572, reg_loss: 0.078), lr: 0.019995181 validation, acc: 0.821, loss: 0.533 \n",
            "epoch: 483, acc: 0.810, loss: 0.635 (data_loss: 0.559, reg_loss: 0.076), lr: 0.019995171 validation, acc: 0.819, loss: 0.541 \n",
            "epoch: 484, acc: 0.809, loss: 0.640 (data_loss: 0.567, reg_loss: 0.073), lr: 0.019995161 validation, acc: 0.819, loss: 0.533 \n",
            "epoch: 485, acc: 0.808, loss: 0.626 (data_loss: 0.556, reg_loss: 0.070), lr: 0.019995151 validation, acc: 0.824, loss: 0.519 \n",
            "epoch: 486, acc: 0.811, loss: 0.612 (data_loss: 0.545, reg_loss: 0.067), lr: 0.019995141 validation, acc: 0.822, loss: 0.511 \n",
            "epoch: 487, acc: 0.818, loss: 0.597 (data_loss: 0.533, reg_loss: 0.064), lr: 0.019995131 validation, acc: 0.829, loss: 0.508 \n",
            "epoch: 488, acc: 0.820, loss: 0.589 (data_loss: 0.528, reg_loss: 0.061), lr: 0.019995121 validation, acc: 0.828, loss: 0.508 \n",
            "epoch: 489, acc: 0.822, loss: 0.576 (data_loss: 0.518, reg_loss: 0.058), lr: 0.019995111 validation, acc: 0.827, loss: 0.502 \n",
            "epoch: 490, acc: 0.823, loss: 0.567 (data_loss: 0.512, reg_loss: 0.055), lr: 0.019995101 validation, acc: 0.833, loss: 0.488 \n",
            "epoch: 491, acc: 0.827, loss: 0.554 (data_loss: 0.502, reg_loss: 0.052), lr: 0.019995091 validation, acc: 0.832, loss: 0.484 \n",
            "epoch: 492, acc: 0.827, loss: 0.551 (data_loss: 0.502, reg_loss: 0.050), lr: 0.019995081 validation, acc: 0.832, loss: 0.488 \n",
            "epoch: 493, acc: 0.828, loss: 0.545 (data_loss: 0.498, reg_loss: 0.047), lr: 0.019995071 validation, acc: 0.829, loss: 0.504 \n",
            "epoch: 494, acc: 0.808, loss: 0.581 (data_loss: 0.536, reg_loss: 0.045), lr: 0.019995061 validation, acc: 0.780, loss: 0.643 \n",
            "epoch: 495, acc: 0.774, loss: 0.706 (data_loss: 0.663, reg_loss: 0.043), lr: 0.019995051 validation, acc: 0.814, loss: 0.552 \n",
            "epoch: 496, acc: 0.800, loss: 0.614 (data_loss: 0.573, reg_loss: 0.041), lr: 0.019995041 validation, acc: 0.821, loss: 0.534 \n",
            "epoch: 497, acc: 0.806, loss: 0.601 (data_loss: 0.560, reg_loss: 0.041), lr: 0.019995031 validation, acc: 0.816, loss: 0.540 \n",
            "epoch: 498, acc: 0.796, loss: 0.617 (data_loss: 0.576, reg_loss: 0.042), lr: 0.019995021 validation, acc: 0.817, loss: 0.525 \n",
            "epoch: 499, acc: 0.798, loss: 0.607 (data_loss: 0.565, reg_loss: 0.042), lr: 0.019995011 validation, acc: 0.819, loss: 0.529 \n",
            "epoch: 500, acc: 0.806, loss: 0.600 (data_loss: 0.556, reg_loss: 0.043), lr: 0.019995001 validation, acc: 0.815, loss: 0.518 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KNnDUP_U8Xn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cf8b68-2b53-4dfb-cdb4-523dd8272356"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7603601131071192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbXMisqQKqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d4d9bc-cc31-44bc-cef3-48e4f2c77a67"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 50 epochs\n",
            "Max accuracy was 84.87% at epoch 311.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smwSXsZVU8Xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99e6e3a-45f0-4d1d-8f6c-ac4bc6520d39"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'training, acc: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training, acc: 0.815, loss: 0.518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5c5xUTNXk2v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "1b879446-1345-44e0-c53d-c306639648ea"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}