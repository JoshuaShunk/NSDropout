{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_implementation_of_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_implementation_of_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYgI3SFHqm4"
      },
      "source": [
        "# MNIST Numbers Implementation of My New Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HD9nTuEVHD"
      },
      "source": [
        "np.random.seed(seed=22) #Random seed used for comparison between old dropout"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cag8ZraxEZbF",
        "outputId": "baf28ced-4cfe-4243-a31f-89e1735340d5"
      },
      "source": [
        "print(np.random.random(size=3)) #Check that seeds line up"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20846054 0.48168106 0.42053804]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers (Credit to Harrison Kinsley & Daniel Kukiela for raw python implementation)\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Og9ZrbKtV"
      },
      "source": [
        "# New Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXBWsHDIUSfh"
      },
      "source": [
        "class Layer_BinaryNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = 1 - rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, inputs, val_inputs):\n",
        "        self.inputs = inputs\n",
        "        self.val_inputs = val_inputs\n",
        "        nummask = round(len(self.inputs[0]) * self.rate)\n",
        "        \n",
        "        #Averaging Values\n",
        "        self.meanarray1 = np.mean(inputs, axis=0)\n",
        "        self.meanarray2 = np.mean(val_inputs, axis=0)\n",
        "\n",
        "        if self.iterations != 0:\n",
        "            # Calculating value\n",
        "            self.difference = self.meanarray1 - self.meanarray2\n",
        "            ind = np.argpartition(self.difference, -nummask)[-nummask:]\n",
        "            mask = np.ones(self.meanarray1.shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            self.difference[~mask] = 1\n",
        "            self.difference[mask] = 0. \n",
        "            self.binary_mask = self.difference / self.rate\n",
        "\n",
        "        else:\n",
        "            self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                                  size=inputs.shape) / self.rate\n",
        "        self.output = inputs * self.binary_mask\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiuCzwWxbRl0"
      },
      "source": [
        "class Layer_CatagoricalNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, X_test, y_test, X, y):        \n",
        "        if self.iterations != 0:\n",
        "          #Sorting data into classes\n",
        "          idx = np.argsort(y_test)\n",
        "          X_test_sorted = X_test[idx]\n",
        "          y_test_sorted = y_test[idx]\n",
        "\n",
        "          idx2 = np.argsort(y)\n",
        "          X_train_sorted = X[idx2]\n",
        "          y_train_sorted = y[idx2]\n",
        "\n",
        "          #Adding sorted data into dictionaries \n",
        "          sorted_x = {}\n",
        "          sorted_y = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x[\"class_{0}\".format(classes)] = X[y == classes]\n",
        "            sorted_y[\"label_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "          sorted_x_test = {}\n",
        "          sorted_y_test = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x_test[\"class_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "            sorted_y_test[\"label_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "          #Averaging sorted data from each class then finding the difference between the averaged train and test inputs\n",
        "          differnce_classes = {}\n",
        "          for i, classes, test_classes in zip(range(len(set(y))), sorted_x, sorted_x_test):\n",
        "            differnce_classes[\"diff_{0}\".format(i)] = np.mean(sorted_x[classes], axis=0) - np.mean(sorted_x_test[classes], axis=0)\n",
        "\n",
        "          #Masking the data taking the high values(greatest difference between train and test) and setting their values to 0\n",
        "          self.diff_mask = {}\n",
        "          for i, classes, test_classes, diff in zip(range(len(set(y))), sorted_x, sorted_x_test, differnce_classes):\n",
        "            ind = np.argpartition(differnce_classes[diff], -round(len(X[0]) * self.rate))[-round(len(X[0]) * self.rate):]\n",
        "            mask = np.ones(np.mean(sorted_x[classes],axis=0).shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            differnce_classes[diff][~mask] = 0.\n",
        "            differnce_classes[diff][mask] = 1\n",
        "            self.diff_mask[\"mask_{0}\".format(i)] = differnce_classes[diff]\n",
        "\n",
        "          #Goes through each input values and applies the apprioprite mask based on what the true output should be.\n",
        "          binary_mask = np.empty(shape=X.shape)\n",
        "          #for i, input, label in zip(range(len(X)), X, y):\n",
        "          for i, (input, label) in enumerate(zip(X,y)): \n",
        "            for true, diff in enumerate(self.diff_mask):\n",
        "              if label == true:\n",
        "                self.binary_mask[i] = self.diff_mask[diff]\n",
        "        else:\n",
        "          self.binary_mask = np.random.binomial(1, (1-self.rate), size=X.shape)\n",
        "        \n",
        "        self.output = (self.binary_mask/(1-self.rate)) * X\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def infrence(self, input, label):\n",
        "        self.input = input\n",
        "        self.label = label\n",
        "        idx = np.argsort(self.label)\n",
        "        input_sorted = input[idx]\n",
        "        label_sorted = label[idx]\n",
        "        self.infrence_binary_mask = np.empty(shape=self.input.shape)\n",
        "        for i, (input, label) in enumerate(zip(self.input, self.label)):\n",
        "          #for true, diff in zip(range(len(set(self.label))),self.diff_mask):\n",
        "          for true, diff in enumerate(self.diff_mask):\n",
        "            if label == true:\n",
        "              self.infrence_binary_mask[i] = self.diff_mask[diff]\n",
        "\n",
        "        self.output = self.infrence_binary_mask * self.input\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "decbb49d-0adc-4540-dbea-e0df7e98c0a4"
      },
      "source": [
        "(X, y), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "X = X[:8000,:,:]\n",
        "X_test = X_test[:1600,:,:]\n",
        "y = y[:8000]\n",
        "y_test  = y_test[:1600]\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 784)\n",
            "(8000,)\n",
            "(1600, 784)\n",
            "(1600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nW7mqGTnem"
      },
      "source": [
        "Sorting Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtvA9y81TpGf",
        "outputId": "42dec268-83f2-4575-c4d5-381a85d05a9c"
      },
      "source": [
        "idx = np.argsort(y)\n",
        "X_sorted = X[idx]\n",
        "y_sorted = y[idx]\n",
        "\n",
        "sorted_x = {}\n",
        "sorted_y = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x[\"X_{0}\".format(classes)] = X[y == classes]\n",
        "  sorted_y[\"y_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x[sorted_lists].shape[0]}')\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_0: 772\n",
            "Number of Samples for X_1: 776\n",
            "Number of Samples for X_2: 833\n",
            "Number of Samples for X_3: 805\n",
            "Number of Samples for X_4: 768\n",
            "Number of Samples for X_5: 788\n",
            "Number of Samples for X_6: 827\n",
            "Number of Samples for X_7: 796\n",
            "Number of Samples for X_8: 839\n",
            "Number of Samples for X_9: 796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaNaUO3kP2G"
      },
      "source": [
        "Sorting Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBLFeGAUkSOs",
        "outputId": "5f7edd63-d2e8-4dec-faf4-6de32fdb6f39"
      },
      "source": [
        "idx = np.argsort(y_test)\n",
        "X_test_sorted = X_test[idx]\n",
        "y_test_sorted = y_test[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_test = {}\n",
        "sorted_y_test = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_test[\"X_test_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "  sorted_y_test[\"y_test_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_test:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_test[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_test[sorted_lists].shape[0])\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_test_0: 161\n",
            "Number of Samples for X_test_1: 162\n",
            "Number of Samples for X_test_2: 177\n",
            "Number of Samples for X_test_3: 148\n",
            "Number of Samples for X_test_4: 184\n",
            "Number of Samples for X_test_5: 158\n",
            "Number of Samples for X_test_6: 158\n",
            "Number of Samples for X_test_7: 154\n",
            "Number of Samples for X_test_8: 151\n",
            "Number of Samples for X_test_9: 147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbnq4TJp1cl",
        "outputId": "20afaaf5-bc3c-48f6-ee05-fe3dc85b5a7f"
      },
      "source": [
        "print(f'Found {X.shape[0]} images belonging to {len(set(y))} unique classes')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 10 unique classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_Dropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,len(set(y)))\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7,learning_rate=0.005)\n",
        "#optimizer = Optimizer_SGD(learning_rate=0.01)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "1a1c9cdd-c228-49b8-ae8e-c8bd610bbede"
      },
      "source": [
        "epochs = 244\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    dropout1.forward(activation1.output)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    dense2.forward(activation1.output)\n",
        "    \n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        "    \n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.082, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.005000000 validation, acc: 0.176, loss: 2.302 \n",
            "epoch: 1, acc: 0.185, loss: 2.304 (data_loss: 2.302, reg_loss: 0.002), lr: 0.004999998 validation, acc: 0.176, loss: 2.295 \n",
            "epoch: 2, acc: 0.180, loss: 2.295 (data_loss: 2.294, reg_loss: 0.001), lr: 0.004999995 validation, acc: 0.241, loss: 2.235 \n",
            "epoch: 3, acc: 0.247, loss: 2.235 (data_loss: 2.234, reg_loss: 0.001), lr: 0.004999993 validation, acc: 0.206, loss: 2.048 \n",
            "epoch: 4, acc: 0.231, loss: 2.048 (data_loss: 2.046, reg_loss: 0.002), lr: 0.004999990 validation, acc: 0.195, loss: 1.844 \n",
            "epoch: 5, acc: 0.209, loss: 1.850 (data_loss: 1.846, reg_loss: 0.004), lr: 0.004999988 validation, acc: 0.208, loss: 2.103 \n",
            "epoch: 6, acc: 0.195, loss: 2.122 (data_loss: 2.117, reg_loss: 0.005), lr: 0.004999985 validation, acc: 0.266, loss: 1.664 \n",
            "epoch: 7, acc: 0.272, loss: 1.658 (data_loss: 1.651, reg_loss: 0.006), lr: 0.004999983 validation, acc: 0.368, loss: 1.618 \n",
            "epoch: 8, acc: 0.378, loss: 1.611 (data_loss: 1.604, reg_loss: 0.007), lr: 0.004999980 validation, acc: 0.306, loss: 1.632 \n",
            "epoch: 9, acc: 0.311, loss: 1.627 (data_loss: 1.619, reg_loss: 0.008), lr: 0.004999978 validation, acc: 0.384, loss: 1.545 \n",
            "epoch: 10, acc: 0.388, loss: 1.543 (data_loss: 1.534, reg_loss: 0.009), lr: 0.004999975 validation, acc: 0.423, loss: 1.466 \n",
            "epoch: 11, acc: 0.424, loss: 1.475 (data_loss: 1.465, reg_loss: 0.010), lr: 0.004999973 validation, acc: 0.442, loss: 1.390 \n",
            "epoch: 12, acc: 0.454, loss: 1.405 (data_loss: 1.394, reg_loss: 0.011), lr: 0.004999970 validation, acc: 0.452, loss: 1.301 \n",
            "epoch: 13, acc: 0.458, loss: 1.317 (data_loss: 1.305, reg_loss: 0.012), lr: 0.004999968 validation, acc: 0.443, loss: 1.246 \n",
            "epoch: 14, acc: 0.448, loss: 1.268 (data_loss: 1.254, reg_loss: 0.014), lr: 0.004999965 validation, acc: 0.508, loss: 1.173 \n",
            "epoch: 15, acc: 0.510, loss: 1.191 (data_loss: 1.177, reg_loss: 0.015), lr: 0.004999963 validation, acc: 0.570, loss: 1.105 \n",
            "epoch: 16, acc: 0.566, loss: 1.118 (data_loss: 1.103, reg_loss: 0.016), lr: 0.004999960 validation, acc: 0.545, loss: 1.086 \n",
            "epoch: 17, acc: 0.565, loss: 1.092 (data_loss: 1.075, reg_loss: 0.017), lr: 0.004999958 validation, acc: 0.552, loss: 1.105 \n",
            "epoch: 18, acc: 0.572, loss: 1.117 (data_loss: 1.099, reg_loss: 0.017), lr: 0.004999955 validation, acc: 0.566, loss: 1.023 \n",
            "epoch: 19, acc: 0.579, loss: 1.047 (data_loss: 1.029, reg_loss: 0.018), lr: 0.004999953 validation, acc: 0.573, loss: 1.007 \n",
            "epoch: 20, acc: 0.578, loss: 1.031 (data_loss: 1.012, reg_loss: 0.019), lr: 0.004999950 validation, acc: 0.605, loss: 0.974 \n",
            "epoch: 21, acc: 0.623, loss: 0.989 (data_loss: 0.969, reg_loss: 0.020), lr: 0.004999948 validation, acc: 0.621, loss: 0.966 \n",
            "epoch: 22, acc: 0.635, loss: 0.979 (data_loss: 0.959, reg_loss: 0.020), lr: 0.004999945 validation, acc: 0.634, loss: 0.963 \n",
            "epoch: 23, acc: 0.634, loss: 0.977 (data_loss: 0.956, reg_loss: 0.020), lr: 0.004999943 validation, acc: 0.618, loss: 0.966 \n",
            "epoch: 24, acc: 0.615, loss: 0.982 (data_loss: 0.961, reg_loss: 0.021), lr: 0.004999940 validation, acc: 0.640, loss: 0.947 \n",
            "epoch: 25, acc: 0.654, loss: 0.959 (data_loss: 0.938, reg_loss: 0.021), lr: 0.004999938 validation, acc: 0.668, loss: 0.921 \n",
            "epoch: 26, acc: 0.681, loss: 0.931 (data_loss: 0.910, reg_loss: 0.021), lr: 0.004999935 validation, acc: 0.664, loss: 0.916 \n",
            "epoch: 27, acc: 0.670, loss: 0.928 (data_loss: 0.907, reg_loss: 0.021), lr: 0.004999933 validation, acc: 0.656, loss: 0.904 \n",
            "epoch: 28, acc: 0.679, loss: 0.905 (data_loss: 0.884, reg_loss: 0.021), lr: 0.004999930 validation, acc: 0.704, loss: 0.876 \n",
            "epoch: 29, acc: 0.695, loss: 0.888 (data_loss: 0.867, reg_loss: 0.021), lr: 0.004999928 validation, acc: 0.710, loss: 0.883 \n",
            "epoch: 30, acc: 0.701, loss: 0.905 (data_loss: 0.884, reg_loss: 0.021), lr: 0.004999925 validation, acc: 0.698, loss: 0.866 \n",
            "epoch: 31, acc: 0.710, loss: 0.863 (data_loss: 0.842, reg_loss: 0.021), lr: 0.004999923 validation, acc: 0.719, loss: 0.822 \n",
            "epoch: 32, acc: 0.717, loss: 0.838 (data_loss: 0.817, reg_loss: 0.021), lr: 0.004999920 validation, acc: 0.726, loss: 0.810 \n",
            "epoch: 33, acc: 0.725, loss: 0.827 (data_loss: 0.806, reg_loss: 0.021), lr: 0.004999918 validation, acc: 0.728, loss: 0.790 \n",
            "epoch: 34, acc: 0.734, loss: 0.804 (data_loss: 0.783, reg_loss: 0.021), lr: 0.004999915 validation, acc: 0.735, loss: 0.764 \n",
            "epoch: 35, acc: 0.739, loss: 0.792 (data_loss: 0.772, reg_loss: 0.021), lr: 0.004999913 validation, acc: 0.742, loss: 0.749 \n",
            "epoch: 36, acc: 0.746, loss: 0.769 (data_loss: 0.748, reg_loss: 0.021), lr: 0.004999910 validation, acc: 0.738, loss: 0.747 \n",
            "epoch: 37, acc: 0.751, loss: 0.762 (data_loss: 0.741, reg_loss: 0.020), lr: 0.004999908 validation, acc: 0.752, loss: 0.718 \n",
            "epoch: 38, acc: 0.753, loss: 0.741 (data_loss: 0.721, reg_loss: 0.020), lr: 0.004999905 validation, acc: 0.750, loss: 0.718 \n",
            "epoch: 39, acc: 0.760, loss: 0.727 (data_loss: 0.707, reg_loss: 0.020), lr: 0.004999903 validation, acc: 0.755, loss: 0.696 \n",
            "epoch: 40, acc: 0.766, loss: 0.715 (data_loss: 0.695, reg_loss: 0.020), lr: 0.004999900 validation, acc: 0.746, loss: 0.711 \n",
            "epoch: 41, acc: 0.749, loss: 0.732 (data_loss: 0.712, reg_loss: 0.020), lr: 0.004999898 validation, acc: 0.720, loss: 0.770 \n",
            "epoch: 42, acc: 0.723, loss: 0.799 (data_loss: 0.779, reg_loss: 0.020), lr: 0.004999895 validation, acc: 0.739, loss: 0.761 \n",
            "epoch: 43, acc: 0.745, loss: 0.770 (data_loss: 0.750, reg_loss: 0.020), lr: 0.004999893 validation, acc: 0.749, loss: 0.721 \n",
            "epoch: 44, acc: 0.754, loss: 0.730 (data_loss: 0.710, reg_loss: 0.020), lr: 0.004999890 validation, acc: 0.754, loss: 0.688 \n",
            "epoch: 45, acc: 0.760, loss: 0.712 (data_loss: 0.692, reg_loss: 0.020), lr: 0.004999888 validation, acc: 0.771, loss: 0.656 \n",
            "epoch: 46, acc: 0.776, loss: 0.676 (data_loss: 0.656, reg_loss: 0.020), lr: 0.004999885 validation, acc: 0.765, loss: 0.673 \n",
            "epoch: 47, acc: 0.769, loss: 0.699 (data_loss: 0.679, reg_loss: 0.020), lr: 0.004999883 validation, acc: 0.781, loss: 0.643 \n",
            "epoch: 48, acc: 0.787, loss: 0.663 (data_loss: 0.643, reg_loss: 0.020), lr: 0.004999880 validation, acc: 0.787, loss: 0.632 \n",
            "epoch: 49, acc: 0.789, loss: 0.660 (data_loss: 0.639, reg_loss: 0.020), lr: 0.004999878 validation, acc: 0.786, loss: 0.617 \n",
            "epoch: 50, acc: 0.792, loss: 0.649 (data_loss: 0.628, reg_loss: 0.021), lr: 0.004999875 validation, acc: 0.784, loss: 0.629 \n",
            "epoch: 51, acc: 0.783, loss: 0.658 (data_loss: 0.637, reg_loss: 0.021), lr: 0.004999873 validation, acc: 0.787, loss: 0.634 \n",
            "epoch: 52, acc: 0.786, loss: 0.650 (data_loss: 0.630, reg_loss: 0.021), lr: 0.004999870 validation, acc: 0.769, loss: 0.649 \n",
            "epoch: 53, acc: 0.771, loss: 0.676 (data_loss: 0.656, reg_loss: 0.020), lr: 0.004999868 validation, acc: 0.790, loss: 0.621 \n",
            "epoch: 54, acc: 0.792, loss: 0.643 (data_loss: 0.623, reg_loss: 0.020), lr: 0.004999865 validation, acc: 0.798, loss: 0.592 \n",
            "epoch: 55, acc: 0.796, loss: 0.616 (data_loss: 0.595, reg_loss: 0.020), lr: 0.004999863 validation, acc: 0.799, loss: 0.594 \n",
            "epoch: 56, acc: 0.801, loss: 0.607 (data_loss: 0.587, reg_loss: 0.020), lr: 0.004999860 validation, acc: 0.785, loss: 0.579 \n",
            "epoch: 57, acc: 0.801, loss: 0.601 (data_loss: 0.580, reg_loss: 0.020), lr: 0.004999858 validation, acc: 0.796, loss: 0.596 \n",
            "epoch: 58, acc: 0.801, loss: 0.605 (data_loss: 0.584, reg_loss: 0.020), lr: 0.004999855 validation, acc: 0.810, loss: 0.577 \n",
            "epoch: 59, acc: 0.810, loss: 0.590 (data_loss: 0.570, reg_loss: 0.020), lr: 0.004999853 validation, acc: 0.805, loss: 0.571 \n",
            "epoch: 60, acc: 0.816, loss: 0.588 (data_loss: 0.568, reg_loss: 0.020), lr: 0.004999850 validation, acc: 0.810, loss: 0.557 \n",
            "epoch: 61, acc: 0.817, loss: 0.565 (data_loss: 0.546, reg_loss: 0.020), lr: 0.004999848 validation, acc: 0.804, loss: 0.552 \n",
            "epoch: 62, acc: 0.822, loss: 0.566 (data_loss: 0.546, reg_loss: 0.020), lr: 0.004999845 validation, acc: 0.802, loss: 0.551 \n",
            "epoch: 63, acc: 0.814, loss: 0.556 (data_loss: 0.536, reg_loss: 0.020), lr: 0.004999843 validation, acc: 0.815, loss: 0.545 \n",
            "epoch: 64, acc: 0.827, loss: 0.553 (data_loss: 0.533, reg_loss: 0.019), lr: 0.004999840 validation, acc: 0.818, loss: 0.546 \n",
            "epoch: 65, acc: 0.834, loss: 0.540 (data_loss: 0.521, reg_loss: 0.019), lr: 0.004999838 validation, acc: 0.824, loss: 0.529 \n",
            "epoch: 66, acc: 0.835, loss: 0.534 (data_loss: 0.515, reg_loss: 0.019), lr: 0.004999835 validation, acc: 0.818, loss: 0.524 \n",
            "epoch: 67, acc: 0.837, loss: 0.525 (data_loss: 0.506, reg_loss: 0.019), lr: 0.004999833 validation, acc: 0.828, loss: 0.515 \n",
            "epoch: 68, acc: 0.839, loss: 0.517 (data_loss: 0.498, reg_loss: 0.019), lr: 0.004999830 validation, acc: 0.831, loss: 0.508 \n",
            "epoch: 69, acc: 0.839, loss: 0.523 (data_loss: 0.504, reg_loss: 0.019), lr: 0.004999828 validation, acc: 0.818, loss: 0.527 \n",
            "epoch: 70, acc: 0.837, loss: 0.523 (data_loss: 0.504, reg_loss: 0.019), lr: 0.004999825 validation, acc: 0.825, loss: 0.507 \n",
            "epoch: 71, acc: 0.844, loss: 0.517 (data_loss: 0.498, reg_loss: 0.018), lr: 0.004999823 validation, acc: 0.832, loss: 0.507 \n",
            "epoch: 72, acc: 0.844, loss: 0.506 (data_loss: 0.488, reg_loss: 0.018), lr: 0.004999820 validation, acc: 0.839, loss: 0.489 \n",
            "epoch: 73, acc: 0.850, loss: 0.497 (data_loss: 0.478, reg_loss: 0.018), lr: 0.004999818 validation, acc: 0.831, loss: 0.505 \n",
            "epoch: 74, acc: 0.841, loss: 0.511 (data_loss: 0.493, reg_loss: 0.018), lr: 0.004999815 validation, acc: 0.812, loss: 0.544 \n",
            "epoch: 75, acc: 0.834, loss: 0.521 (data_loss: 0.503, reg_loss: 0.018), lr: 0.004999813 validation, acc: 0.825, loss: 0.526 \n",
            "epoch: 76, acc: 0.839, loss: 0.528 (data_loss: 0.510, reg_loss: 0.018), lr: 0.004999810 validation, acc: 0.827, loss: 0.493 \n",
            "epoch: 77, acc: 0.853, loss: 0.479 (data_loss: 0.461, reg_loss: 0.018), lr: 0.004999808 validation, acc: 0.833, loss: 0.494 \n",
            "epoch: 78, acc: 0.854, loss: 0.479 (data_loss: 0.461, reg_loss: 0.018), lr: 0.004999805 validation, acc: 0.828, loss: 0.503 \n",
            "epoch: 79, acc: 0.844, loss: 0.496 (data_loss: 0.478, reg_loss: 0.018), lr: 0.004999803 validation, acc: 0.842, loss: 0.477 \n",
            "epoch: 80, acc: 0.854, loss: 0.464 (data_loss: 0.446, reg_loss: 0.018), lr: 0.004999800 validation, acc: 0.828, loss: 0.511 \n",
            "epoch: 81, acc: 0.849, loss: 0.486 (data_loss: 0.468, reg_loss: 0.018), lr: 0.004999798 validation, acc: 0.841, loss: 0.489 \n",
            "epoch: 82, acc: 0.858, loss: 0.469 (data_loss: 0.451, reg_loss: 0.018), lr: 0.004999795 validation, acc: 0.842, loss: 0.478 \n",
            "epoch: 83, acc: 0.864, loss: 0.458 (data_loss: 0.440, reg_loss: 0.018), lr: 0.004999793 validation, acc: 0.825, loss: 0.513 \n",
            "epoch: 84, acc: 0.851, loss: 0.476 (data_loss: 0.458, reg_loss: 0.018), lr: 0.004999790 validation, acc: 0.839, loss: 0.464 \n",
            "epoch: 85, acc: 0.862, loss: 0.448 (data_loss: 0.431, reg_loss: 0.018), lr: 0.004999788 validation, acc: 0.839, loss: 0.490 \n",
            "epoch: 86, acc: 0.857, loss: 0.470 (data_loss: 0.452, reg_loss: 0.018), lr: 0.004999785 validation, acc: 0.842, loss: 0.490 \n",
            "epoch: 87, acc: 0.858, loss: 0.468 (data_loss: 0.450, reg_loss: 0.017), lr: 0.004999783 validation, acc: 0.782, loss: 0.645 \n",
            "epoch: 88, acc: 0.799, loss: 0.616 (data_loss: 0.598, reg_loss: 0.017), lr: 0.004999780 validation, acc: 0.728, loss: 0.834 \n",
            "epoch: 89, acc: 0.739, loss: 0.848 (data_loss: 0.830, reg_loss: 0.018), lr: 0.004999778 validation, acc: 0.806, loss: 0.566 \n",
            "epoch: 90, acc: 0.818, loss: 0.563 (data_loss: 0.545, reg_loss: 0.018), lr: 0.004999775 validation, acc: 0.768, loss: 0.692 \n",
            "epoch: 91, acc: 0.780, loss: 0.671 (data_loss: 0.653, reg_loss: 0.018), lr: 0.004999773 validation, acc: 0.804, loss: 0.594 \n",
            "epoch: 92, acc: 0.825, loss: 0.567 (data_loss: 0.548, reg_loss: 0.019), lr: 0.004999770 validation, acc: 0.814, loss: 0.579 \n",
            "epoch: 93, acc: 0.822, loss: 0.572 (data_loss: 0.553, reg_loss: 0.020), lr: 0.004999768 validation, acc: 0.803, loss: 0.593 \n",
            "epoch: 94, acc: 0.812, loss: 0.595 (data_loss: 0.575, reg_loss: 0.020), lr: 0.004999765 validation, acc: 0.819, loss: 0.535 \n",
            "epoch: 95, acc: 0.839, loss: 0.521 (data_loss: 0.500, reg_loss: 0.021), lr: 0.004999763 validation, acc: 0.799, loss: 0.564 \n",
            "epoch: 96, acc: 0.831, loss: 0.543 (data_loss: 0.522, reg_loss: 0.021), lr: 0.004999760 validation, acc: 0.818, loss: 0.559 \n",
            "epoch: 97, acc: 0.837, loss: 0.527 (data_loss: 0.505, reg_loss: 0.022), lr: 0.004999758 validation, acc: 0.820, loss: 0.542 \n",
            "epoch: 98, acc: 0.841, loss: 0.506 (data_loss: 0.484, reg_loss: 0.022), lr: 0.004999755 validation, acc: 0.829, loss: 0.525 \n",
            "epoch: 99, acc: 0.841, loss: 0.507 (data_loss: 0.484, reg_loss: 0.023), lr: 0.004999753 validation, acc: 0.833, loss: 0.500 \n",
            "epoch: 100, acc: 0.852, loss: 0.487 (data_loss: 0.464, reg_loss: 0.023), lr: 0.004999750 validation, acc: 0.823, loss: 0.515 \n",
            "epoch: 101, acc: 0.847, loss: 0.504 (data_loss: 0.481, reg_loss: 0.023), lr: 0.004999748 validation, acc: 0.831, loss: 0.495 \n",
            "epoch: 102, acc: 0.857, loss: 0.472 (data_loss: 0.449, reg_loss: 0.024), lr: 0.004999745 validation, acc: 0.841, loss: 0.498 \n",
            "epoch: 103, acc: 0.855, loss: 0.475 (data_loss: 0.451, reg_loss: 0.024), lr: 0.004999743 validation, acc: 0.830, loss: 0.505 \n",
            "epoch: 104, acc: 0.858, loss: 0.474 (data_loss: 0.450, reg_loss: 0.024), lr: 0.004999740 validation, acc: 0.831, loss: 0.509 \n",
            "epoch: 105, acc: 0.854, loss: 0.470 (data_loss: 0.446, reg_loss: 0.024), lr: 0.004999738 validation, acc: 0.842, loss: 0.486 \n",
            "epoch: 106, acc: 0.862, loss: 0.457 (data_loss: 0.433, reg_loss: 0.024), lr: 0.004999735 validation, acc: 0.837, loss: 0.480 \n",
            "epoch: 107, acc: 0.863, loss: 0.463 (data_loss: 0.439, reg_loss: 0.024), lr: 0.004999733 validation, acc: 0.836, loss: 0.478 \n",
            "epoch: 108, acc: 0.869, loss: 0.443 (data_loss: 0.419, reg_loss: 0.024), lr: 0.004999730 validation, acc: 0.838, loss: 0.482 \n",
            "epoch: 109, acc: 0.869, loss: 0.438 (data_loss: 0.414, reg_loss: 0.024), lr: 0.004999728 validation, acc: 0.846, loss: 0.461 \n",
            "epoch: 110, acc: 0.874, loss: 0.435 (data_loss: 0.411, reg_loss: 0.024), lr: 0.004999725 validation, acc: 0.846, loss: 0.459 \n",
            "epoch: 111, acc: 0.874, loss: 0.430 (data_loss: 0.406, reg_loss: 0.024), lr: 0.004999723 validation, acc: 0.841, loss: 0.468 \n",
            "epoch: 112, acc: 0.873, loss: 0.423 (data_loss: 0.399, reg_loss: 0.024), lr: 0.004999720 validation, acc: 0.846, loss: 0.455 \n",
            "epoch: 113, acc: 0.880, loss: 0.416 (data_loss: 0.393, reg_loss: 0.024), lr: 0.004999718 validation, acc: 0.840, loss: 0.454 \n",
            "epoch: 114, acc: 0.876, loss: 0.427 (data_loss: 0.404, reg_loss: 0.024), lr: 0.004999715 validation, acc: 0.849, loss: 0.448 \n",
            "epoch: 115, acc: 0.882, loss: 0.406 (data_loss: 0.383, reg_loss: 0.023), lr: 0.004999713 validation, acc: 0.849, loss: 0.448 \n",
            "epoch: 116, acc: 0.883, loss: 0.406 (data_loss: 0.382, reg_loss: 0.023), lr: 0.004999710 validation, acc: 0.851, loss: 0.447 \n",
            "epoch: 117, acc: 0.881, loss: 0.414 (data_loss: 0.391, reg_loss: 0.023), lr: 0.004999708 validation, acc: 0.854, loss: 0.441 \n",
            "epoch: 118, acc: 0.883, loss: 0.401 (data_loss: 0.378, reg_loss: 0.023), lr: 0.004999705 validation, acc: 0.846, loss: 0.442 \n",
            "epoch: 119, acc: 0.885, loss: 0.392 (data_loss: 0.370, reg_loss: 0.023), lr: 0.004999703 validation, acc: 0.849, loss: 0.439 \n",
            "epoch: 120, acc: 0.891, loss: 0.388 (data_loss: 0.365, reg_loss: 0.022), lr: 0.004999700 validation, acc: 0.851, loss: 0.443 \n",
            "epoch: 121, acc: 0.889, loss: 0.388 (data_loss: 0.365, reg_loss: 0.022), lr: 0.004999698 validation, acc: 0.855, loss: 0.445 \n",
            "epoch: 122, acc: 0.893, loss: 0.382 (data_loss: 0.360, reg_loss: 0.022), lr: 0.004999695 validation, acc: 0.854, loss: 0.434 \n",
            "epoch: 123, acc: 0.898, loss: 0.375 (data_loss: 0.353, reg_loss: 0.022), lr: 0.004999693 validation, acc: 0.854, loss: 0.431 \n",
            "epoch: 124, acc: 0.899, loss: 0.371 (data_loss: 0.350, reg_loss: 0.022), lr: 0.004999690 validation, acc: 0.856, loss: 0.437 \n",
            "epoch: 125, acc: 0.898, loss: 0.369 (data_loss: 0.348, reg_loss: 0.021), lr: 0.004999688 validation, acc: 0.852, loss: 0.433 \n",
            "epoch: 126, acc: 0.896, loss: 0.365 (data_loss: 0.344, reg_loss: 0.021), lr: 0.004999685 validation, acc: 0.854, loss: 0.434 \n",
            "epoch: 127, acc: 0.899, loss: 0.362 (data_loss: 0.341, reg_loss: 0.021), lr: 0.004999683 validation, acc: 0.857, loss: 0.432 \n",
            "epoch: 128, acc: 0.900, loss: 0.360 (data_loss: 0.339, reg_loss: 0.021), lr: 0.004999680 validation, acc: 0.857, loss: 0.437 \n",
            "epoch: 129, acc: 0.901, loss: 0.369 (data_loss: 0.349, reg_loss: 0.020), lr: 0.004999678 validation, acc: 0.843, loss: 0.475 \n",
            "epoch: 130, acc: 0.886, loss: 0.390 (data_loss: 0.370, reg_loss: 0.020), lr: 0.004999675 validation, acc: 0.843, loss: 0.480 \n",
            "epoch: 131, acc: 0.878, loss: 0.414 (data_loss: 0.394, reg_loss: 0.020), lr: 0.004999673 validation, acc: 0.834, loss: 0.498 \n",
            "epoch: 132, acc: 0.879, loss: 0.405 (data_loss: 0.385, reg_loss: 0.020), lr: 0.004999670 validation, acc: 0.836, loss: 0.492 \n",
            "epoch: 133, acc: 0.875, loss: 0.413 (data_loss: 0.393, reg_loss: 0.020), lr: 0.004999668 validation, acc: 0.830, loss: 0.524 \n",
            "epoch: 134, acc: 0.867, loss: 0.453 (data_loss: 0.434, reg_loss: 0.020), lr: 0.004999665 validation, acc: 0.830, loss: 0.513 \n",
            "epoch: 135, acc: 0.874, loss: 0.429 (data_loss: 0.410, reg_loss: 0.020), lr: 0.004999663 validation, acc: 0.789, loss: 0.615 \n",
            "epoch: 136, acc: 0.823, loss: 0.556 (data_loss: 0.536, reg_loss: 0.020), lr: 0.004999660 validation, acc: 0.798, loss: 0.638 \n",
            "epoch: 137, acc: 0.824, loss: 0.566 (data_loss: 0.546, reg_loss: 0.020), lr: 0.004999658 validation, acc: 0.792, loss: 0.666 \n",
            "epoch: 138, acc: 0.825, loss: 0.583 (data_loss: 0.563, reg_loss: 0.020), lr: 0.004999655 validation, acc: 0.830, loss: 0.557 \n",
            "epoch: 139, acc: 0.847, loss: 0.488 (data_loss: 0.468, reg_loss: 0.020), lr: 0.004999653 validation, acc: 0.825, loss: 0.544 \n",
            "epoch: 140, acc: 0.858, loss: 0.485 (data_loss: 0.465, reg_loss: 0.020), lr: 0.004999650 validation, acc: 0.814, loss: 0.572 \n",
            "epoch: 141, acc: 0.851, loss: 0.487 (data_loss: 0.466, reg_loss: 0.021), lr: 0.004999648 validation, acc: 0.826, loss: 0.525 \n",
            "epoch: 142, acc: 0.856, loss: 0.467 (data_loss: 0.446, reg_loss: 0.021), lr: 0.004999645 validation, acc: 0.814, loss: 0.572 \n",
            "epoch: 143, acc: 0.833, loss: 0.532 (data_loss: 0.511, reg_loss: 0.021), lr: 0.004999643 validation, acc: 0.819, loss: 0.602 \n",
            "epoch: 144, acc: 0.837, loss: 0.546 (data_loss: 0.524, reg_loss: 0.022), lr: 0.004999640 validation, acc: 0.831, loss: 0.514 \n",
            "epoch: 145, acc: 0.860, loss: 0.454 (data_loss: 0.432, reg_loss: 0.022), lr: 0.004999638 validation, acc: 0.825, loss: 0.528 \n",
            "epoch: 146, acc: 0.858, loss: 0.467 (data_loss: 0.445, reg_loss: 0.022), lr: 0.004999635 validation, acc: 0.840, loss: 0.473 \n",
            "epoch: 147, acc: 0.876, loss: 0.415 (data_loss: 0.392, reg_loss: 0.023), lr: 0.004999633 validation, acc: 0.831, loss: 0.483 \n",
            "epoch: 148, acc: 0.869, loss: 0.433 (data_loss: 0.410, reg_loss: 0.023), lr: 0.004999630 validation, acc: 0.847, loss: 0.473 \n",
            "epoch: 149, acc: 0.875, loss: 0.419 (data_loss: 0.396, reg_loss: 0.023), lr: 0.004999628 validation, acc: 0.834, loss: 0.499 \n",
            "epoch: 150, acc: 0.875, loss: 0.433 (data_loss: 0.409, reg_loss: 0.024), lr: 0.004999625 validation, acc: 0.847, loss: 0.491 \n",
            "epoch: 151, acc: 0.878, loss: 0.418 (data_loss: 0.394, reg_loss: 0.024), lr: 0.004999623 validation, acc: 0.844, loss: 0.493 \n",
            "epoch: 152, acc: 0.879, loss: 0.420 (data_loss: 0.396, reg_loss: 0.024), lr: 0.004999620 validation, acc: 0.852, loss: 0.468 \n",
            "epoch: 153, acc: 0.891, loss: 0.388 (data_loss: 0.364, reg_loss: 0.024), lr: 0.004999618 validation, acc: 0.848, loss: 0.471 \n",
            "epoch: 154, acc: 0.884, loss: 0.402 (data_loss: 0.378, reg_loss: 0.024), lr: 0.004999615 validation, acc: 0.848, loss: 0.456 \n",
            "epoch: 155, acc: 0.892, loss: 0.381 (data_loss: 0.356, reg_loss: 0.024), lr: 0.004999613 validation, acc: 0.849, loss: 0.454 \n",
            "epoch: 156, acc: 0.893, loss: 0.379 (data_loss: 0.355, reg_loss: 0.024), lr: 0.004999610 validation, acc: 0.851, loss: 0.449 \n",
            "epoch: 157, acc: 0.897, loss: 0.372 (data_loss: 0.348, reg_loss: 0.024), lr: 0.004999608 validation, acc: 0.854, loss: 0.446 \n",
            "epoch: 158, acc: 0.898, loss: 0.366 (data_loss: 0.341, reg_loss: 0.024), lr: 0.004999605 validation, acc: 0.850, loss: 0.450 \n",
            "epoch: 159, acc: 0.900, loss: 0.366 (data_loss: 0.341, reg_loss: 0.024), lr: 0.004999603 validation, acc: 0.851, loss: 0.441 \n",
            "epoch: 160, acc: 0.908, loss: 0.353 (data_loss: 0.328, reg_loss: 0.024), lr: 0.004999600 validation, acc: 0.851, loss: 0.442 \n",
            "epoch: 161, acc: 0.902, loss: 0.359 (data_loss: 0.335, reg_loss: 0.024), lr: 0.004999598 validation, acc: 0.854, loss: 0.446 \n",
            "epoch: 162, acc: 0.904, loss: 0.351 (data_loss: 0.327, reg_loss: 0.024), lr: 0.004999595 validation, acc: 0.858, loss: 0.452 \n",
            "epoch: 163, acc: 0.905, loss: 0.351 (data_loss: 0.327, reg_loss: 0.024), lr: 0.004999593 validation, acc: 0.851, loss: 0.454 \n",
            "epoch: 164, acc: 0.904, loss: 0.348 (data_loss: 0.324, reg_loss: 0.024), lr: 0.004999590 validation, acc: 0.854, loss: 0.448 \n",
            "epoch: 165, acc: 0.903, loss: 0.351 (data_loss: 0.327, reg_loss: 0.024), lr: 0.004999588 validation, acc: 0.849, loss: 0.460 \n",
            "epoch: 166, acc: 0.904, loss: 0.351 (data_loss: 0.327, reg_loss: 0.024), lr: 0.004999585 validation, acc: 0.859, loss: 0.442 \n",
            "epoch: 167, acc: 0.909, loss: 0.340 (data_loss: 0.316, reg_loss: 0.024), lr: 0.004999583 validation, acc: 0.859, loss: 0.441 \n",
            "epoch: 168, acc: 0.913, loss: 0.333 (data_loss: 0.309, reg_loss: 0.023), lr: 0.004999580 validation, acc: 0.851, loss: 0.450 \n",
            "epoch: 169, acc: 0.912, loss: 0.335 (data_loss: 0.312, reg_loss: 0.023), lr: 0.004999578 validation, acc: 0.862, loss: 0.435 \n",
            "epoch: 170, acc: 0.911, loss: 0.330 (data_loss: 0.306, reg_loss: 0.023), lr: 0.004999575 validation, acc: 0.861, loss: 0.429 \n",
            "epoch: 171, acc: 0.916, loss: 0.318 (data_loss: 0.295, reg_loss: 0.023), lr: 0.004999573 validation, acc: 0.858, loss: 0.446 \n",
            "epoch: 172, acc: 0.914, loss: 0.327 (data_loss: 0.304, reg_loss: 0.023), lr: 0.004999570 validation, acc: 0.858, loss: 0.445 \n",
            "epoch: 173, acc: 0.909, loss: 0.327 (data_loss: 0.304, reg_loss: 0.023), lr: 0.004999568 validation, acc: 0.851, loss: 0.443 \n",
            "epoch: 174, acc: 0.915, loss: 0.319 (data_loss: 0.297, reg_loss: 0.022), lr: 0.004999565 validation, acc: 0.863, loss: 0.430 \n",
            "epoch: 175, acc: 0.918, loss: 0.309 (data_loss: 0.287, reg_loss: 0.022), lr: 0.004999563 validation, acc: 0.857, loss: 0.438 \n",
            "epoch: 176, acc: 0.920, loss: 0.316 (data_loss: 0.294, reg_loss: 0.022), lr: 0.004999560 validation, acc: 0.856, loss: 0.449 \n",
            "epoch: 177, acc: 0.917, loss: 0.315 (data_loss: 0.293, reg_loss: 0.022), lr: 0.004999558 validation, acc: 0.858, loss: 0.458 \n",
            "epoch: 178, acc: 0.910, loss: 0.324 (data_loss: 0.302, reg_loss: 0.022), lr: 0.004999555 validation, acc: 0.849, loss: 0.470 \n",
            "epoch: 179, acc: 0.908, loss: 0.328 (data_loss: 0.306, reg_loss: 0.022), lr: 0.004999553 validation, acc: 0.861, loss: 0.445 \n",
            "epoch: 180, acc: 0.917, loss: 0.309 (data_loss: 0.288, reg_loss: 0.021), lr: 0.004999550 validation, acc: 0.855, loss: 0.455 \n",
            "epoch: 181, acc: 0.923, loss: 0.305 (data_loss: 0.283, reg_loss: 0.021), lr: 0.004999548 validation, acc: 0.856, loss: 0.446 \n",
            "epoch: 182, acc: 0.922, loss: 0.306 (data_loss: 0.285, reg_loss: 0.021), lr: 0.004999545 validation, acc: 0.861, loss: 0.443 \n",
            "epoch: 183, acc: 0.923, loss: 0.305 (data_loss: 0.283, reg_loss: 0.021), lr: 0.004999543 validation, acc: 0.863, loss: 0.451 \n",
            "epoch: 184, acc: 0.922, loss: 0.304 (data_loss: 0.283, reg_loss: 0.021), lr: 0.004999540 validation, acc: 0.857, loss: 0.441 \n",
            "epoch: 185, acc: 0.921, loss: 0.304 (data_loss: 0.284, reg_loss: 0.021), lr: 0.004999538 validation, acc: 0.857, loss: 0.449 \n",
            "epoch: 186, acc: 0.924, loss: 0.295 (data_loss: 0.274, reg_loss: 0.021), lr: 0.004999535 validation, acc: 0.867, loss: 0.444 \n",
            "epoch: 187, acc: 0.929, loss: 0.286 (data_loss: 0.265, reg_loss: 0.021), lr: 0.004999533 validation, acc: 0.860, loss: 0.442 \n",
            "epoch: 188, acc: 0.923, loss: 0.299 (data_loss: 0.278, reg_loss: 0.020), lr: 0.004999530 validation, acc: 0.854, loss: 0.460 \n",
            "epoch: 189, acc: 0.920, loss: 0.304 (data_loss: 0.284, reg_loss: 0.020), lr: 0.004999528 validation, acc: 0.859, loss: 0.448 \n",
            "epoch: 190, acc: 0.927, loss: 0.294 (data_loss: 0.274, reg_loss: 0.020), lr: 0.004999525 validation, acc: 0.849, loss: 0.460 \n",
            "epoch: 191, acc: 0.921, loss: 0.298 (data_loss: 0.278, reg_loss: 0.020), lr: 0.004999523 validation, acc: 0.858, loss: 0.447 \n",
            "epoch: 192, acc: 0.929, loss: 0.292 (data_loss: 0.272, reg_loss: 0.020), lr: 0.004999520 validation, acc: 0.859, loss: 0.455 \n",
            "epoch: 193, acc: 0.926, loss: 0.296 (data_loss: 0.276, reg_loss: 0.020), lr: 0.004999518 validation, acc: 0.849, loss: 0.488 \n",
            "epoch: 194, acc: 0.906, loss: 0.332 (data_loss: 0.312, reg_loss: 0.020), lr: 0.004999515 validation, acc: 0.789, loss: 0.679 \n",
            "epoch: 195, acc: 0.833, loss: 0.546 (data_loss: 0.526, reg_loss: 0.020), lr: 0.004999513 validation, acc: 0.772, loss: 0.923 \n",
            "epoch: 196, acc: 0.803, loss: 0.819 (data_loss: 0.799, reg_loss: 0.020), lr: 0.004999510 validation, acc: 0.821, loss: 0.585 \n",
            "epoch: 197, acc: 0.868, loss: 0.472 (data_loss: 0.452, reg_loss: 0.020), lr: 0.004999508 validation, acc: 0.805, loss: 0.604 \n",
            "epoch: 198, acc: 0.834, loss: 0.520 (data_loss: 0.500, reg_loss: 0.020), lr: 0.004999505 validation, acc: 0.748, loss: 0.832 \n",
            "epoch: 199, acc: 0.783, loss: 0.719 (data_loss: 0.698, reg_loss: 0.021), lr: 0.004999503 validation, acc: 0.776, loss: 0.775 \n",
            "epoch: 200, acc: 0.812, loss: 0.673 (data_loss: 0.651, reg_loss: 0.021), lr: 0.004999500 validation, acc: 0.799, loss: 0.637 \n",
            "epoch: 201, acc: 0.838, loss: 0.535 (data_loss: 0.513, reg_loss: 0.022), lr: 0.004999498 validation, acc: 0.818, loss: 0.578 \n",
            "epoch: 202, acc: 0.851, loss: 0.505 (data_loss: 0.482, reg_loss: 0.023), lr: 0.004999495 validation, acc: 0.797, loss: 0.641 \n",
            "epoch: 203, acc: 0.831, loss: 0.549 (data_loss: 0.525, reg_loss: 0.023), lr: 0.004999493 validation, acc: 0.811, loss: 0.586 \n",
            "epoch: 204, acc: 0.846, loss: 0.524 (data_loss: 0.500, reg_loss: 0.024), lr: 0.004999490 validation, acc: 0.821, loss: 0.567 \n",
            "epoch: 205, acc: 0.862, loss: 0.474 (data_loss: 0.449, reg_loss: 0.025), lr: 0.004999488 validation, acc: 0.827, loss: 0.547 \n",
            "epoch: 206, acc: 0.866, loss: 0.465 (data_loss: 0.439, reg_loss: 0.026), lr: 0.004999485 validation, acc: 0.840, loss: 0.535 \n",
            "epoch: 207, acc: 0.867, loss: 0.471 (data_loss: 0.445, reg_loss: 0.026), lr: 0.004999483 validation, acc: 0.842, loss: 0.532 \n",
            "epoch: 208, acc: 0.867, loss: 0.457 (data_loss: 0.430, reg_loss: 0.027), lr: 0.004999480 validation, acc: 0.848, loss: 0.491 \n",
            "epoch: 209, acc: 0.880, loss: 0.424 (data_loss: 0.397, reg_loss: 0.027), lr: 0.004999478 validation, acc: 0.844, loss: 0.492 \n",
            "epoch: 210, acc: 0.880, loss: 0.427 (data_loss: 0.399, reg_loss: 0.028), lr: 0.004999475 validation, acc: 0.844, loss: 0.491 \n",
            "epoch: 211, acc: 0.883, loss: 0.417 (data_loss: 0.389, reg_loss: 0.028), lr: 0.004999473 validation, acc: 0.850, loss: 0.475 \n",
            "epoch: 212, acc: 0.892, loss: 0.395 (data_loss: 0.366, reg_loss: 0.029), lr: 0.004999470 validation, acc: 0.844, loss: 0.467 \n",
            "epoch: 213, acc: 0.893, loss: 0.387 (data_loss: 0.358, reg_loss: 0.029), lr: 0.004999468 validation, acc: 0.849, loss: 0.461 \n",
            "epoch: 214, acc: 0.899, loss: 0.375 (data_loss: 0.346, reg_loss: 0.029), lr: 0.004999465 validation, acc: 0.849, loss: 0.463 \n",
            "epoch: 215, acc: 0.900, loss: 0.376 (data_loss: 0.346, reg_loss: 0.030), lr: 0.004999463 validation, acc: 0.844, loss: 0.456 \n",
            "epoch: 216, acc: 0.901, loss: 0.372 (data_loss: 0.342, reg_loss: 0.030), lr: 0.004999460 validation, acc: 0.853, loss: 0.459 \n",
            "epoch: 217, acc: 0.903, loss: 0.362 (data_loss: 0.332, reg_loss: 0.030), lr: 0.004999458 validation, acc: 0.860, loss: 0.457 \n",
            "epoch: 218, acc: 0.904, loss: 0.358 (data_loss: 0.328, reg_loss: 0.030), lr: 0.004999455 validation, acc: 0.859, loss: 0.446 \n",
            "epoch: 219, acc: 0.905, loss: 0.354 (data_loss: 0.324, reg_loss: 0.030), lr: 0.004999453 validation, acc: 0.859, loss: 0.447 \n",
            "epoch: 220, acc: 0.907, loss: 0.355 (data_loss: 0.325, reg_loss: 0.030), lr: 0.004999450 validation, acc: 0.858, loss: 0.463 \n",
            "epoch: 221, acc: 0.907, loss: 0.355 (data_loss: 0.325, reg_loss: 0.030), lr: 0.004999448 validation, acc: 0.856, loss: 0.461 \n",
            "epoch: 222, acc: 0.914, loss: 0.348 (data_loss: 0.317, reg_loss: 0.030), lr: 0.004999445 validation, acc: 0.861, loss: 0.457 \n",
            "epoch: 223, acc: 0.913, loss: 0.338 (data_loss: 0.308, reg_loss: 0.030), lr: 0.004999443 validation, acc: 0.866, loss: 0.448 \n",
            "epoch: 224, acc: 0.916, loss: 0.334 (data_loss: 0.304, reg_loss: 0.030), lr: 0.004999440 validation, acc: 0.866, loss: 0.433 \n",
            "epoch: 225, acc: 0.918, loss: 0.326 (data_loss: 0.296, reg_loss: 0.030), lr: 0.004999438 validation, acc: 0.863, loss: 0.440 \n",
            "epoch: 226, acc: 0.919, loss: 0.330 (data_loss: 0.300, reg_loss: 0.030), lr: 0.004999435 validation, acc: 0.863, loss: 0.447 \n",
            "epoch: 227, acc: 0.918, loss: 0.333 (data_loss: 0.304, reg_loss: 0.030), lr: 0.004999433 validation, acc: 0.861, loss: 0.436 \n",
            "epoch: 228, acc: 0.924, loss: 0.316 (data_loss: 0.287, reg_loss: 0.029), lr: 0.004999430 validation, acc: 0.854, loss: 0.440 \n",
            "epoch: 229, acc: 0.920, loss: 0.319 (data_loss: 0.290, reg_loss: 0.029), lr: 0.004999428 validation, acc: 0.860, loss: 0.464 \n",
            "epoch: 230, acc: 0.911, loss: 0.332 (data_loss: 0.303, reg_loss: 0.029), lr: 0.004999425 validation, acc: 0.858, loss: 0.452 \n",
            "epoch: 231, acc: 0.921, loss: 0.315 (data_loss: 0.286, reg_loss: 0.029), lr: 0.004999423 validation, acc: 0.863, loss: 0.455 \n",
            "epoch: 232, acc: 0.923, loss: 0.308 (data_loss: 0.279, reg_loss: 0.029), lr: 0.004999420 validation, acc: 0.864, loss: 0.455 \n",
            "epoch: 233, acc: 0.925, loss: 0.309 (data_loss: 0.281, reg_loss: 0.028), lr: 0.004999418 validation, acc: 0.860, loss: 0.451 \n",
            "epoch: 234, acc: 0.925, loss: 0.304 (data_loss: 0.275, reg_loss: 0.028), lr: 0.004999415 validation, acc: 0.863, loss: 0.451 \n",
            "epoch: 235, acc: 0.930, loss: 0.297 (data_loss: 0.269, reg_loss: 0.028), lr: 0.004999413 validation, acc: 0.868, loss: 0.450 \n",
            "epoch: 236, acc: 0.925, loss: 0.302 (data_loss: 0.274, reg_loss: 0.028), lr: 0.004999410 validation, acc: 0.851, loss: 0.459 \n",
            "epoch: 237, acc: 0.927, loss: 0.299 (data_loss: 0.272, reg_loss: 0.027), lr: 0.004999408 validation, acc: 0.863, loss: 0.451 \n",
            "epoch: 238, acc: 0.929, loss: 0.297 (data_loss: 0.270, reg_loss: 0.027), lr: 0.004999405 validation, acc: 0.864, loss: 0.443 \n",
            "epoch: 239, acc: 0.933, loss: 0.287 (data_loss: 0.260, reg_loss: 0.027), lr: 0.004999403 validation, acc: 0.862, loss: 0.452 \n",
            "epoch: 240, acc: 0.927, loss: 0.295 (data_loss: 0.268, reg_loss: 0.027), lr: 0.004999400 validation, acc: 0.861, loss: 0.454 \n",
            "epoch: 241, acc: 0.924, loss: 0.303 (data_loss: 0.277, reg_loss: 0.026), lr: 0.004999398 validation, acc: 0.858, loss: 0.486 \n",
            "epoch: 242, acc: 0.917, loss: 0.314 (data_loss: 0.288, reg_loss: 0.026), lr: 0.004999395 validation, acc: 0.860, loss: 0.458 \n",
            "epoch: 243, acc: 0.923, loss: 0.301 (data_loss: 0.275, reg_loss: 0.026), lr: 0.004999393 validation, acc: 0.858, loss: 0.460 \n",
            "epoch: 244, acc: 0.925, loss: 0.296 (data_loss: 0.271, reg_loss: 0.026), lr: 0.004999390 validation, acc: 0.869, loss: 0.439 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNnDUP_U8Xn",
        "outputId": "e5f5e6bf-a378-4721-f9c6-4434a214eb9a"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8122459183673469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbXMisqQKqF",
        "outputId": "ff3b2b02-bece-4dae-fc72-dda2d47954c1"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 58 epochs\n",
            "Model hit 85% validation accuracy in 116 epochs\n",
            "Max accuracy was 86.875% at epoch 244.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwSXsZVU8Xo",
        "outputId": "df4cc11c-19aa-4ecb-e763-7a447ccf2349"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "index = 27\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {fashion_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {fashion_mnist_labels[y_test[index]]}')\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50.606% Confident True is Dress. True is actually T-shirt/top\n",
            "Accuracy: 0.869, loss: 0.439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k0Ve2M0bPG3"
      },
      "source": [
        "training_diff = []\n",
        "testing_diff = []\n",
        "combined_diff = []"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByL_RwvlIx3"
      },
      "source": [
        "Individual Training Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTOnqnDXa0ME",
        "outputId": "ec61806b-0fdc-4496-ac10-9d07dfffad0e"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x, sorted_y)):\n",
        "  accuracy = Accuracy_Categorical()\n",
        "\n",
        "  y = sorted_y[y_sorted_lists]\n",
        "  X = sorted_x[X_sorted_lists]\n",
        "  accuracy.init(y)\n",
        "\n",
        "  dense1.forward(X)\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "  train_train_mean = activation1.output\n",
        "\n",
        "  dense2.forward(activation1.output)\n",
        "\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, y)\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, y)\n",
        "  print(f'{fashion_mnist_labels[classes]} Train Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T-shirt/top Train Accuracy: 0.890, loss: 0.460\n",
            "Trouser Train Accuracy: 0.963, loss: 0.181\n",
            "Pullover Train Accuracy: 0.918, loss: 0.368\n",
            "Dress Train Accuracy: 0.960, loss: 0.152\n",
            "Coat Train Accuracy: 0.964, loss: 0.192\n",
            "Sandal Train Accuracy: 0.978, loss: 0.097\n",
            "Shirt Train Accuracy: 0.838, loss: 0.552\n",
            "Sneaker Train Accuracy: 0.975, loss: 0.134\n",
            "Bag Train Accuracy: 0.977, loss: 0.104\n",
            "Ankle boot Train Accuracy: 0.986, loss: 0.059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scjb7Wh_sn6b",
        "outputId": "0557a2e3-3cab-4bfa-9c6a-1fe8dcccc1c7"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x_test, sorted_y_test)):\n",
        "  accuracy.init(y_sorted_lists)\n",
        "  #print(sorted_y[y_sorted_lists].shape)\n",
        "  #print(sorted_x[X_sorted_lists].shape)\n",
        "  dense1.forward(sorted_x_test[X_sorted_lists])\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  testmean = np.mean(activation1.output, axis=0)\n",
        "  testing_diff.append(testmean)\n",
        "  dense2.forward(activation1.output)\n",
        "\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  print(f'{fashion_mnist_labels[classes]} Test Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T-shirt/top Test Accuracy: 0.758, loss: 0.659\n",
            "Trouser Test Accuracy: 0.957, loss: 0.173\n",
            "Pullover Test Accuracy: 0.802, loss: 0.734\n",
            "Dress Test Accuracy: 0.885, loss: 0.420\n",
            "Coat Test Accuracy: 0.826, loss: 0.570\n",
            "Sandal Test Accuracy: 0.924, loss: 0.250\n",
            "Shirt Test Accuracy: 0.677, loss: 0.954\n",
            "Sneaker Test Accuracy: 0.981, loss: 0.132\n",
            "Bag Test Accuracy: 0.947, loss: 0.288\n",
            "Ankle boot Test Accuracy: 0.959, loss: 0.114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u-O8oNZ0qA"
      },
      "source": [
        "# Full mnist test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbD4KrLMnTcR"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMfBGUHeZ4L5",
        "outputId": "68bd9490-6005-4c38-bd52-fa4a1fd922af"
      },
      "source": [
        "(input, label), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(input.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "input = input[keys]\n",
        "label = label[keys]\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "input = (input.reshape(input.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(label)\n",
        "\n",
        "dense1.forward(input)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, label)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, label)\n",
        "\n",
        "print(f'Full Training Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full Training Accuracy: 0.867, loss: 0.429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aat-uVF6nYu7"
      },
      "source": [
        "Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hyU1tBDna8x",
        "outputId": "cec8ac94-911d-4de2-bb17-d70c6717d8ba"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Full Testing Accuracy: {testaccuracy:.5f}, loss: {loss:.3f}')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full Testing Accuracy: 0.84770, loss: 0.496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbIMZ7Pk_Tnp"
      },
      "source": [
        "Change idex to get confidence of different samples of testing data. Index values 0-1600 were refrenced in training. Anything past was never seen during training. Lowest confidence is at index 1645 when trained with 244 epochs and numpy seed set to 22."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "JaxWcRIr_BCV",
        "outputId": "62f2308b-8094-4cac-ed75-a4bd9277da35"
      },
      "source": [
        "index = 5000\n",
        "\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {fashion_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {fashion_mnist_labels[y_test[index]]}')\n",
        "\n",
        "X_test.resize(X_test.shape[0],28,28)\n",
        "image = X_test[index]\n",
        "fig = plt.figure\n",
        "plt.title(f'{fashion_mnist_labels[y_test[index]]}')\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.934% Confident True is Pullover. True is actually Pullover\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUW0lEQVR4nO3dfZBUVXoG8OdhYEA+ZYTFAQYRpCgNRrQQV9ciuKsrUeMHf5AliWJiHFO1araylVpirFr+IWVF3XXjVq2LpTLo6kZrFSk1cVk0fhbCQI2I6AIiyOeAysfwzQxv/uiLO+rc94x9u+c2c55f1dT03LdPz+keHu7tPvfcQzODiHR/PfLugIh0DYVdJBIKu0gkFHaRSCjsIpFQ2EUiobBHiuTNJN9s97ORPCvPPkl5KezdAMmNJA+R3E+ymeR8kv3z7pdUFoW9+/grM+sP4AIAkwDcnXN/XCR75t2H2Cjs3YyZbQXwPwAmJIfmX4SK5P+R/MfQY5AcRHIByV0kN5G8m2QPkr1J7iE5od19hyZHFd9Kfr6GZFNyv7dJ/nm7+24k+ROSqwAcUOC7lsLezZCsA3AVgN0ZHuZBAIMAjAHwFwBuAvD3ZnYEwLMAZra77wwAr5nZTpLnA3gUwG0ATgPwawCLSPZud/+ZAK4GcKqZtWboo3xDCnv3sZDkHgBvAngNwH8U8yAkqwD8AMC/mVmLmW0EcD+AG5O7PJnUT/ibZBsA1AP4tZm9Y2ZtZtYA4AiAb7e7/3+Z2WYzO1RM/6R4OozqPq43sz+c+IHk6CIfZwiAXgA2tdu2CcCI5ParAPqSvAhAM4CJAJ5LamcAmEXyjnZtqwEMb/fz5iL7JRkp7N3XgeR7XwD7ktund6LdpwCOoRDcNcm2UQC2AoCZtZF8GoXD8WYAL5hZS3K/zQDmmtlc5/E1zTInOozvpsxsFwoB/TuSVST/AcDYTrRrA/A0gLkkB5A8A8C/AHii3d2eBPDXAP4WfzqEB4CHAfwTyYtY0I/k1SQHlOhpSQYKe/d2K4B/BfAZgD8D8HYn292BwpHBBhQ+A3gShQ/eAABm9k5SH47CJ/8ntjcmv/OXKHxAuB7AzRmfg5QIdfEKkThozy4SCYVdJBIKu0gkFHaRSHTpODtJfRooUmZmxo62Z9qzk5xG8o8k15OcneWxyo2k+yXdT+hvHtu/h6KH3pJzqNcCuALAFgDLAcw0szVOm9z27KE/oIYgu58soT2Z/z2UY88+GcB6M9tgZkcB/BbAdRkeT0TKKEvYR+DLkxq24E+TJb5Asp5kI8nGDL9LRDIq+wd0ZjYPwDxAH9CJ5CnLnn0rgLp2P49MtolIBcoS9uUAxpE8k2Q1Chc0WFSabolIqRV9GG9mrSRvB/AygCoAj5rZ+yXrWYnl+elqjx7+/6k33XSTW58zZ45bf+ihh1Jrb7zxhts2pK2tza337t3brU+dOjW1NnPmzNQaAMyYMcOtr1q1yq1n+Zt3x9GbTO/ZzewlAC+VqC8iUkY6XVYkEgq7SCQUdpFIKOwikVDYRSKhsItEoksvONldT5dtaGhw69OmTXPr+/fvd+tHjx5162PGjHHrnr1797r10L+PQYMGuXWv7+vWrXPbDh8+3K2vX7/erd99d/ralq+99prb9mRWlvnsInLyUNhFIqGwi0RCYReJhMIuEgmFXSQSGnrrpIULF6bWrr76arft5s3+kuRVVVVufdeuXW593759qbV+/fq5baurq916aKrnoUOH3PqxY8dSa6GhtdD02T59+rj1vn37ptamT5/utn355ZfdeiVPgdXQm0jkFHaRSCjsIpFQ2EUiobCLREJhF4mEwi4SCY2zJ0JTNdeuXZtaO3LkSKbfHfobDBw40K17Y74tLS1u29A4eajujWUDwJAhQ1Jroam9x48fz1SvqalJrb3yyitu2xtuuMGtVzKNs4tETmEXiYTCLhIJhV0kEgq7SCQUdpFIKOwikci0imt3MmXKFLd+6qmnptaam5vdtj17+i9zaJw9NB59+PDh1NrSpUvdtqE54aG5+qHnvnv37tRaaE54aD576HXzzn+YMGGC27Y7yhR2khsBtABoA9BqZpNK0SkRKb1S7NkvM7NPS/A4IlJGes8uEomsYTcAvye5gmR9R3cgWU+ykWRjxt8lIhlkPYy/1My2kvwWgMUkPzSz19vfwczmAZgHVPZEGJHuLtOe3cy2Jt93AngOwORSdEpESq/osJPsR3LAidsAvg9gdak6JiKlleUwfhiA55Kx0p4AnjSz/y1Jr3IQGmdva2tLrfXq1ctt613XHQDWrFnj1kNLG3tj2aG58KG+h84RCM3lP+WUU9y6x3vNgXDfvfYjR450206e7B+kLlu2zK1XoqLDbmYbAJxXwr6ISBlp6E0kEgq7SCQUdpFIKOwikVDYRSKhKa6JqVOnunVvGCc0FfOjjz7KVPemsAL+ktChoa8777zTrYeGDUNLQh89ejS1Fpri2qOHvy/KsmxyaKnq0NTek3HoTXt2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSWrI50dra6tb37t2bWgu9hqeddppbX7JkiVt/++233frw4cNTa+PHj3fbXnzxxW7dmz4LAJ9+6l9r1JtKGjp/ILQkc//+/d36wYMHU2sDBgxw265cudKtX3LJJW49T1qyWSRyCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMbZEx9++KFbHzp0aGot9Bpu2bLFrZ93nn+RXm+8GPDnrIcu9bxhwwa3PmrUKLf+zDPPuPXLL788tRZaLjo059ybKx9qX1VV5bbdtWuXWx8zZoxbz5PG2UUip7CLREJhF4mEwi4SCYVdJBIKu0gkFHaRSOi68YnQ0sbeNcqPHTvmtn3iiSfcemic/fPPP3frntCyxgcOHHDrd9xxh1tftGiRW7/22mtTa4899pjbNnQ9/fvuu8+tP/jgg6m10PMO1U9GwT07yUdJ7iS5ut22GpKLSa5Lvg8ubzdFJKvOHMbPBzDtK9tmA1hiZuMALEl+FpEKFgy7mb0O4KvHkdcBaEhuNwC4vsT9EpESK/Y9+zAz257c3gFgWNodSdYDqC/y94hIiWT+gM7MzJvgYmbzAMwDKnsijEh3V+zQWzPJWgBIvu8sXZdEpByKDfsiALOS27MAPF+a7ohIuQQP40k+BWAqgCEktwD4KYB7ADxN8hYAmwDMKGcnu4K3/jrgr0O+YsUKt22onpW3jnlozveFF17o1ocMGeLWQ+cA3Hrrram1Tz75xG0burZ7Y2OjW+/ZM/2fd+gaBIMGDXLrJ6Ng2M1sZkrpeyXui4iUkU6XFYmEwi4SCYVdJBIKu0gkFHaRSEQzxbW2tjZTe2+qaGgq5quvvprpd/ft29ete0sXHzp0yG3b3Nzs1u+99163PnfuXLfuDWl6l8AGwktVh4besvCGWk9W2rOLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpGIZpz9u9/9rluvqalx69548eLFi4vqU2d99tlnbr2hoSG1ds0117ht6+rq3PqePXvceu/evd363r17U2ve5blDbbPypgUD4b4NG5Z6JTYA4fMX8qA9u0gkFHaRSCjsIpFQ2EUiobCLREJhF4mEwi4SiWjG2UPj6MePH3fr3rjru+++67Y966yz3HpIaJx9165dqbX169e7bUOXgg69Lt7lmgFg9OjRqbXQWHbo3IgsQr+7urrarV900UVuPbSUdR60ZxeJhMIuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIhHNOLt3bXUgvIRvVVVVai0073r69OluPWTgwIFu/Zxzzkmtha7N3tTU5NavuOIKt/7AAw+49fr6+tRanz593LahJZtHjBjh1j2h8wNCQucfVKLgnp3koyR3klzdbtsckltJNiVfV5W3myKSVWcO4+cDmNbB9p+b2cTk66XSdktESi0YdjN7HYB/TqWIVLwsH9DdTnJVcpg/OO1OJOtJNpIs38JcIhJUbNh/BWAsgIkAtgO4P+2OZjbPzCaZ2aQif5eIlEBRYTezZjNrM7PjAB4GMLm03RKRUisq7CTbr398A4DVafcVkcoQHGwk+RSAqQCGkNwC4KcAppKcCMAAbARwWxn7WBJjx44t22N7Y/AAcOWVV2Z6/JEjR7r1F154oejHDs21X7lypVufMmWKW58wYUJqbdOmTW7b0Dj7xIkT3brX93PPPddtG5rvPn78eLee5W9SLsGwm9nMDjY/Uoa+iEgZ6XRZkUgo7CKRUNhFIqGwi0RCYReJRDRTXPv16+fWQ0MtntDyvGeeeaZbb21tdeuh5YUvu+yy1Npbb73ltl27dq1bHzVqlFufNWuWW/cug53lNQfCU3+XLl2aWrvgggvctocPH3brQ4cOdeuVSHt2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSGmcvgayXPG5ra3PrLS0tbt2b6nn22We7bb3lngF/yWUg25LPWS/n7J1fAAA7duwo+rFD5z7U1dUV/dh50Z5dJBIKu0gkFHaRSCjsIpFQ2EUiobCLREJhF4lENOPsobnToctBe0JjrjU1NW49NNYd6rs3nlxdXe22HTw4deUuAMDWrVvdeuh185bCDo1lHzt2zK1PmuQvMvT444+7dU9oCe/Q61aJtGcXiYTCLhIJhV0kEgq7SCQUdpFIKOwikVDYRSLRmSWb6wAsADAMhSWa55nZL0jWAPhvAKNRWLZ5hpntLl9Xyys0Xrx7d/pTq62tTa115rGzzrXv27dvai00Xhyqh+bqh9p7Dh065NZ79erl1kNz7UPXfvd48/CB7HPx89CZPXsrgB+b2TkAvg3ghyTPATAbwBIzGwdgSfKziFSoYNjNbLuZrUxutwD4AMAIANcBaEju1gDg+nJ1UkSy+0bv2UmOBnA+gHcADDOz7UlpBwqH+SJSoTr9xoNkfwC/A/AjM9vX/nxtMzOSHb55I1kPoD5rR0Ukm07t2Un2QiHovzGzZ5PNzSRrk3otgJ0dtTWzeWY2ycz8WQsiUlbBsLOwC38EwAdm9rN2pUUATizhOQvA86XvnoiUSmcO478D4EYA75FsSrbdBeAeAE+TvAXAJgAzytPF0ggNEYWmkXrDONu3b0+tAcDy5cvdemiKa2gIyhvaCy33HHreoXqW1zV0Ce3Q8962bZtb37hxo1v3ZH3elSgYdjN7E0DaM/9eabsjIuWiM+hEIqGwi0RCYReJhMIuEgmFXSQSCrtIJE6+eXpFCl2WODSuunfv3tTauHHj3LYjR4506/v27XPrWS5zHXpeWcfhswg99tChQ9366aef7tYXLlyYWtu/f7/btpzPOy/as4tEQmEXiYTCLhIJhV0kEgq7SCQUdpFIKOwikYhmnD00lh2an+zNZ3/xxRfdtrNn+xfePXr0qFsP9c27rHFoHL1Pnz5uPbTkc+iSy1nm2ofms4fmq3vXCQg974MHD7r10Dh9JdKeXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJRDTj7KFx0dCYbu/evVNrobnyCxYscOtSHt5y06Fr1mddRrsSac8uEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0QiOM5Osg7AAgDDABiAeWb2C5JzANwK4MSk4bvM7KVydTSrZcuWufXdu3e79dAa7FmExvhD89lD7T2h66OHziEop9B89yNHjrj1lpaW1NqaNWvctt4YPQDMnz/frVeizpxU0wrgx2a2kuQAACtILk5qPzez+8rXPREplWDYzWw7gO3J7RaSHwAYUe6OiUhpfaP37CRHAzgfwDvJpttJriL5KMnBKW3qSTaSbMzUUxHJpNNhJ9kfwO8A/MjM9gH4FYCxACaisOe/v6N2ZjbPzCaZ2aQS9FdEitSpsJPshULQf2NmzwKAmTWbWZuZHQfwMIDJ5eumiGQVDDsLH9c+AuADM/tZu+217e52A4DVpe+eiJRKZz6N/w6AGwG8R7Ip2XYXgJkkJ6IwHLcRwG1l6WGJfPzxx269qanJrW/btq2U3fmSrMNbra2tJepJZQkNvYV4l9g+cOCA2zb0NwkN1Vaiznwa/yaAjgZjK3ZMXUS+TmfQiURCYReJhMIuEgmFXSQSCrtIJBR2kUhEcynpPXv2uHVvOmRn2mcRmmYamuLaXWV93t75B42N/lSN2tpat75jx46i+pQn7dlFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUiwK8dwSe4CsKndpiEAPu2yDnwzldq3Su0XoL4Vq5R9O8PMhnZU6NKwf+2Xk42Vem26Su1bpfYLUN+K1VV902G8SCQUdpFI5B32eTn/fk+l9q1S+wWob8Xqkr7l+p5dRLpO3nt2EekiCrtIJHIJO8lpJP9Icj3J2Xn0IQ3JjSTfI9mU9/p0yRp6O0mubrethuRikuuS7x2usZdT3+aQ3Jq8dk0kr8qpb3UkXyW5huT7JP852Z7ra+f0q0tety5/z06yCsBaAFcA2AJgOYCZZuYvmN1FSG4EMMnMcj8Bg+QUAPsBLDCzCcm2/wTwuZndk/xHOdjMflIhfZsDYH/ey3gnqxXVtl9mHMD1AG5Gjq+d068Z6ILXLY89+2QA681sg5kdBfBbANfl0I+KZ2avA/j8K5uvA9CQ3G5A4R9Ll0vpW0Uws+1mtjK53QLgxDLjub52Tr+6RB5hHwFgc7uft6Cy1ns3AL8nuYJkfd6d6cAwM9ue3N4BYFienelAcBnvrvSVZcYr5rUrZvnzrPQB3dddamYXAPhLAD9MDlcrkhXeg1XS2GmnlvHuKh0sM/6FPF+7Ypc/zyqPsG8FUNfu55HJtopgZluT7zsBPIfKW4q6+cQKusn3nTn35wuVtIx3R8uMowJeuzyXP88j7MsBjCN5JslqAD8AsCiHfnwNyX7JBycg2Q/A91F5S1EvAjAruT0LwPM59uVLKmUZ77RlxpHza5f78udm1uVfAK5C4RP5jwD8ex59SOnXGADvJl/v5903AE+hcFh3DIXPNm4BcBqAJQDWAfgDgJoK6tvjAN4DsAqFYNXm1LdLUThEXwWgKfm6Ku/XzulXl7xuOl1WJBL6gE4kEgq7SCQUdpFIKOwikVDYRSKhsItEQmEXicT/A76Xj+jkLMpOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "h5c5xUTNXk2v",
        "outputId": "10f0551a-d9ad-4949-e81b-e82b2c6c4297"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}