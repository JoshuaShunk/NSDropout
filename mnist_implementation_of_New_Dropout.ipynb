{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_implementation_of_New_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_implementation_of_New_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYgI3SFHqm4"
      },
      "source": [
        "# MNIST Numbers Implementation of My New Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HD9nTuEVHD"
      },
      "source": [
        "np.random.seed(seed=22) #Random seed used for comparison between old dropout"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cag8ZraxEZbF",
        "outputId": "ab0be1f0-116a-4657-a938-b6118b208b33"
      },
      "source": [
        "print(np.random.random(size=3)) #Check that seeds line up"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20846054 0.48168106 0.42053804]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers (Credit to Harrison Kinsley & Daniel Kukiela for raw python implementation)\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Og9ZrbKtV"
      },
      "source": [
        "# New Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXBWsHDIUSfh"
      },
      "source": [
        "class Layer_BinaryNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = 1 - rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, inputs, val_inputs):\n",
        "        self.inputs = inputs\n",
        "        self.val_inputs = val_inputs\n",
        "        nummask = round(len(self.inputs[0]) * self.rate)\n",
        "        \n",
        "        #Averaging Values\n",
        "        self.meanarray1 = np.mean(inputs, axis=0)\n",
        "        self.meanarray2 = np.mean(val_inputs, axis=0)\n",
        "\n",
        "        if self.iterations != 0:\n",
        "            # Calculating value\n",
        "            self.difference = self.meanarray1 - self.meanarray2\n",
        "            ind = np.argpartition(self.difference, -nummask)[-nummask:]\n",
        "            mask = np.ones(self.meanarray1.shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            self.difference[~mask] = 1\n",
        "            self.difference[mask] = 0. \n",
        "            self.binary_mask = self.difference / self.rate\n",
        "\n",
        "        else:\n",
        "            self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                                  size=inputs.shape) / self.rate\n",
        "        self.output = inputs * self.binary_mask\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiuCzwWxbRl0"
      },
      "source": [
        "class Layer_CatagoricalNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, X_test, y_test, X, y):        \n",
        "        if self.iterations != 0:\n",
        "          #Sorting data into classes\n",
        "          idx = np.argsort(y_test)\n",
        "          X_test_sorted = X_test[idx]\n",
        "          y_test_sorted = y_test[idx]\n",
        "\n",
        "          idx2 = np.argsort(y)\n",
        "          X_train_sorted = X[idx2]\n",
        "          y_train_sorted = y[idx2]\n",
        "\n",
        "          #Adding sorted data into dictionaries \n",
        "          sorted_x = {}\n",
        "          sorted_y = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x[\"class_{0}\".format(classes)] = X[y == classes]\n",
        "            sorted_y[\"label_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "          sorted_x_test = {}\n",
        "          sorted_y_test = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x_test[\"class_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "            sorted_y_test[\"label_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "          #Averaging sorted data from each class then finding the difference between the averaged train and test inputs\n",
        "          differnce_classes = {}\n",
        "          for i, classes, test_classes in zip(range(len(set(y))), sorted_x, sorted_x_test):\n",
        "            differnce_classes[\"diff_{0}\".format(i)] = np.mean(sorted_x[classes], axis=0) - np.mean(sorted_x_test[classes], axis=0)\n",
        "\n",
        "          #Masking the data taking the high values(greatest difference between train and test) and setting their values to 0\n",
        "          self.diff_mask = {}\n",
        "          for i, classes, test_classes, diff in zip(range(len(set(y))), sorted_x, sorted_x_test, differnce_classes):\n",
        "            ind = np.argpartition(differnce_classes[diff], -round(len(X[0]) * self.rate))[-round(len(X[0]) * self.rate):]\n",
        "            mask = np.ones(np.mean(sorted_x[classes],axis=0).shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            differnce_classes[diff][~mask] = 0.\n",
        "            differnce_classes[diff][mask] = 1\n",
        "            self.diff_mask[\"mask_{0}\".format(i)] = differnce_classes[diff]\n",
        "\n",
        "          #Goes through each input values and applies the apprioprite mask based on what the true output should be.\n",
        "          binary_mask = np.empty(shape=X.shape)\n",
        "          #for i, input, label in zip(range(len(X)), X, y):\n",
        "          for i, (input, label) in enumerate(zip(X,y)): \n",
        "            for true, diff in enumerate(self.diff_mask):\n",
        "              if label == true:\n",
        "                self.binary_mask[i] = self.diff_mask[diff]\n",
        "        else:\n",
        "          self.binary_mask = np.random.binomial(1, (1-self.rate), size=X.shape)\n",
        "        \n",
        "        self.output = (self.binary_mask/(1-self.rate)) * X\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def infrence(self, input, label):\n",
        "        self.input = input\n",
        "        self.label = label\n",
        "        idx = np.argsort(self.label)\n",
        "        input_sorted = input[idx]\n",
        "        label_sorted = label[idx]\n",
        "        self.infrence_binary_mask = np.empty(shape=self.input.shape)\n",
        "        for i, (input, label) in enumerate(zip(self.input, self.label)):\n",
        "          #for true, diff in zip(range(len(set(self.label))),self.diff_mask):\n",
        "          for true, diff in enumerate(self.diff_mask):\n",
        "            if label == true:\n",
        "              self.infrence_binary_mask[i] = self.diff_mask[diff]\n",
        "\n",
        "        self.output = self.infrence_binary_mask * self.input\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "fc9863b0-7191-4d10-b2e7-522da87fdcc5"
      },
      "source": [
        "(X, y), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "X = X[:8000,:,:]\n",
        "X_test = X_test[:1600,:,:]\n",
        "y = y[:8000]\n",
        "y_test  = y_test[:1600]\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 784)\n",
            "(8000,)\n",
            "(1600, 784)\n",
            "(1600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nW7mqGTnem"
      },
      "source": [
        "Sorting Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtvA9y81TpGf",
        "outputId": "bbb8fff0-62f5-4e24-8ca8-ea50c87fc4f4"
      },
      "source": [
        "idx = np.argsort(y)\n",
        "X_sorted = X[idx]\n",
        "y_sorted = y[idx]\n",
        "\n",
        "sorted_x = {}\n",
        "sorted_y = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x[\"X_{0}\".format(classes)] = X[y == classes]\n",
        "  sorted_y[\"y_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x[sorted_lists].shape[0]}')\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_0: 772\n",
            "Number of Samples for X_1: 776\n",
            "Number of Samples for X_2: 833\n",
            "Number of Samples for X_3: 805\n",
            "Number of Samples for X_4: 768\n",
            "Number of Samples for X_5: 788\n",
            "Number of Samples for X_6: 827\n",
            "Number of Samples for X_7: 796\n",
            "Number of Samples for X_8: 839\n",
            "Number of Samples for X_9: 796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaNaUO3kP2G"
      },
      "source": [
        "Sorting Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBLFeGAUkSOs",
        "outputId": "737a01af-ad76-4f7e-f599-632afc400b52"
      },
      "source": [
        "idx = np.argsort(y_test)\n",
        "X_test_sorted = X_test[idx]\n",
        "y_test_sorted = y_test[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_test = {}\n",
        "sorted_y_test = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_test[\"X_test_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "  sorted_y_test[\"y_test_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_test:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_test[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_test[sorted_lists].shape[0])\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_test_0: 161\n",
            "Number of Samples for X_test_1: 162\n",
            "Number of Samples for X_test_2: 177\n",
            "Number of Samples for X_test_3: 148\n",
            "Number of Samples for X_test_4: 184\n",
            "Number of Samples for X_test_5: 158\n",
            "Number of Samples for X_test_6: 158\n",
            "Number of Samples for X_test_7: 154\n",
            "Number of Samples for X_test_8: 151\n",
            "Number of Samples for X_test_9: 147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbnq4TJp1cl",
        "outputId": "dfd2be06-bd2a-494c-9643-89e0f60d8e92"
      },
      "source": [
        "print(f'Found {X.shape[0]} images belonging to {len(set(y))} unique classes')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 10 unique classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_CatagoricalNSDropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,len(set(y)))\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7,learning_rate=0.005)\n",
        "#optimizer = Optimizer_SGD(learning_rate=0.01)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "25942e86-01da-44d6-c904-da3f66253fc7"
      },
      "source": [
        "epochs = 488\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    if epoch != 0:\n",
        "      cached_val_inputs = cached_val_inputs\n",
        "      cached_train_inputs = activation1.output\n",
        "    else:\n",
        "      cached_val_inputs = np.random.random(size=128) #Never used just needed to pass to dropout\n",
        "      cached_train_inputs = activation1.output\n",
        "\n",
        "\n",
        "    dropout1.forward(X=activation1.output, y=y, X_test=cached_val_inputs, y_test=y_test)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "    dropout1.post_update_params()\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    if epoch == 0:\n",
        "      dense2.forward(activation1.output)\n",
        "    else:\n",
        "      dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "      dense2.forward(dropout1.output)\n",
        "    \n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        "    \n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.080, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.005000000 validation, acc: 0.156, loss: 2.302 \n",
            "epoch: 1, acc: 0.166, loss: 2.304 (data_loss: 2.302, reg_loss: 0.002), lr: 0.004999998 validation, acc: 0.179, loss: 2.297 \n",
            "epoch: 2, acc: 0.215, loss: 2.296 (data_loss: 2.295, reg_loss: 0.001), lr: 0.004999995 validation, acc: 0.221, loss: 2.249 \n",
            "epoch: 3, acc: 0.189, loss: 2.237 (data_loss: 2.236, reg_loss: 0.001), lr: 0.004999993 validation, acc: 0.203, loss: 2.087 \n",
            "epoch: 4, acc: 0.212, loss: 2.040 (data_loss: 2.039, reg_loss: 0.002), lr: 0.004999990 validation, acc: 0.286, loss: 1.819 \n",
            "epoch: 5, acc: 0.284, loss: 1.779 (data_loss: 1.776, reg_loss: 0.003), lr: 0.004999988 validation, acc: 0.212, loss: 1.800 \n",
            "epoch: 6, acc: 0.257, loss: 1.834 (data_loss: 1.829, reg_loss: 0.005), lr: 0.004999985 validation, acc: 0.330, loss: 1.583 \n",
            "epoch: 7, acc: 0.360, loss: 1.565 (data_loss: 1.559, reg_loss: 0.006), lr: 0.004999983 validation, acc: 0.402, loss: 1.516 \n",
            "epoch: 8, acc: 0.394, loss: 1.486 (data_loss: 1.480, reg_loss: 0.007), lr: 0.004999980 validation, acc: 0.429, loss: 1.396 \n",
            "epoch: 9, acc: 0.417, loss: 1.398 (data_loss: 1.391, reg_loss: 0.008), lr: 0.004999978 validation, acc: 0.463, loss: 1.311 \n",
            "epoch: 10, acc: 0.450, loss: 1.320 (data_loss: 1.311, reg_loss: 0.008), lr: 0.004999975 validation, acc: 0.464, loss: 1.219 \n",
            "epoch: 11, acc: 0.477, loss: 1.215 (data_loss: 1.206, reg_loss: 0.009), lr: 0.004999973 validation, acc: 0.561, loss: 1.117 \n",
            "epoch: 12, acc: 0.537, loss: 1.177 (data_loss: 1.167, reg_loss: 0.010), lr: 0.004999970 validation, acc: 0.632, loss: 1.082 \n",
            "epoch: 13, acc: 0.580, loss: 1.276 (data_loss: 1.265, reg_loss: 0.011), lr: 0.004999968 validation, acc: 0.574, loss: 1.093 \n",
            "epoch: 14, acc: 0.549, loss: 1.398 (data_loss: 1.387, reg_loss: 0.012), lr: 0.004999965 validation, acc: 0.664, loss: 1.043 \n",
            "epoch: 15, acc: 0.576, loss: 1.179 (data_loss: 1.167, reg_loss: 0.012), lr: 0.004999963 validation, acc: 0.541, loss: 1.216 \n",
            "epoch: 16, acc: 0.521, loss: 1.234 (data_loss: 1.221, reg_loss: 0.013), lr: 0.004999960 validation, acc: 0.692, loss: 0.923 \n",
            "epoch: 17, acc: 0.675, loss: 0.887 (data_loss: 0.874, reg_loss: 0.013), lr: 0.004999958 validation, acc: 0.734, loss: 0.953 \n",
            "epoch: 18, acc: 0.583, loss: 1.215 (data_loss: 1.202, reg_loss: 0.014), lr: 0.004999955 validation, acc: 0.667, loss: 1.051 \n",
            "epoch: 19, acc: 0.613, loss: 1.258 (data_loss: 1.244, reg_loss: 0.014), lr: 0.004999953 validation, acc: 0.627, loss: 1.129 \n",
            "epoch: 20, acc: 0.622, loss: 1.119 (data_loss: 1.104, reg_loss: 0.015), lr: 0.004999950 validation, acc: 0.639, loss: 0.922 \n",
            "epoch: 21, acc: 0.656, loss: 0.880 (data_loss: 0.864, reg_loss: 0.016), lr: 0.004999948 validation, acc: 0.689, loss: 0.833 \n",
            "epoch: 22, acc: 0.710, loss: 0.836 (data_loss: 0.819, reg_loss: 0.017), lr: 0.004999945 validation, acc: 0.854, loss: 0.686 \n",
            "epoch: 23, acc: 0.800, loss: 0.685 (data_loss: 0.668, reg_loss: 0.018), lr: 0.004999943 validation, acc: 0.792, loss: 0.697 \n",
            "epoch: 24, acc: 0.808, loss: 0.701 (data_loss: 0.683, reg_loss: 0.019), lr: 0.004999940 validation, acc: 0.877, loss: 0.557 \n",
            "epoch: 25, acc: 0.777, loss: 0.685 (data_loss: 0.666, reg_loss: 0.020), lr: 0.004999938 validation, acc: 0.821, loss: 0.611 \n",
            "epoch: 26, acc: 0.740, loss: 0.769 (data_loss: 0.749, reg_loss: 0.021), lr: 0.004999935 validation, acc: 0.871, loss: 0.526 \n",
            "epoch: 27, acc: 0.842, loss: 0.565 (data_loss: 0.544, reg_loss: 0.022), lr: 0.004999933 validation, acc: 0.859, loss: 0.548 \n",
            "epoch: 28, acc: 0.804, loss: 0.710 (data_loss: 0.688, reg_loss: 0.023), lr: 0.004999930 validation, acc: 0.848, loss: 0.576 \n",
            "epoch: 29, acc: 0.811, loss: 0.818 (data_loss: 0.795, reg_loss: 0.024), lr: 0.004999928 validation, acc: 0.810, loss: 0.728 \n",
            "epoch: 30, acc: 0.746, loss: 0.988 (data_loss: 0.964, reg_loss: 0.025), lr: 0.004999925 validation, acc: 0.838, loss: 0.615 \n",
            "epoch: 31, acc: 0.831, loss: 0.744 (data_loss: 0.718, reg_loss: 0.026), lr: 0.004999923 validation, acc: 0.819, loss: 0.604 \n",
            "epoch: 32, acc: 0.761, loss: 0.778 (data_loss: 0.751, reg_loss: 0.026), lr: 0.004999920 validation, acc: 0.805, loss: 0.576 \n",
            "epoch: 33, acc: 0.798, loss: 0.647 (data_loss: 0.620, reg_loss: 0.027), lr: 0.004999918 validation, acc: 0.856, loss: 0.434 \n",
            "epoch: 34, acc: 0.853, loss: 0.494 (data_loss: 0.466, reg_loss: 0.028), lr: 0.004999915 validation, acc: 0.861, loss: 0.443 \n",
            "epoch: 35, acc: 0.833, loss: 0.549 (data_loss: 0.520, reg_loss: 0.029), lr: 0.004999913 validation, acc: 0.887, loss: 0.418 \n",
            "epoch: 36, acc: 0.882, loss: 0.448 (data_loss: 0.418, reg_loss: 0.030), lr: 0.004999910 validation, acc: 0.917, loss: 0.329 \n",
            "epoch: 37, acc: 0.906, loss: 0.356 (data_loss: 0.326, reg_loss: 0.030), lr: 0.004999908 validation, acc: 0.902, loss: 0.304 \n",
            "epoch: 38, acc: 0.863, loss: 0.423 (data_loss: 0.392, reg_loss: 0.031), lr: 0.004999905 validation, acc: 0.921, loss: 0.275 \n",
            "epoch: 39, acc: 0.904, loss: 0.378 (data_loss: 0.346, reg_loss: 0.032), lr: 0.004999903 validation, acc: 0.903, loss: 0.286 \n",
            "epoch: 40, acc: 0.902, loss: 0.342 (data_loss: 0.309, reg_loss: 0.032), lr: 0.004999900 validation, acc: 0.886, loss: 0.308 \n",
            "epoch: 41, acc: 0.882, loss: 0.375 (data_loss: 0.342, reg_loss: 0.033), lr: 0.004999898 validation, acc: 0.935, loss: 0.207 \n",
            "epoch: 42, acc: 0.929, loss: 0.282 (data_loss: 0.248, reg_loss: 0.034), lr: 0.004999895 validation, acc: 0.942, loss: 0.192 \n",
            "epoch: 43, acc: 0.938, loss: 0.265 (data_loss: 0.231, reg_loss: 0.034), lr: 0.004999893 validation, acc: 0.940, loss: 0.203 \n",
            "epoch: 44, acc: 0.938, loss: 0.261 (data_loss: 0.226, reg_loss: 0.035), lr: 0.004999890 validation, acc: 0.946, loss: 0.190 \n",
            "epoch: 45, acc: 0.907, loss: 0.334 (data_loss: 0.299, reg_loss: 0.035), lr: 0.004999888 validation, acc: 0.939, loss: 0.212 \n",
            "epoch: 46, acc: 0.939, loss: 0.266 (data_loss: 0.231, reg_loss: 0.035), lr: 0.004999885 validation, acc: 0.953, loss: 0.170 \n",
            "epoch: 47, acc: 0.911, loss: 0.347 (data_loss: 0.312, reg_loss: 0.036), lr: 0.004999883 validation, acc: 0.934, loss: 0.228 \n",
            "epoch: 48, acc: 0.877, loss: 0.356 (data_loss: 0.320, reg_loss: 0.036), lr: 0.004999880 validation, acc: 0.951, loss: 0.195 \n",
            "epoch: 49, acc: 0.920, loss: 0.299 (data_loss: 0.263, reg_loss: 0.036), lr: 0.004999878 validation, acc: 0.949, loss: 0.182 \n",
            "epoch: 50, acc: 0.867, loss: 0.404 (data_loss: 0.368, reg_loss: 0.036), lr: 0.004999875 validation, acc: 0.934, loss: 0.224 \n",
            "epoch: 51, acc: 0.910, loss: 0.362 (data_loss: 0.325, reg_loss: 0.036), lr: 0.004999873 validation, acc: 0.839, loss: 0.391 \n",
            "epoch: 52, acc: 0.707, loss: 0.846 (data_loss: 0.810, reg_loss: 0.037), lr: 0.004999870 validation, acc: 0.875, loss: 0.418 \n",
            "epoch: 53, acc: 0.813, loss: 0.677 (data_loss: 0.640, reg_loss: 0.037), lr: 0.004999868 validation, acc: 0.906, loss: 0.355 \n",
            "epoch: 54, acc: 0.861, loss: 0.822 (data_loss: 0.785, reg_loss: 0.038), lr: 0.004999865 validation, acc: 0.867, loss: 0.689 \n",
            "epoch: 55, acc: 0.855, loss: 0.856 (data_loss: 0.818, reg_loss: 0.038), lr: 0.004999863 validation, acc: 0.872, loss: 0.603 \n",
            "epoch: 56, acc: 0.782, loss: 0.998 (data_loss: 0.959, reg_loss: 0.039), lr: 0.004999860 validation, acc: 0.901, loss: 0.384 \n",
            "epoch: 57, acc: 0.896, loss: 0.489 (data_loss: 0.450, reg_loss: 0.039), lr: 0.004999858 validation, acc: 0.933, loss: 0.277 \n",
            "epoch: 58, acc: 0.899, loss: 0.435 (data_loss: 0.396, reg_loss: 0.040), lr: 0.004999855 validation, acc: 0.916, loss: 0.289 \n",
            "epoch: 59, acc: 0.881, loss: 0.439 (data_loss: 0.399, reg_loss: 0.040), lr: 0.004999853 validation, acc: 0.936, loss: 0.241 \n",
            "epoch: 60, acc: 0.847, loss: 0.566 (data_loss: 0.525, reg_loss: 0.041), lr: 0.004999850 validation, acc: 0.921, loss: 0.255 \n",
            "epoch: 61, acc: 0.854, loss: 0.582 (data_loss: 0.540, reg_loss: 0.041), lr: 0.004999848 validation, acc: 0.922, loss: 0.288 \n",
            "epoch: 62, acc: 0.857, loss: 0.597 (data_loss: 0.555, reg_loss: 0.042), lr: 0.004999845 validation, acc: 0.868, loss: 0.412 \n",
            "epoch: 63, acc: 0.879, loss: 0.491 (data_loss: 0.448, reg_loss: 0.043), lr: 0.004999843 validation, acc: 0.932, loss: 0.249 \n",
            "epoch: 64, acc: 0.889, loss: 0.413 (data_loss: 0.370, reg_loss: 0.043), lr: 0.004999840 validation, acc: 0.930, loss: 0.247 \n",
            "epoch: 65, acc: 0.924, loss: 0.321 (data_loss: 0.278, reg_loss: 0.044), lr: 0.004999838 validation, acc: 0.943, loss: 0.226 \n",
            "epoch: 66, acc: 0.922, loss: 0.344 (data_loss: 0.300, reg_loss: 0.044), lr: 0.004999835 validation, acc: 0.953, loss: 0.206 \n",
            "epoch: 67, acc: 0.954, loss: 0.255 (data_loss: 0.210, reg_loss: 0.045), lr: 0.004999833 validation, acc: 0.980, loss: 0.128 \n",
            "epoch: 68, acc: 0.950, loss: 0.255 (data_loss: 0.210, reg_loss: 0.046), lr: 0.004999830 validation, acc: 0.965, loss: 0.155 \n",
            "epoch: 69, acc: 0.898, loss: 0.532 (data_loss: 0.486, reg_loss: 0.046), lr: 0.004999828 validation, acc: 0.941, loss: 0.256 \n",
            "epoch: 70, acc: 0.808, loss: 0.645 (data_loss: 0.599, reg_loss: 0.046), lr: 0.004999825 validation, acc: 0.922, loss: 0.290 \n",
            "epoch: 71, acc: 0.891, loss: 0.387 (data_loss: 0.340, reg_loss: 0.047), lr: 0.004999823 validation, acc: 0.940, loss: 0.212 \n",
            "epoch: 72, acc: 0.827, loss: 0.584 (data_loss: 0.536, reg_loss: 0.047), lr: 0.004999820 validation, acc: 0.885, loss: 0.299 \n",
            "epoch: 73, acc: 0.737, loss: 1.217 (data_loss: 1.170, reg_loss: 0.048), lr: 0.004999818 validation, acc: 0.858, loss: 0.827 \n",
            "epoch: 74, acc: 0.806, loss: 1.024 (data_loss: 0.977, reg_loss: 0.048), lr: 0.004999815 validation, acc: 0.692, loss: 1.081 \n",
            "epoch: 75, acc: 0.742, loss: 1.219 (data_loss: 1.171, reg_loss: 0.048), lr: 0.004999813 validation, acc: 0.744, loss: 1.103 \n",
            "epoch: 76, acc: 0.755, loss: 1.946 (data_loss: 1.897, reg_loss: 0.048), lr: 0.004999810 validation, acc: 0.822, loss: 1.390 \n",
            "epoch: 77, acc: 0.805, loss: 1.176 (data_loss: 1.127, reg_loss: 0.049), lr: 0.004999808 validation, acc: 0.877, loss: 0.935 \n",
            "epoch: 78, acc: 0.794, loss: 1.405 (data_loss: 1.355, reg_loss: 0.049), lr: 0.004999805 validation, acc: 0.718, loss: 1.287 \n",
            "epoch: 79, acc: 0.720, loss: 1.493 (data_loss: 1.444, reg_loss: 0.050), lr: 0.004999803 validation, acc: 0.811, loss: 0.828 \n",
            "epoch: 80, acc: 0.810, loss: 0.981 (data_loss: 0.931, reg_loss: 0.050), lr: 0.004999800 validation, acc: 0.867, loss: 0.537 \n",
            "epoch: 81, acc: 0.844, loss: 0.716 (data_loss: 0.666, reg_loss: 0.050), lr: 0.004999798 validation, acc: 0.910, loss: 0.237 \n",
            "epoch: 82, acc: 0.913, loss: 0.309 (data_loss: 0.259, reg_loss: 0.050), lr: 0.004999795 validation, acc: 0.891, loss: 0.291 \n",
            "epoch: 83, acc: 0.775, loss: 0.671 (data_loss: 0.620, reg_loss: 0.051), lr: 0.004999793 validation, acc: 0.855, loss: 0.438 \n",
            "epoch: 84, acc: 0.863, loss: 0.561 (data_loss: 0.510, reg_loss: 0.051), lr: 0.004999790 validation, acc: 0.916, loss: 0.239 \n",
            "epoch: 85, acc: 0.916, loss: 0.317 (data_loss: 0.265, reg_loss: 0.051), lr: 0.004999788 validation, acc: 0.903, loss: 0.308 \n",
            "epoch: 86, acc: 0.839, loss: 0.506 (data_loss: 0.455, reg_loss: 0.051), lr: 0.004999785 validation, acc: 0.919, loss: 0.229 \n",
            "epoch: 87, acc: 0.911, loss: 0.327 (data_loss: 0.276, reg_loss: 0.052), lr: 0.004999783 validation, acc: 0.930, loss: 0.222 \n",
            "epoch: 88, acc: 0.954, loss: 0.215 (data_loss: 0.163, reg_loss: 0.052), lr: 0.004999780 validation, acc: 0.955, loss: 0.155 \n",
            "epoch: 89, acc: 0.933, loss: 0.256 (data_loss: 0.204, reg_loss: 0.052), lr: 0.004999778 validation, acc: 0.933, loss: 0.202 \n",
            "epoch: 90, acc: 0.923, loss: 0.276 (data_loss: 0.223, reg_loss: 0.052), lr: 0.004999775 validation, acc: 0.948, loss: 0.170 \n",
            "epoch: 91, acc: 0.938, loss: 0.235 (data_loss: 0.182, reg_loss: 0.053), lr: 0.004999773 validation, acc: 0.976, loss: 0.111 \n",
            "epoch: 92, acc: 0.953, loss: 0.216 (data_loss: 0.163, reg_loss: 0.053), lr: 0.004999770 validation, acc: 0.971, loss: 0.122 \n",
            "epoch: 93, acc: 0.838, loss: 0.576 (data_loss: 0.523, reg_loss: 0.053), lr: 0.004999768 validation, acc: 0.926, loss: 0.272 \n",
            "epoch: 94, acc: 0.784, loss: 1.007 (data_loss: 0.954, reg_loss: 0.053), lr: 0.004999765 validation, acc: 0.899, loss: 0.390 \n",
            "epoch: 95, acc: 0.956, loss: 0.260 (data_loss: 0.207, reg_loss: 0.053), lr: 0.004999763 validation, acc: 0.968, loss: 0.147 \n",
            "epoch: 96, acc: 0.952, loss: 0.238 (data_loss: 0.185, reg_loss: 0.053), lr: 0.004999760 validation, acc: 0.967, loss: 0.124 \n",
            "epoch: 97, acc: 0.897, loss: 0.469 (data_loss: 0.417, reg_loss: 0.052), lr: 0.004999758 validation, acc: 0.948, loss: 0.211 \n",
            "epoch: 98, acc: 0.937, loss: 0.289 (data_loss: 0.237, reg_loss: 0.052), lr: 0.004999755 validation, acc: 0.941, loss: 0.227 \n",
            "epoch: 99, acc: 0.940, loss: 0.281 (data_loss: 0.229, reg_loss: 0.052), lr: 0.004999753 validation, acc: 0.966, loss: 0.142 \n",
            "epoch: 100, acc: 0.971, loss: 0.178 (data_loss: 0.126, reg_loss: 0.052), lr: 0.004999750 validation, acc: 0.978, loss: 0.097 \n",
            "epoch: 101, acc: 0.984, loss: 0.133 (data_loss: 0.081, reg_loss: 0.052), lr: 0.004999748 validation, acc: 0.976, loss: 0.112 \n",
            "epoch: 102, acc: 0.943, loss: 0.222 (data_loss: 0.170, reg_loss: 0.052), lr: 0.004999745 validation, acc: 0.976, loss: 0.107 \n",
            "epoch: 103, acc: 0.975, loss: 0.164 (data_loss: 0.113, reg_loss: 0.052), lr: 0.004999743 validation, acc: 0.974, loss: 0.112 \n",
            "epoch: 104, acc: 0.983, loss: 0.143 (data_loss: 0.092, reg_loss: 0.051), lr: 0.004999740 validation, acc: 0.984, loss: 0.085 \n",
            "epoch: 105, acc: 0.951, loss: 0.252 (data_loss: 0.200, reg_loss: 0.051), lr: 0.004999738 validation, acc: 0.971, loss: 0.158 \n",
            "epoch: 106, acc: 0.951, loss: 0.252 (data_loss: 0.201, reg_loss: 0.051), lr: 0.004999735 validation, acc: 0.976, loss: 0.132 \n",
            "epoch: 107, acc: 0.983, loss: 0.160 (data_loss: 0.110, reg_loss: 0.051), lr: 0.004999733 validation, acc: 0.982, loss: 0.090 \n",
            "epoch: 108, acc: 0.981, loss: 0.159 (data_loss: 0.109, reg_loss: 0.050), lr: 0.004999730 validation, acc: 0.979, loss: 0.076 \n",
            "epoch: 109, acc: 0.883, loss: 0.422 (data_loss: 0.372, reg_loss: 0.050), lr: 0.004999728 validation, acc: 0.980, loss: 0.084 \n",
            "epoch: 110, acc: 0.981, loss: 0.144 (data_loss: 0.094, reg_loss: 0.049), lr: 0.004999725 validation, acc: 0.973, loss: 0.126 \n",
            "epoch: 111, acc: 0.960, loss: 0.198 (data_loss: 0.150, reg_loss: 0.049), lr: 0.004999723 validation, acc: 0.985, loss: 0.082 \n",
            "epoch: 112, acc: 0.984, loss: 0.128 (data_loss: 0.080, reg_loss: 0.048), lr: 0.004999720 validation, acc: 0.984, loss: 0.069 \n",
            "epoch: 113, acc: 0.982, loss: 0.134 (data_loss: 0.086, reg_loss: 0.048), lr: 0.004999718 validation, acc: 0.979, loss: 0.084 \n",
            "epoch: 114, acc: 0.981, loss: 0.130 (data_loss: 0.083, reg_loss: 0.048), lr: 0.004999715 validation, acc: 0.986, loss: 0.064 \n",
            "epoch: 115, acc: 0.980, loss: 0.131 (data_loss: 0.084, reg_loss: 0.047), lr: 0.004999713 validation, acc: 0.989, loss: 0.055 \n",
            "epoch: 116, acc: 0.989, loss: 0.109 (data_loss: 0.062, reg_loss: 0.047), lr: 0.004999710 validation, acc: 0.989, loss: 0.045 \n",
            "epoch: 117, acc: 0.991, loss: 0.101 (data_loss: 0.055, reg_loss: 0.046), lr: 0.004999708 validation, acc: 0.990, loss: 0.044 \n",
            "epoch: 118, acc: 0.992, loss: 0.095 (data_loss: 0.049, reg_loss: 0.046), lr: 0.004999705 validation, acc: 0.994, loss: 0.038 \n",
            "epoch: 119, acc: 0.992, loss: 0.091 (data_loss: 0.046, reg_loss: 0.045), lr: 0.004999703 validation, acc: 0.994, loss: 0.035 \n",
            "epoch: 120, acc: 0.994, loss: 0.084 (data_loss: 0.040, reg_loss: 0.044), lr: 0.004999700 validation, acc: 0.994, loss: 0.030 \n",
            "epoch: 121, acc: 0.816, loss: 0.831 (data_loss: 0.788, reg_loss: 0.044), lr: 0.004999698 validation, acc: 0.894, loss: 0.247 \n",
            "epoch: 122, acc: 0.899, loss: 0.278 (data_loss: 0.235, reg_loss: 0.043), lr: 0.004999695 validation, acc: 0.941, loss: 0.185 \n",
            "epoch: 123, acc: 0.911, loss: 0.321 (data_loss: 0.278, reg_loss: 0.043), lr: 0.004999693 validation, acc: 0.922, loss: 0.234 \n",
            "epoch: 124, acc: 0.916, loss: 0.332 (data_loss: 0.290, reg_loss: 0.042), lr: 0.004999690 validation, acc: 0.964, loss: 0.119 \n",
            "epoch: 125, acc: 0.964, loss: 0.170 (data_loss: 0.128, reg_loss: 0.042), lr: 0.004999688 validation, acc: 0.981, loss: 0.056 \n",
            "epoch: 126, acc: 0.942, loss: 0.219 (data_loss: 0.178, reg_loss: 0.041), lr: 0.004999685 validation, acc: 0.974, loss: 0.076 \n",
            "epoch: 127, acc: 0.970, loss: 0.129 (data_loss: 0.088, reg_loss: 0.041), lr: 0.004999683 validation, acc: 0.965, loss: 0.108 \n",
            "epoch: 128, acc: 0.932, loss: 0.233 (data_loss: 0.192, reg_loss: 0.040), lr: 0.004999680 validation, acc: 0.977, loss: 0.081 \n",
            "epoch: 129, acc: 0.980, loss: 0.121 (data_loss: 0.082, reg_loss: 0.040), lr: 0.004999678 validation, acc: 0.974, loss: 0.091 \n",
            "epoch: 130, acc: 0.926, loss: 0.249 (data_loss: 0.210, reg_loss: 0.039), lr: 0.004999675 validation, acc: 0.950, loss: 0.149 \n",
            "epoch: 131, acc: 0.952, loss: 0.195 (data_loss: 0.155, reg_loss: 0.039), lr: 0.004999673 validation, acc: 0.962, loss: 0.118 \n",
            "epoch: 132, acc: 0.967, loss: 0.166 (data_loss: 0.127, reg_loss: 0.039), lr: 0.004999670 validation, acc: 0.981, loss: 0.077 \n",
            "epoch: 133, acc: 0.985, loss: 0.097 (data_loss: 0.058, reg_loss: 0.039), lr: 0.004999668 validation, acc: 0.987, loss: 0.041 \n",
            "epoch: 134, acc: 0.987, loss: 0.093 (data_loss: 0.054, reg_loss: 0.039), lr: 0.004999665 validation, acc: 0.982, loss: 0.053 \n",
            "epoch: 135, acc: 0.984, loss: 0.100 (data_loss: 0.062, reg_loss: 0.038), lr: 0.004999663 validation, acc: 0.985, loss: 0.048 \n",
            "epoch: 136, acc: 0.985, loss: 0.087 (data_loss: 0.049, reg_loss: 0.038), lr: 0.004999660 validation, acc: 0.991, loss: 0.031 \n",
            "epoch: 137, acc: 0.991, loss: 0.072 (data_loss: 0.034, reg_loss: 0.038), lr: 0.004999658 validation, acc: 0.993, loss: 0.026 \n",
            "epoch: 138, acc: 0.990, loss: 0.080 (data_loss: 0.042, reg_loss: 0.037), lr: 0.004999655 validation, acc: 0.991, loss: 0.033 \n",
            "epoch: 139, acc: 0.982, loss: 0.096 (data_loss: 0.059, reg_loss: 0.037), lr: 0.004999653 validation, acc: 0.990, loss: 0.032 \n",
            "epoch: 140, acc: 0.993, loss: 0.072 (data_loss: 0.035, reg_loss: 0.037), lr: 0.004999650 validation, acc: 0.994, loss: 0.028 \n",
            "epoch: 141, acc: 0.994, loss: 0.071 (data_loss: 0.035, reg_loss: 0.036), lr: 0.004999648 validation, acc: 0.996, loss: 0.031 \n",
            "epoch: 142, acc: 0.992, loss: 0.077 (data_loss: 0.041, reg_loss: 0.036), lr: 0.004999645 validation, acc: 0.999, loss: 0.020 \n",
            "epoch: 143, acc: 0.995, loss: 0.063 (data_loss: 0.028, reg_loss: 0.035), lr: 0.004999643 validation, acc: 0.999, loss: 0.016 \n",
            "epoch: 144, acc: 0.891, loss: 0.455 (data_loss: 0.420, reg_loss: 0.035), lr: 0.004999640 validation, acc: 0.947, loss: 0.176 \n",
            "epoch: 145, acc: 0.940, loss: 0.239 (data_loss: 0.205, reg_loss: 0.034), lr: 0.004999638 validation, acc: 0.959, loss: 0.141 \n",
            "epoch: 146, acc: 0.924, loss: 0.240 (data_loss: 0.207, reg_loss: 0.034), lr: 0.004999635 validation, acc: 0.960, loss: 0.109 \n",
            "epoch: 147, acc: 0.951, loss: 0.163 (data_loss: 0.130, reg_loss: 0.033), lr: 0.004999633 validation, acc: 0.978, loss: 0.082 \n",
            "epoch: 148, acc: 0.919, loss: 0.326 (data_loss: 0.293, reg_loss: 0.033), lr: 0.004999630 validation, acc: 0.962, loss: 0.117 \n",
            "epoch: 149, acc: 0.967, loss: 0.149 (data_loss: 0.117, reg_loss: 0.033), lr: 0.004999628 validation, acc: 0.923, loss: 0.214 \n",
            "epoch: 150, acc: 0.914, loss: 0.311 (data_loss: 0.278, reg_loss: 0.032), lr: 0.004999625 validation, acc: 0.948, loss: 0.168 \n",
            "epoch: 151, acc: 0.822, loss: 1.035 (data_loss: 1.003, reg_loss: 0.032), lr: 0.004999623 validation, acc: 0.810, loss: 0.926 \n",
            "epoch: 152, acc: 0.839, loss: 0.545 (data_loss: 0.514, reg_loss: 0.032), lr: 0.004999620 validation, acc: 0.905, loss: 0.328 \n",
            "epoch: 153, acc: 0.874, loss: 0.582 (data_loss: 0.551, reg_loss: 0.032), lr: 0.004999618 validation, acc: 0.948, loss: 0.156 \n",
            "epoch: 154, acc: 0.939, loss: 0.215 (data_loss: 0.184, reg_loss: 0.031), lr: 0.004999615 validation, acc: 0.944, loss: 0.154 \n",
            "epoch: 155, acc: 0.909, loss: 0.292 (data_loss: 0.260, reg_loss: 0.031), lr: 0.004999613 validation, acc: 0.963, loss: 0.112 \n",
            "epoch: 156, acc: 0.857, loss: 1.128 (data_loss: 1.097, reg_loss: 0.031), lr: 0.004999610 validation, acc: 0.864, loss: 1.065 \n",
            "epoch: 157, acc: 0.815, loss: 1.729 (data_loss: 1.699, reg_loss: 0.031), lr: 0.004999608 validation, acc: 0.873, loss: 1.269 \n",
            "epoch: 158, acc: 0.871, loss: 1.636 (data_loss: 1.605, reg_loss: 0.031), lr: 0.004999605 validation, acc: 0.884, loss: 1.339 \n",
            "epoch: 159, acc: 0.845, loss: 1.645 (data_loss: 1.615, reg_loss: 0.031), lr: 0.004999603 validation, acc: 0.879, loss: 1.381 \n",
            "epoch: 160, acc: 0.750, loss: 2.444 (data_loss: 2.414, reg_loss: 0.031), lr: 0.004999600 validation, acc: 0.789, loss: 2.310 \n",
            "epoch: 161, acc: 0.718, loss: 2.780 (data_loss: 2.750, reg_loss: 0.031), lr: 0.004999598 validation, acc: 0.771, loss: 2.643 \n",
            "epoch: 162, acc: 0.767, loss: 2.943 (data_loss: 2.912, reg_loss: 0.031), lr: 0.004999595 validation, acc: 0.762, loss: 2.813 \n",
            "epoch: 163, acc: 0.759, loss: 3.126 (data_loss: 3.096, reg_loss: 0.031), lr: 0.004999593 validation, acc: 0.783, loss: 2.683 \n",
            "epoch: 164, acc: 0.772, loss: 2.561 (data_loss: 2.530, reg_loss: 0.031), lr: 0.004999590 validation, acc: 0.787, loss: 2.360 \n",
            "epoch: 165, acc: 0.636, loss: 2.728 (data_loss: 2.697, reg_loss: 0.031), lr: 0.004999588 validation, acc: 0.736, loss: 1.995 \n",
            "epoch: 166, acc: 0.618, loss: 3.027 (data_loss: 2.996, reg_loss: 0.031), lr: 0.004999585 validation, acc: 0.728, loss: 2.235 \n",
            "epoch: 167, acc: 0.629, loss: 2.803 (data_loss: 2.772, reg_loss: 0.031), lr: 0.004999583 validation, acc: 0.727, loss: 2.144 \n",
            "epoch: 168, acc: 0.630, loss: 2.733 (data_loss: 2.701, reg_loss: 0.031), lr: 0.004999580 validation, acc: 0.642, loss: 2.315 \n",
            "epoch: 169, acc: 0.589, loss: 2.719 (data_loss: 2.687, reg_loss: 0.032), lr: 0.004999578 validation, acc: 0.681, loss: 2.335 \n",
            "epoch: 170, acc: 0.614, loss: 2.963 (data_loss: 2.931, reg_loss: 0.032), lr: 0.004999575 validation, acc: 0.615, loss: 3.134 \n",
            "epoch: 171, acc: 0.527, loss: 3.648 (data_loss: 3.615, reg_loss: 0.033), lr: 0.004999573 validation, acc: 0.572, loss: 3.841 \n",
            "epoch: 172, acc: 0.565, loss: 4.803 (data_loss: 4.769, reg_loss: 0.034), lr: 0.004999570 validation, acc: 0.590, loss: 4.467 \n",
            "epoch: 173, acc: 0.494, loss: 5.040 (data_loss: 5.005, reg_loss: 0.035), lr: 0.004999568 validation, acc: 0.552, loss: 4.468 \n",
            "epoch: 174, acc: 0.546, loss: 4.730 (data_loss: 4.695, reg_loss: 0.036), lr: 0.004999565 validation, acc: 0.580, loss: 4.037 \n",
            "epoch: 175, acc: 0.485, loss: 4.837 (data_loss: 4.800, reg_loss: 0.037), lr: 0.004999563 validation, acc: 0.547, loss: 3.815 \n",
            "epoch: 176, acc: 0.539, loss: 4.557 (data_loss: 4.520, reg_loss: 0.037), lr: 0.004999560 validation, acc: 0.576, loss: 3.851 \n",
            "epoch: 177, acc: 0.596, loss: 4.083 (data_loss: 4.045, reg_loss: 0.038), lr: 0.004999558 validation, acc: 0.611, loss: 3.686 \n",
            "epoch: 178, acc: 0.583, loss: 4.303 (data_loss: 4.264, reg_loss: 0.039), lr: 0.004999555 validation, acc: 0.616, loss: 3.739 \n",
            "epoch: 179, acc: 0.612, loss: 4.677 (data_loss: 4.637, reg_loss: 0.040), lr: 0.004999553 validation, acc: 0.617, loss: 4.088 \n",
            "epoch: 180, acc: 0.602, loss: 4.236 (data_loss: 4.196, reg_loss: 0.040), lr: 0.004999550 validation, acc: 0.624, loss: 3.518 \n",
            "epoch: 181, acc: 0.625, loss: 3.937 (data_loss: 3.896, reg_loss: 0.041), lr: 0.004999548 validation, acc: 0.634, loss: 3.395 \n",
            "epoch: 182, acc: 0.543, loss: 4.016 (data_loss: 3.975, reg_loss: 0.041), lr: 0.004999545 validation, acc: 0.641, loss: 3.373 \n",
            "epoch: 183, acc: 0.513, loss: 3.893 (data_loss: 3.851, reg_loss: 0.042), lr: 0.004999543 validation, acc: 0.629, loss: 2.719 \n",
            "epoch: 184, acc: 0.635, loss: 2.980 (data_loss: 2.938, reg_loss: 0.042), lr: 0.004999540 validation, acc: 0.734, loss: 2.104 \n",
            "epoch: 185, acc: 0.717, loss: 2.579 (data_loss: 2.536, reg_loss: 0.043), lr: 0.004999538 validation, acc: 0.731, loss: 1.869 \n",
            "epoch: 186, acc: 0.721, loss: 2.274 (data_loss: 2.230, reg_loss: 0.044), lr: 0.004999535 validation, acc: 0.731, loss: 1.813 \n",
            "epoch: 187, acc: 0.726, loss: 2.083 (data_loss: 2.039, reg_loss: 0.045), lr: 0.004999533 validation, acc: 0.731, loss: 1.845 \n",
            "epoch: 188, acc: 0.659, loss: 2.396 (data_loss: 2.351, reg_loss: 0.045), lr: 0.004999530 validation, acc: 0.685, loss: 2.134 \n",
            "epoch: 189, acc: 0.578, loss: 3.015 (data_loss: 2.970, reg_loss: 0.046), lr: 0.004999528 validation, acc: 0.655, loss: 2.574 \n",
            "epoch: 190, acc: 0.631, loss: 2.856 (data_loss: 2.810, reg_loss: 0.046), lr: 0.004999525 validation, acc: 0.654, loss: 2.252 \n",
            "epoch: 191, acc: 0.576, loss: 3.986 (data_loss: 3.940, reg_loss: 0.046), lr: 0.004999523 validation, acc: 0.614, loss: 3.029 \n",
            "epoch: 192, acc: 0.488, loss: 4.538 (data_loss: 4.492, reg_loss: 0.046), lr: 0.004999520 validation, acc: 0.553, loss: 3.575 \n",
            "epoch: 193, acc: 0.456, loss: 4.516 (data_loss: 4.470, reg_loss: 0.046), lr: 0.004999518 validation, acc: 0.600, loss: 3.515 \n",
            "epoch: 194, acc: 0.559, loss: 4.183 (data_loss: 4.136, reg_loss: 0.046), lr: 0.004999515 validation, acc: 0.594, loss: 3.576 \n",
            "epoch: 195, acc: 0.585, loss: 4.036 (data_loss: 3.990, reg_loss: 0.047), lr: 0.004999513 validation, acc: 0.637, loss: 3.457 \n",
            "epoch: 196, acc: 0.595, loss: 4.357 (data_loss: 4.311, reg_loss: 0.047), lr: 0.004999510 validation, acc: 0.613, loss: 3.552 \n",
            "epoch: 197, acc: 0.585, loss: 4.107 (data_loss: 4.060, reg_loss: 0.047), lr: 0.004999508 validation, acc: 0.600, loss: 3.343 \n",
            "epoch: 198, acc: 0.567, loss: 3.997 (data_loss: 3.950, reg_loss: 0.047), lr: 0.004999505 validation, acc: 0.599, loss: 3.201 \n",
            "epoch: 199, acc: 0.592, loss: 3.507 (data_loss: 3.461, reg_loss: 0.047), lr: 0.004999503 validation, acc: 0.637, loss: 2.940 \n",
            "epoch: 200, acc: 0.625, loss: 3.479 (data_loss: 3.432, reg_loss: 0.046), lr: 0.004999500 validation, acc: 0.652, loss: 2.911 \n",
            "epoch: 201, acc: 0.627, loss: 3.316 (data_loss: 3.270, reg_loss: 0.046), lr: 0.004999498 validation, acc: 0.642, loss: 2.921 \n",
            "epoch: 202, acc: 0.638, loss: 3.518 (data_loss: 3.472, reg_loss: 0.046), lr: 0.004999495 validation, acc: 0.640, loss: 3.123 \n",
            "epoch: 203, acc: 0.648, loss: 3.542 (data_loss: 3.496, reg_loss: 0.046), lr: 0.004999493 validation, acc: 0.662, loss: 3.112 \n",
            "epoch: 204, acc: 0.662, loss: 3.834 (data_loss: 3.789, reg_loss: 0.046), lr: 0.004999490 validation, acc: 0.679, loss: 3.229 \n",
            "epoch: 205, acc: 0.656, loss: 3.724 (data_loss: 3.678, reg_loss: 0.045), lr: 0.004999488 validation, acc: 0.684, loss: 3.128 \n",
            "epoch: 206, acc: 0.673, loss: 3.596 (data_loss: 3.551, reg_loss: 0.045), lr: 0.004999485 validation, acc: 0.680, loss: 3.111 \n",
            "epoch: 207, acc: 0.678, loss: 3.513 (data_loss: 3.469, reg_loss: 0.044), lr: 0.004999483 validation, acc: 0.682, loss: 3.045 \n",
            "epoch: 208, acc: 0.678, loss: 3.466 (data_loss: 3.422, reg_loss: 0.044), lr: 0.004999480 validation, acc: 0.684, loss: 2.999 \n",
            "epoch: 209, acc: 0.681, loss: 3.348 (data_loss: 3.305, reg_loss: 0.044), lr: 0.004999478 validation, acc: 0.689, loss: 2.896 \n",
            "epoch: 210, acc: 0.680, loss: 3.210 (data_loss: 3.167, reg_loss: 0.043), lr: 0.004999475 validation, acc: 0.681, loss: 2.783 \n",
            "epoch: 211, acc: 0.656, loss: 3.136 (data_loss: 3.094, reg_loss: 0.043), lr: 0.004999473 validation, acc: 0.679, loss: 2.745 \n",
            "epoch: 212, acc: 0.638, loss: 3.099 (data_loss: 3.057, reg_loss: 0.042), lr: 0.004999470 validation, acc: 0.654, loss: 2.694 \n",
            "epoch: 213, acc: 0.660, loss: 2.978 (data_loss: 2.936, reg_loss: 0.042), lr: 0.004999468 validation, acc: 0.674, loss: 2.648 \n",
            "epoch: 214, acc: 0.595, loss: 3.006 (data_loss: 2.964, reg_loss: 0.041), lr: 0.004999465 validation, acc: 0.662, loss: 2.634 \n",
            "epoch: 215, acc: 0.630, loss: 2.882 (data_loss: 2.841, reg_loss: 0.041), lr: 0.004999463 validation, acc: 0.646, loss: 2.565 \n",
            "epoch: 216, acc: 0.648, loss: 2.696 (data_loss: 2.656, reg_loss: 0.040), lr: 0.004999460 validation, acc: 0.649, loss: 2.429 \n",
            "epoch: 217, acc: 0.641, loss: 2.568 (data_loss: 2.528, reg_loss: 0.040), lr: 0.004999458 validation, acc: 0.665, loss: 2.295 \n",
            "epoch: 218, acc: 0.662, loss: 2.434 (data_loss: 2.395, reg_loss: 0.039), lr: 0.004999455 validation, acc: 0.688, loss: 2.235 \n",
            "epoch: 219, acc: 0.683, loss: 2.333 (data_loss: 2.294, reg_loss: 0.039), lr: 0.004999453 validation, acc: 0.759, loss: 2.190 \n",
            "epoch: 220, acc: 0.736, loss: 2.267 (data_loss: 2.229, reg_loss: 0.039), lr: 0.004999450 validation, acc: 0.782, loss: 2.092 \n",
            "epoch: 221, acc: 0.771, loss: 2.176 (data_loss: 2.138, reg_loss: 0.038), lr: 0.004999448 validation, acc: 0.777, loss: 2.038 \n",
            "epoch: 222, acc: 0.633, loss: 2.486 (data_loss: 2.448, reg_loss: 0.038), lr: 0.004999445 validation, acc: 0.706, loss: 2.227 \n",
            "epoch: 223, acc: 0.718, loss: 2.321 (data_loss: 2.283, reg_loss: 0.038), lr: 0.004999443 validation, acc: 0.736, loss: 2.185 \n",
            "epoch: 224, acc: 0.737, loss: 2.305 (data_loss: 2.268, reg_loss: 0.037), lr: 0.004999440 validation, acc: 0.742, loss: 2.200 \n",
            "epoch: 225, acc: 0.748, loss: 2.228 (data_loss: 2.191, reg_loss: 0.037), lr: 0.004999438 validation, acc: 0.745, loss: 2.078 \n",
            "epoch: 226, acc: 0.752, loss: 2.171 (data_loss: 2.134, reg_loss: 0.037), lr: 0.004999435 validation, acc: 0.759, loss: 1.969 \n",
            "epoch: 227, acc: 0.755, loss: 2.011 (data_loss: 1.974, reg_loss: 0.037), lr: 0.004999433 validation, acc: 0.819, loss: 1.895 \n",
            "epoch: 228, acc: 0.756, loss: 2.033 (data_loss: 1.997, reg_loss: 0.036), lr: 0.004999430 validation, acc: 0.782, loss: 1.903 \n",
            "epoch: 229, acc: 0.769, loss: 2.016 (data_loss: 1.980, reg_loss: 0.036), lr: 0.004999428 validation, acc: 0.792, loss: 1.889 \n",
            "epoch: 230, acc: 0.789, loss: 1.932 (data_loss: 1.897, reg_loss: 0.035), lr: 0.004999425 validation, acc: 0.808, loss: 1.854 \n",
            "epoch: 231, acc: 0.804, loss: 1.888 (data_loss: 1.853, reg_loss: 0.035), lr: 0.004999423 validation, acc: 0.870, loss: 1.771 \n",
            "epoch: 232, acc: 0.862, loss: 1.819 (data_loss: 1.784, reg_loss: 0.034), lr: 0.004999420 validation, acc: 0.887, loss: 1.704 \n",
            "epoch: 233, acc: 0.867, loss: 1.816 (data_loss: 1.782, reg_loss: 0.034), lr: 0.004999418 validation, acc: 0.886, loss: 1.713 \n",
            "epoch: 234, acc: 0.878, loss: 1.779 (data_loss: 1.745, reg_loss: 0.034), lr: 0.004999415 validation, acc: 0.880, loss: 1.714 \n",
            "epoch: 235, acc: 0.880, loss: 1.778 (data_loss: 1.745, reg_loss: 0.033), lr: 0.004999413 validation, acc: 0.884, loss: 1.711 \n",
            "epoch: 236, acc: 0.871, loss: 1.769 (data_loss: 1.736, reg_loss: 0.033), lr: 0.004999410 validation, acc: 0.877, loss: 1.709 \n",
            "epoch: 237, acc: 0.869, loss: 1.765 (data_loss: 1.732, reg_loss: 0.033), lr: 0.004999408 validation, acc: 0.886, loss: 1.665 \n",
            "epoch: 238, acc: 0.880, loss: 1.729 (data_loss: 1.696, reg_loss: 0.032), lr: 0.004999405 validation, acc: 0.887, loss: 1.661 \n",
            "epoch: 239, acc: 0.880, loss: 1.718 (data_loss: 1.686, reg_loss: 0.032), lr: 0.004999403 validation, acc: 0.891, loss: 1.648 \n",
            "epoch: 240, acc: 0.887, loss: 1.713 (data_loss: 1.682, reg_loss: 0.032), lr: 0.004999400 validation, acc: 0.879, loss: 1.674 \n",
            "epoch: 241, acc: 0.872, loss: 1.743 (data_loss: 1.712, reg_loss: 0.031), lr: 0.004999398 validation, acc: 0.891, loss: 1.640 \n",
            "epoch: 242, acc: 0.881, loss: 1.707 (data_loss: 1.676, reg_loss: 0.031), lr: 0.004999395 validation, acc: 0.891, loss: 1.628 \n",
            "epoch: 243, acc: 0.787, loss: 2.078 (data_loss: 2.047, reg_loss: 0.031), lr: 0.004999393 validation, acc: 0.773, loss: 1.956 \n",
            "epoch: 244, acc: 0.751, loss: 2.097 (data_loss: 2.067, reg_loss: 0.030), lr: 0.004999390 validation, acc: 0.762, loss: 1.901 \n",
            "epoch: 245, acc: 0.777, loss: 1.967 (data_loss: 1.937, reg_loss: 0.030), lr: 0.004999388 validation, acc: 0.798, loss: 1.853 \n",
            "epoch: 246, acc: 0.765, loss: 2.059 (data_loss: 2.029, reg_loss: 0.030), lr: 0.004999385 validation, acc: 0.736, loss: 1.944 \n",
            "epoch: 247, acc: 0.756, loss: 1.962 (data_loss: 1.933, reg_loss: 0.029), lr: 0.004999383 validation, acc: 0.858, loss: 1.788 \n",
            "epoch: 248, acc: 0.880, loss: 1.788 (data_loss: 1.759, reg_loss: 0.029), lr: 0.004999380 validation, acc: 0.877, loss: 1.731 \n",
            "epoch: 249, acc: 0.863, loss: 1.803 (data_loss: 1.774, reg_loss: 0.029), lr: 0.004999378 validation, acc: 0.859, loss: 1.745 \n",
            "epoch: 250, acc: 0.859, loss: 1.808 (data_loss: 1.779, reg_loss: 0.029), lr: 0.004999375 validation, acc: 0.877, loss: 1.690 \n",
            "epoch: 251, acc: 0.864, loss: 1.761 (data_loss: 1.733, reg_loss: 0.028), lr: 0.004999373 validation, acc: 0.888, loss: 1.650 \n",
            "epoch: 252, acc: 0.884, loss: 1.714 (data_loss: 1.686, reg_loss: 0.028), lr: 0.004999370 validation, acc: 0.882, loss: 1.655 \n",
            "epoch: 253, acc: 0.860, loss: 1.765 (data_loss: 1.737, reg_loss: 0.028), lr: 0.004999368 validation, acc: 0.863, loss: 1.694 \n",
            "epoch: 254, acc: 0.860, loss: 1.769 (data_loss: 1.742, reg_loss: 0.028), lr: 0.004999365 validation, acc: 0.861, loss: 1.681 \n",
            "epoch: 255, acc: 0.862, loss: 1.752 (data_loss: 1.725, reg_loss: 0.028), lr: 0.004999363 validation, acc: 0.887, loss: 1.624 \n",
            "epoch: 256, acc: 0.773, loss: 2.288 (data_loss: 2.260, reg_loss: 0.027), lr: 0.004999360 validation, acc: 0.784, loss: 2.079 \n",
            "epoch: 257, acc: 0.675, loss: 2.830 (data_loss: 2.803, reg_loss: 0.027), lr: 0.004999358 validation, acc: 0.662, loss: 2.549 \n",
            "epoch: 258, acc: 0.568, loss: 3.036 (data_loss: 3.009, reg_loss: 0.027), lr: 0.004999355 validation, acc: 0.612, loss: 2.566 \n",
            "epoch: 259, acc: 0.573, loss: 2.935 (data_loss: 2.907, reg_loss: 0.027), lr: 0.004999353 validation, acc: 0.641, loss: 2.411 \n",
            "epoch: 260, acc: 0.662, loss: 2.527 (data_loss: 2.500, reg_loss: 0.027), lr: 0.004999350 validation, acc: 0.654, loss: 2.331 \n",
            "epoch: 261, acc: 0.640, loss: 2.624 (data_loss: 2.597, reg_loss: 0.027), lr: 0.004999348 validation, acc: 0.622, loss: 2.444 \n",
            "epoch: 262, acc: 0.664, loss: 2.372 (data_loss: 2.344, reg_loss: 0.027), lr: 0.004999345 validation, acc: 0.671, loss: 2.112 \n",
            "epoch: 263, acc: 0.624, loss: 2.372 (data_loss: 2.344, reg_loss: 0.028), lr: 0.004999343 validation, acc: 0.767, loss: 1.970 \n",
            "epoch: 264, acc: 0.774, loss: 2.008 (data_loss: 1.980, reg_loss: 0.028), lr: 0.004999340 validation, acc: 0.806, loss: 1.899 \n",
            "epoch: 265, acc: 0.760, loss: 2.022 (data_loss: 1.994, reg_loss: 0.028), lr: 0.004999338 validation, acc: 0.773, loss: 1.937 \n",
            "epoch: 266, acc: 0.797, loss: 1.956 (data_loss: 1.927, reg_loss: 0.029), lr: 0.004999335 validation, acc: 0.805, loss: 1.844 \n",
            "epoch: 267, acc: 0.830, loss: 1.868 (data_loss: 1.839, reg_loss: 0.029), lr: 0.004999333 validation, acc: 0.877, loss: 1.738 \n",
            "epoch: 268, acc: 0.853, loss: 1.808 (data_loss: 1.779, reg_loss: 0.029), lr: 0.004999330 validation, acc: 0.887, loss: 1.720 \n",
            "epoch: 269, acc: 0.867, loss: 1.827 (data_loss: 1.797, reg_loss: 0.030), lr: 0.004999328 validation, acc: 0.884, loss: 1.655 \n",
            "epoch: 270, acc: 0.876, loss: 1.753 (data_loss: 1.723, reg_loss: 0.030), lr: 0.004999325 validation, acc: 0.886, loss: 1.676 \n",
            "epoch: 271, acc: 0.862, loss: 1.780 (data_loss: 1.750, reg_loss: 0.030), lr: 0.004999323 validation, acc: 0.869, loss: 1.691 \n",
            "epoch: 272, acc: 0.863, loss: 1.772 (data_loss: 1.742, reg_loss: 0.030), lr: 0.004999320 validation, acc: 0.883, loss: 1.646 \n",
            "epoch: 273, acc: 0.855, loss: 1.815 (data_loss: 1.785, reg_loss: 0.030), lr: 0.004999318 validation, acc: 0.881, loss: 1.699 \n",
            "epoch: 274, acc: 0.765, loss: 2.185 (data_loss: 2.154, reg_loss: 0.031), lr: 0.004999315 validation, acc: 0.847, loss: 1.791 \n",
            "epoch: 275, acc: 0.735, loss: 2.112 (data_loss: 2.082, reg_loss: 0.031), lr: 0.004999313 validation, acc: 0.833, loss: 1.848 \n",
            "epoch: 276, acc: 0.657, loss: 2.297 (data_loss: 2.267, reg_loss: 0.031), lr: 0.004999310 validation, acc: 0.789, loss: 1.995 \n",
            "epoch: 277, acc: 0.698, loss: 2.367 (data_loss: 2.337, reg_loss: 0.031), lr: 0.004999308 validation, acc: 0.733, loss: 2.005 \n",
            "epoch: 278, acc: 0.735, loss: 2.101 (data_loss: 2.070, reg_loss: 0.031), lr: 0.004999305 validation, acc: 0.768, loss: 1.862 \n",
            "epoch: 279, acc: 0.787, loss: 1.901 (data_loss: 1.870, reg_loss: 0.031), lr: 0.004999303 validation, acc: 0.851, loss: 1.739 \n",
            "epoch: 280, acc: 0.755, loss: 2.035 (data_loss: 2.003, reg_loss: 0.031), lr: 0.004999300 validation, acc: 0.877, loss: 1.630 \n",
            "epoch: 281, acc: 0.771, loss: 2.078 (data_loss: 2.047, reg_loss: 0.032), lr: 0.004999298 validation, acc: 0.741, loss: 2.009 \n",
            "epoch: 282, acc: 0.668, loss: 2.388 (data_loss: 2.357, reg_loss: 0.032), lr: 0.004999295 validation, acc: 0.766, loss: 2.061 \n",
            "epoch: 283, acc: 0.640, loss: 2.778 (data_loss: 2.746, reg_loss: 0.032), lr: 0.004999293 validation, acc: 0.651, loss: 2.557 \n",
            "epoch: 284, acc: 0.585, loss: 2.924 (data_loss: 2.892, reg_loss: 0.033), lr: 0.004999290 validation, acc: 0.650, loss: 2.435 \n",
            "epoch: 285, acc: 0.533, loss: 3.452 (data_loss: 3.419, reg_loss: 0.033), lr: 0.004999288 validation, acc: 0.626, loss: 2.530 \n",
            "epoch: 286, acc: 0.534, loss: 3.173 (data_loss: 3.140, reg_loss: 0.033), lr: 0.004999285 validation, acc: 0.534, loss: 2.658 \n",
            "epoch: 287, acc: 0.536, loss: 2.900 (data_loss: 2.867, reg_loss: 0.034), lr: 0.004999283 validation, acc: 0.625, loss: 2.470 \n",
            "epoch: 288, acc: 0.580, loss: 3.071 (data_loss: 3.037, reg_loss: 0.034), lr: 0.004999280 validation, acc: 0.634, loss: 2.609 \n",
            "epoch: 289, acc: 0.603, loss: 3.079 (data_loss: 3.045, reg_loss: 0.034), lr: 0.004999278 validation, acc: 0.664, loss: 2.419 \n",
            "epoch: 290, acc: 0.643, loss: 2.780 (data_loss: 2.745, reg_loss: 0.035), lr: 0.004999275 validation, acc: 0.729, loss: 2.130 \n",
            "epoch: 291, acc: 0.692, loss: 2.718 (data_loss: 2.683, reg_loss: 0.035), lr: 0.004999273 validation, acc: 0.741, loss: 2.311 \n",
            "epoch: 292, acc: 0.702, loss: 2.570 (data_loss: 2.534, reg_loss: 0.035), lr: 0.004999270 validation, acc: 0.761, loss: 2.245 \n",
            "epoch: 293, acc: 0.760, loss: 2.385 (data_loss: 2.349, reg_loss: 0.036), lr: 0.004999268 validation, acc: 0.771, loss: 2.122 \n",
            "epoch: 294, acc: 0.771, loss: 2.342 (data_loss: 2.305, reg_loss: 0.036), lr: 0.004999265 validation, acc: 0.774, loss: 1.977 \n",
            "epoch: 295, acc: 0.778, loss: 2.212 (data_loss: 2.175, reg_loss: 0.037), lr: 0.004999263 validation, acc: 0.787, loss: 1.795 \n",
            "epoch: 296, acc: 0.704, loss: 2.796 (data_loss: 2.759, reg_loss: 0.037), lr: 0.004999260 validation, acc: 0.749, loss: 2.270 \n",
            "epoch: 297, acc: 0.759, loss: 2.699 (data_loss: 2.662, reg_loss: 0.037), lr: 0.004999258 validation, acc: 0.719, loss: 2.334 \n",
            "epoch: 298, acc: 0.731, loss: 2.814 (data_loss: 2.776, reg_loss: 0.037), lr: 0.004999255 validation, acc: 0.752, loss: 2.327 \n",
            "epoch: 299, acc: 0.665, loss: 2.950 (data_loss: 2.912, reg_loss: 0.038), lr: 0.004999253 validation, acc: 0.738, loss: 2.171 \n",
            "epoch: 300, acc: 0.747, loss: 2.557 (data_loss: 2.519, reg_loss: 0.038), lr: 0.004999250 validation, acc: 0.763, loss: 2.068 \n",
            "epoch: 301, acc: 0.703, loss: 2.478 (data_loss: 2.440, reg_loss: 0.038), lr: 0.004999248 validation, acc: 0.797, loss: 1.892 \n",
            "epoch: 302, acc: 0.758, loss: 2.176 (data_loss: 2.138, reg_loss: 0.039), lr: 0.004999245 validation, acc: 0.811, loss: 1.758 \n",
            "epoch: 303, acc: 0.817, loss: 1.907 (data_loss: 1.868, reg_loss: 0.039), lr: 0.004999243 validation, acc: 0.858, loss: 1.655 \n",
            "epoch: 304, acc: 0.852, loss: 1.832 (data_loss: 1.792, reg_loss: 0.039), lr: 0.004999240 validation, acc: 0.866, loss: 1.640 \n",
            "epoch: 305, acc: 0.860, loss: 1.792 (data_loss: 1.752, reg_loss: 0.040), lr: 0.004999238 validation, acc: 0.873, loss: 1.646 \n",
            "epoch: 306, acc: 0.857, loss: 1.786 (data_loss: 1.746, reg_loss: 0.040), lr: 0.004999235 validation, acc: 0.875, loss: 1.636 \n",
            "epoch: 307, acc: 0.872, loss: 1.738 (data_loss: 1.698, reg_loss: 0.040), lr: 0.004999233 validation, acc: 0.881, loss: 1.628 \n",
            "epoch: 308, acc: 0.833, loss: 1.861 (data_loss: 1.821, reg_loss: 0.040), lr: 0.004999230 validation, acc: 0.864, loss: 1.673 \n",
            "epoch: 309, acc: 0.765, loss: 2.023 (data_loss: 1.983, reg_loss: 0.040), lr: 0.004999228 validation, acc: 0.842, loss: 1.760 \n",
            "epoch: 310, acc: 0.838, loss: 1.840 (data_loss: 1.799, reg_loss: 0.041), lr: 0.004999225 validation, acc: 0.861, loss: 1.721 \n",
            "epoch: 311, acc: 0.705, loss: 2.492 (data_loss: 2.452, reg_loss: 0.041), lr: 0.004999223 validation, acc: 0.756, loss: 2.157 \n",
            "epoch: 312, acc: 0.868, loss: 1.754 (data_loss: 1.713, reg_loss: 0.041), lr: 0.004999220 validation, acc: 0.868, loss: 1.648 \n",
            "epoch: 313, acc: 0.763, loss: 2.354 (data_loss: 2.313, reg_loss: 0.041), lr: 0.004999218 validation, acc: 0.764, loss: 1.974 \n",
            "epoch: 314, acc: 0.773, loss: 2.112 (data_loss: 2.070, reg_loss: 0.041), lr: 0.004999215 validation, acc: 0.876, loss: 1.660 \n",
            "epoch: 315, acc: 0.868, loss: 1.765 (data_loss: 1.724, reg_loss: 0.041), lr: 0.004999213 validation, acc: 0.886, loss: 1.616 \n",
            "epoch: 316, acc: 0.881, loss: 1.706 (data_loss: 1.665, reg_loss: 0.041), lr: 0.004999210 validation, acc: 0.886, loss: 1.615 \n",
            "epoch: 317, acc: 0.881, loss: 1.714 (data_loss: 1.673, reg_loss: 0.041), lr: 0.004999208 validation, acc: 0.880, loss: 1.639 \n",
            "epoch: 318, acc: 0.879, loss: 1.735 (data_loss: 1.694, reg_loss: 0.041), lr: 0.004999205 validation, acc: 0.877, loss: 1.650 \n",
            "epoch: 319, acc: 0.862, loss: 1.844 (data_loss: 1.802, reg_loss: 0.041), lr: 0.004999203 validation, acc: 0.879, loss: 1.666 \n",
            "epoch: 320, acc: 0.872, loss: 1.795 (data_loss: 1.754, reg_loss: 0.041), lr: 0.004999200 validation, acc: 0.890, loss: 1.610 \n",
            "epoch: 321, acc: 0.781, loss: 2.160 (data_loss: 2.119, reg_loss: 0.041), lr: 0.004999198 validation, acc: 0.882, loss: 1.615 \n",
            "epoch: 322, acc: 0.877, loss: 1.732 (data_loss: 1.692, reg_loss: 0.040), lr: 0.004999195 validation, acc: 0.882, loss: 1.609 \n",
            "epoch: 323, acc: 0.870, loss: 1.729 (data_loss: 1.689, reg_loss: 0.040), lr: 0.004999193 validation, acc: 0.896, loss: 1.587 \n",
            "epoch: 324, acc: 0.889, loss: 1.701 (data_loss: 1.661, reg_loss: 0.040), lr: 0.004999190 validation, acc: 0.889, loss: 1.598 \n",
            "epoch: 325, acc: 0.861, loss: 1.757 (data_loss: 1.717, reg_loss: 0.039), lr: 0.004999188 validation, acc: 0.876, loss: 1.631 \n",
            "epoch: 326, acc: 0.880, loss: 1.730 (data_loss: 1.690, reg_loss: 0.039), lr: 0.004999185 validation, acc: 0.867, loss: 1.664 \n",
            "epoch: 327, acc: 0.871, loss: 1.757 (data_loss: 1.718, reg_loss: 0.039), lr: 0.004999183 validation, acc: 0.884, loss: 1.614 \n",
            "epoch: 328, acc: 0.868, loss: 1.775 (data_loss: 1.736, reg_loss: 0.039), lr: 0.004999180 validation, acc: 0.891, loss: 1.599 \n",
            "epoch: 329, acc: 0.886, loss: 1.704 (data_loss: 1.666, reg_loss: 0.039), lr: 0.004999178 validation, acc: 0.896, loss: 1.584 \n",
            "epoch: 330, acc: 0.890, loss: 1.685 (data_loss: 1.647, reg_loss: 0.038), lr: 0.004999175 validation, acc: 0.890, loss: 1.597 \n",
            "epoch: 331, acc: 0.870, loss: 1.742 (data_loss: 1.704, reg_loss: 0.038), lr: 0.004999173 validation, acc: 0.891, loss: 1.612 \n",
            "epoch: 332, acc: 0.799, loss: 1.873 (data_loss: 1.836, reg_loss: 0.037), lr: 0.004999170 validation, acc: 0.899, loss: 1.587 \n",
            "epoch: 333, acc: 0.892, loss: 1.685 (data_loss: 1.648, reg_loss: 0.037), lr: 0.004999168 validation, acc: 0.896, loss: 1.585 \n",
            "epoch: 334, acc: 0.879, loss: 1.732 (data_loss: 1.695, reg_loss: 0.037), lr: 0.004999165 validation, acc: 0.886, loss: 1.619 \n",
            "epoch: 335, acc: 0.884, loss: 1.716 (data_loss: 1.679, reg_loss: 0.037), lr: 0.004999163 validation, acc: 0.891, loss: 1.603 \n",
            "epoch: 336, acc: 0.785, loss: 2.062 (data_loss: 2.026, reg_loss: 0.036), lr: 0.004999160 validation, acc: 0.887, loss: 1.627 \n",
            "epoch: 337, acc: 0.852, loss: 1.769 (data_loss: 1.734, reg_loss: 0.036), lr: 0.004999158 validation, acc: 0.881, loss: 1.634 \n",
            "epoch: 338, acc: 0.783, loss: 2.620 (data_loss: 2.584, reg_loss: 0.036), lr: 0.004999155 validation, acc: 0.796, loss: 2.404 \n",
            "epoch: 339, acc: 0.679, loss: 3.411 (data_loss: 3.375, reg_loss: 0.035), lr: 0.004999153 validation, acc: 0.702, loss: 2.831 \n",
            "epoch: 340, acc: 0.677, loss: 3.450 (data_loss: 3.415, reg_loss: 0.035), lr: 0.004999150 validation, acc: 0.773, loss: 2.790 \n",
            "epoch: 341, acc: 0.751, loss: 3.147 (data_loss: 3.112, reg_loss: 0.035), lr: 0.004999148 validation, acc: 0.787, loss: 2.725 \n",
            "epoch: 342, acc: 0.765, loss: 3.070 (data_loss: 3.035, reg_loss: 0.035), lr: 0.004999145 validation, acc: 0.789, loss: 2.649 \n",
            "epoch: 343, acc: 0.763, loss: 3.081 (data_loss: 3.046, reg_loss: 0.035), lr: 0.004999143 validation, acc: 0.789, loss: 2.687 \n",
            "epoch: 344, acc: 0.713, loss: 3.193 (data_loss: 3.158, reg_loss: 0.035), lr: 0.004999140 validation, acc: 0.774, loss: 2.700 \n",
            "epoch: 345, acc: 0.754, loss: 3.083 (data_loss: 3.048, reg_loss: 0.035), lr: 0.004999138 validation, acc: 0.752, loss: 2.742 \n",
            "epoch: 346, acc: 0.730, loss: 3.170 (data_loss: 3.135, reg_loss: 0.035), lr: 0.004999135 validation, acc: 0.790, loss: 2.627 \n",
            "epoch: 347, acc: 0.774, loss: 2.948 (data_loss: 2.913, reg_loss: 0.035), lr: 0.004999133 validation, acc: 0.779, loss: 2.565 \n",
            "epoch: 348, acc: 0.760, loss: 2.860 (data_loss: 2.825, reg_loss: 0.034), lr: 0.004999130 validation, acc: 0.786, loss: 2.484 \n",
            "epoch: 349, acc: 0.756, loss: 2.808 (data_loss: 2.774, reg_loss: 0.034), lr: 0.004999128 validation, acc: 0.791, loss: 2.441 \n",
            "epoch: 350, acc: 0.780, loss: 2.797 (data_loss: 2.763, reg_loss: 0.034), lr: 0.004999125 validation, acc: 0.787, loss: 2.438 \n",
            "epoch: 351, acc: 0.771, loss: 2.672 (data_loss: 2.638, reg_loss: 0.034), lr: 0.004999123 validation, acc: 0.792, loss: 2.282 \n",
            "epoch: 352, acc: 0.678, loss: 3.430 (data_loss: 3.396, reg_loss: 0.034), lr: 0.004999120 validation, acc: 0.739, loss: 2.647 \n",
            "epoch: 353, acc: 0.737, loss: 2.885 (data_loss: 2.852, reg_loss: 0.033), lr: 0.004999118 validation, acc: 0.811, loss: 2.331 \n",
            "epoch: 354, acc: 0.771, loss: 2.518 (data_loss: 2.485, reg_loss: 0.033), lr: 0.004999115 validation, acc: 0.793, loss: 2.167 \n",
            "epoch: 355, acc: 0.784, loss: 2.314 (data_loss: 2.281, reg_loss: 0.034), lr: 0.004999113 validation, acc: 0.817, loss: 2.010 \n",
            "epoch: 356, acc: 0.799, loss: 2.199 (data_loss: 2.166, reg_loss: 0.034), lr: 0.004999110 validation, acc: 0.831, loss: 1.961 \n",
            "epoch: 357, acc: 0.776, loss: 2.064 (data_loss: 2.030, reg_loss: 0.034), lr: 0.004999108 validation, acc: 0.820, loss: 1.863 \n",
            "epoch: 358, acc: 0.752, loss: 2.264 (data_loss: 2.231, reg_loss: 0.034), lr: 0.004999105 validation, acc: 0.827, loss: 1.862 \n",
            "epoch: 359, acc: 0.735, loss: 2.249 (data_loss: 2.215, reg_loss: 0.034), lr: 0.004999103 validation, acc: 0.819, loss: 1.874 \n",
            "epoch: 360, acc: 0.812, loss: 1.995 (data_loss: 1.961, reg_loss: 0.034), lr: 0.004999100 validation, acc: 0.853, loss: 1.766 \n",
            "epoch: 361, acc: 0.844, loss: 1.813 (data_loss: 1.779, reg_loss: 0.034), lr: 0.004999098 validation, acc: 0.881, loss: 1.633 \n",
            "epoch: 362, acc: 0.874, loss: 1.751 (data_loss: 1.717, reg_loss: 0.034), lr: 0.004999095 validation, acc: 0.892, loss: 1.617 \n",
            "epoch: 363, acc: 0.797, loss: 2.325 (data_loss: 2.291, reg_loss: 0.034), lr: 0.004999093 validation, acc: 0.834, loss: 1.883 \n",
            "epoch: 364, acc: 0.726, loss: 2.401 (data_loss: 2.367, reg_loss: 0.035), lr: 0.004999090 validation, acc: 0.809, loss: 1.980 \n",
            "epoch: 365, acc: 0.793, loss: 2.196 (data_loss: 2.161, reg_loss: 0.035), lr: 0.004999088 validation, acc: 0.826, loss: 1.855 \n",
            "epoch: 366, acc: 0.802, loss: 2.109 (data_loss: 2.073, reg_loss: 0.035), lr: 0.004999085 validation, acc: 0.852, loss: 1.746 \n",
            "epoch: 367, acc: 0.748, loss: 2.472 (data_loss: 2.437, reg_loss: 0.036), lr: 0.004999083 validation, acc: 0.781, loss: 2.268 \n",
            "epoch: 368, acc: 0.780, loss: 2.528 (data_loss: 2.492, reg_loss: 0.036), lr: 0.004999080 validation, acc: 0.808, loss: 1.993 \n",
            "epoch: 369, acc: 0.808, loss: 2.153 (data_loss: 2.117, reg_loss: 0.036), lr: 0.004999078 validation, acc: 0.861, loss: 1.718 \n",
            "epoch: 370, acc: 0.855, loss: 1.808 (data_loss: 1.771, reg_loss: 0.037), lr: 0.004999075 validation, acc: 0.854, loss: 1.723 \n",
            "epoch: 371, acc: 0.726, loss: 2.214 (data_loss: 2.177, reg_loss: 0.037), lr: 0.004999073 validation, acc: 0.834, loss: 1.797 \n",
            "epoch: 372, acc: 0.761, loss: 2.003 (data_loss: 1.965, reg_loss: 0.038), lr: 0.004999070 validation, acc: 0.783, loss: 1.852 \n",
            "epoch: 373, acc: 0.689, loss: 2.760 (data_loss: 2.722, reg_loss: 0.038), lr: 0.004999068 validation, acc: 0.713, loss: 2.323 \n",
            "epoch: 374, acc: 0.734, loss: 2.343 (data_loss: 2.305, reg_loss: 0.038), lr: 0.004999065 validation, acc: 0.768, loss: 2.048 \n",
            "epoch: 375, acc: 0.786, loss: 2.229 (data_loss: 2.190, reg_loss: 0.039), lr: 0.004999063 validation, acc: 0.760, loss: 1.992 \n",
            "epoch: 376, acc: 0.777, loss: 2.075 (data_loss: 2.036, reg_loss: 0.039), lr: 0.004999060 validation, acc: 0.864, loss: 1.711 \n",
            "epoch: 377, acc: 0.858, loss: 1.790 (data_loss: 1.751, reg_loss: 0.040), lr: 0.004999058 validation, acc: 0.882, loss: 1.633 \n",
            "epoch: 378, acc: 0.882, loss: 1.712 (data_loss: 1.672, reg_loss: 0.040), lr: 0.004999055 validation, acc: 0.873, loss: 1.648 \n",
            "epoch: 379, acc: 0.872, loss: 1.732 (data_loss: 1.692, reg_loss: 0.040), lr: 0.004999053 validation, acc: 0.878, loss: 1.638 \n",
            "epoch: 380, acc: 0.878, loss: 1.719 (data_loss: 1.679, reg_loss: 0.041), lr: 0.004999050 validation, acc: 0.879, loss: 1.642 \n",
            "epoch: 381, acc: 0.874, loss: 1.732 (data_loss: 1.692, reg_loss: 0.041), lr: 0.004999048 validation, acc: 0.882, loss: 1.635 \n",
            "epoch: 382, acc: 0.880, loss: 1.713 (data_loss: 1.672, reg_loss: 0.041), lr: 0.004999045 validation, acc: 0.887, loss: 1.618 \n",
            "epoch: 383, acc: 0.876, loss: 1.722 (data_loss: 1.681, reg_loss: 0.041), lr: 0.004999043 validation, acc: 0.884, loss: 1.627 \n",
            "epoch: 384, acc: 0.884, loss: 1.706 (data_loss: 1.665, reg_loss: 0.041), lr: 0.004999040 validation, acc: 0.889, loss: 1.613 \n",
            "epoch: 385, acc: 0.884, loss: 1.705 (data_loss: 1.664, reg_loss: 0.041), lr: 0.004999038 validation, acc: 0.892, loss: 1.610 \n",
            "epoch: 386, acc: 0.871, loss: 1.730 (data_loss: 1.690, reg_loss: 0.040), lr: 0.004999035 validation, acc: 0.894, loss: 1.606 \n",
            "epoch: 387, acc: 0.883, loss: 1.705 (data_loss: 1.665, reg_loss: 0.040), lr: 0.004999033 validation, acc: 0.892, loss: 1.606 \n",
            "epoch: 388, acc: 0.891, loss: 1.686 (data_loss: 1.646, reg_loss: 0.040), lr: 0.004999030 validation, acc: 0.894, loss: 1.597 \n",
            "epoch: 389, acc: 0.887, loss: 1.698 (data_loss: 1.658, reg_loss: 0.040), lr: 0.004999028 validation, acc: 0.895, loss: 1.600 \n",
            "epoch: 390, acc: 0.892, loss: 1.684 (data_loss: 1.644, reg_loss: 0.040), lr: 0.004999025 validation, acc: 0.899, loss: 1.591 \n",
            "epoch: 391, acc: 0.894, loss: 1.673 (data_loss: 1.634, reg_loss: 0.039), lr: 0.004999023 validation, acc: 0.899, loss: 1.587 \n",
            "epoch: 392, acc: 0.888, loss: 1.689 (data_loss: 1.650, reg_loss: 0.039), lr: 0.004999020 validation, acc: 0.891, loss: 1.602 \n",
            "epoch: 393, acc: 0.889, loss: 1.684 (data_loss: 1.646, reg_loss: 0.038), lr: 0.004999018 validation, acc: 0.886, loss: 1.626 \n",
            "epoch: 394, acc: 0.878, loss: 1.712 (data_loss: 1.674, reg_loss: 0.038), lr: 0.004999015 validation, acc: 0.894, loss: 1.597 \n",
            "epoch: 395, acc: 0.890, loss: 1.682 (data_loss: 1.645, reg_loss: 0.037), lr: 0.004999013 validation, acc: 0.896, loss: 1.601 \n",
            "epoch: 396, acc: 0.795, loss: 1.875 (data_loss: 1.838, reg_loss: 0.037), lr: 0.004999010 validation, acc: 0.891, loss: 1.624 \n",
            "epoch: 397, acc: 0.877, loss: 1.712 (data_loss: 1.675, reg_loss: 0.036), lr: 0.004999008 validation, acc: 0.873, loss: 1.659 \n",
            "epoch: 398, acc: 0.837, loss: 1.847 (data_loss: 1.811, reg_loss: 0.036), lr: 0.004999005 validation, acc: 0.865, loss: 1.637 \n",
            "epoch: 399, acc: 0.864, loss: 1.759 (data_loss: 1.723, reg_loss: 0.035), lr: 0.004999003 validation, acc: 0.879, loss: 1.464 \n",
            "epoch: 400, acc: 0.865, loss: 1.738 (data_loss: 1.703, reg_loss: 0.035), lr: 0.004999000 validation, acc: 0.884, loss: 1.435 \n",
            "epoch: 401, acc: 0.885, loss: 1.681 (data_loss: 1.647, reg_loss: 0.034), lr: 0.004998998 validation, acc: 0.895, loss: 1.431 \n",
            "epoch: 402, acc: 0.877, loss: 1.700 (data_loss: 1.666, reg_loss: 0.034), lr: 0.004998995 validation, acc: 0.897, loss: 1.233 \n",
            "epoch: 403, acc: 0.891, loss: 1.670 (data_loss: 1.636, reg_loss: 0.034), lr: 0.004998993 validation, acc: 0.896, loss: 1.303 \n",
            "epoch: 404, acc: 0.894, loss: 1.584 (data_loss: 1.551, reg_loss: 0.033), lr: 0.004998990 validation, acc: 0.894, loss: 1.099 \n",
            "epoch: 405, acc: 0.892, loss: 1.433 (data_loss: 1.399, reg_loss: 0.033), lr: 0.004998988 validation, acc: 0.894, loss: 0.999 \n",
            "epoch: 406, acc: 0.889, loss: 1.302 (data_loss: 1.269, reg_loss: 0.033), lr: 0.004998985 validation, acc: 0.896, loss: 0.953 \n",
            "epoch: 407, acc: 0.823, loss: 1.379 (data_loss: 1.346, reg_loss: 0.033), lr: 0.004998983 validation, acc: 0.894, loss: 0.873 \n",
            "epoch: 408, acc: 0.788, loss: 1.593 (data_loss: 1.560, reg_loss: 0.033), lr: 0.004998980 validation, acc: 0.841, loss: 1.008 \n",
            "epoch: 409, acc: 0.715, loss: 1.764 (data_loss: 1.732, reg_loss: 0.032), lr: 0.004998978 validation, acc: 0.747, loss: 1.194 \n",
            "epoch: 410, acc: 0.708, loss: 1.663 (data_loss: 1.631, reg_loss: 0.032), lr: 0.004998975 validation, acc: 0.757, loss: 1.186 \n",
            "epoch: 411, acc: 0.642, loss: 2.015 (data_loss: 1.982, reg_loss: 0.032), lr: 0.004998973 validation, acc: 0.724, loss: 1.221 \n",
            "epoch: 412, acc: 0.610, loss: 2.757 (data_loss: 2.724, reg_loss: 0.032), lr: 0.004998970 validation, acc: 0.710, loss: 2.010 \n",
            "epoch: 413, acc: 0.666, loss: 2.848 (data_loss: 2.815, reg_loss: 0.033), lr: 0.004998968 validation, acc: 0.699, loss: 2.301 \n",
            "epoch: 414, acc: 0.684, loss: 2.820 (data_loss: 2.786, reg_loss: 0.033), lr: 0.004998965 validation, acc: 0.686, loss: 2.466 \n",
            "epoch: 415, acc: 0.673, loss: 2.950 (data_loss: 2.916, reg_loss: 0.034), lr: 0.004998963 validation, acc: 0.761, loss: 2.150 \n",
            "epoch: 416, acc: 0.669, loss: 2.740 (data_loss: 2.706, reg_loss: 0.034), lr: 0.004998960 validation, acc: 0.744, loss: 2.063 \n",
            "epoch: 417, acc: 0.726, loss: 2.522 (data_loss: 2.487, reg_loss: 0.035), lr: 0.004998958 validation, acc: 0.744, loss: 2.019 \n",
            "epoch: 418, acc: 0.732, loss: 2.431 (data_loss: 2.395, reg_loss: 0.035), lr: 0.004998955 validation, acc: 0.791, loss: 1.798 \n",
            "epoch: 419, acc: 0.668, loss: 2.818 (data_loss: 2.782, reg_loss: 0.036), lr: 0.004998953 validation, acc: 0.672, loss: 2.256 \n",
            "epoch: 420, acc: 0.667, loss: 2.789 (data_loss: 2.752, reg_loss: 0.036), lr: 0.004998950 validation, acc: 0.696, loss: 2.178 \n",
            "epoch: 421, acc: 0.670, loss: 2.708 (data_loss: 2.671, reg_loss: 0.037), lr: 0.004998948 validation, acc: 0.682, loss: 2.185 \n",
            "epoch: 422, acc: 0.679, loss: 2.496 (data_loss: 2.459, reg_loss: 0.037), lr: 0.004998945 validation, acc: 0.724, loss: 2.007 \n",
            "epoch: 423, acc: 0.678, loss: 2.306 (data_loss: 2.269, reg_loss: 0.038), lr: 0.004998943 validation, acc: 0.744, loss: 1.915 \n",
            "epoch: 424, acc: 0.695, loss: 2.274 (data_loss: 2.236, reg_loss: 0.038), lr: 0.004998940 validation, acc: 0.792, loss: 1.774 \n",
            "epoch: 425, acc: 0.674, loss: 2.692 (data_loss: 2.654, reg_loss: 0.038), lr: 0.004998938 validation, acc: 0.765, loss: 2.192 \n",
            "epoch: 426, acc: 0.614, loss: 2.904 (data_loss: 2.865, reg_loss: 0.039), lr: 0.004998935 validation, acc: 0.744, loss: 2.134 \n",
            "epoch: 427, acc: 0.653, loss: 2.789 (data_loss: 2.750, reg_loss: 0.039), lr: 0.004998933 validation, acc: 0.659, loss: 2.071 \n",
            "epoch: 428, acc: 0.559, loss: 2.622 (data_loss: 2.583, reg_loss: 0.039), lr: 0.004998930 validation, acc: 0.658, loss: 1.781 \n",
            "epoch: 429, acc: 0.674, loss: 2.405 (data_loss: 2.366, reg_loss: 0.039), lr: 0.004998928 validation, acc: 0.706, loss: 1.783 \n",
            "epoch: 430, acc: 0.579, loss: 2.513 (data_loss: 2.473, reg_loss: 0.040), lr: 0.004998925 validation, acc: 0.639, loss: 1.944 \n",
            "epoch: 431, acc: 0.570, loss: 2.409 (data_loss: 2.369, reg_loss: 0.040), lr: 0.004998923 validation, acc: 0.620, loss: 1.953 \n",
            "epoch: 432, acc: 0.632, loss: 2.343 (data_loss: 2.302, reg_loss: 0.041), lr: 0.004998920 validation, acc: 0.616, loss: 2.048 \n",
            "epoch: 433, acc: 0.621, loss: 2.422 (data_loss: 2.381, reg_loss: 0.041), lr: 0.004998918 validation, acc: 0.723, loss: 1.810 \n",
            "epoch: 434, acc: 0.778, loss: 2.008 (data_loss: 1.966, reg_loss: 0.042), lr: 0.004998915 validation, acc: 0.809, loss: 1.557 \n",
            "epoch: 435, acc: 0.805, loss: 1.904 (data_loss: 1.861, reg_loss: 0.042), lr: 0.004998913 validation, acc: 0.841, loss: 1.523 \n",
            "epoch: 436, acc: 0.870, loss: 1.735 (data_loss: 1.692, reg_loss: 0.043), lr: 0.004998910 validation, acc: 0.873, loss: 1.499 \n",
            "epoch: 437, acc: 0.871, loss: 1.686 (data_loss: 1.643, reg_loss: 0.043), lr: 0.004998908 validation, acc: 0.884, loss: 1.404 \n",
            "epoch: 438, acc: 0.882, loss: 1.687 (data_loss: 1.644, reg_loss: 0.044), lr: 0.004998905 validation, acc: 0.889, loss: 1.408 \n",
            "epoch: 439, acc: 0.887, loss: 1.681 (data_loss: 1.637, reg_loss: 0.044), lr: 0.004998903 validation, acc: 0.894, loss: 1.403 \n",
            "epoch: 440, acc: 0.872, loss: 1.739 (data_loss: 1.695, reg_loss: 0.044), lr: 0.004998900 validation, acc: 0.891, loss: 1.436 \n",
            "epoch: 441, acc: 0.888, loss: 1.690 (data_loss: 1.646, reg_loss: 0.045), lr: 0.004998898 validation, acc: 0.893, loss: 1.427 \n",
            "epoch: 442, acc: 0.806, loss: 1.902 (data_loss: 1.857, reg_loss: 0.045), lr: 0.004998895 validation, acc: 0.894, loss: 1.555 \n",
            "epoch: 443, acc: 0.849, loss: 1.794 (data_loss: 1.749, reg_loss: 0.045), lr: 0.004998893 validation, acc: 0.889, loss: 1.602 \n",
            "epoch: 444, acc: 0.884, loss: 1.707 (data_loss: 1.662, reg_loss: 0.045), lr: 0.004998890 validation, acc: 0.896, loss: 1.601 \n",
            "epoch: 445, acc: 0.787, loss: 2.198 (data_loss: 2.153, reg_loss: 0.045), lr: 0.004998888 validation, acc: 0.799, loss: 2.001 \n",
            "epoch: 446, acc: 0.730, loss: 2.802 (data_loss: 2.757, reg_loss: 0.044), lr: 0.004998885 validation, acc: 0.746, loss: 2.318 \n",
            "epoch: 447, acc: 0.743, loss: 2.636 (data_loss: 2.592, reg_loss: 0.044), lr: 0.004998883 validation, acc: 0.791, loss: 2.154 \n",
            "epoch: 448, acc: 0.776, loss: 2.443 (data_loss: 2.399, reg_loss: 0.044), lr: 0.004998880 validation, acc: 0.787, loss: 2.171 \n",
            "epoch: 449, acc: 0.778, loss: 2.446 (data_loss: 2.402, reg_loss: 0.044), lr: 0.004998878 validation, acc: 0.800, loss: 2.157 \n",
            "epoch: 450, acc: 0.787, loss: 2.430 (data_loss: 2.387, reg_loss: 0.043), lr: 0.004998875 validation, acc: 0.806, loss: 2.127 \n",
            "epoch: 451, acc: 0.791, loss: 2.401 (data_loss: 2.358, reg_loss: 0.043), lr: 0.004998873 validation, acc: 0.806, loss: 2.125 \n",
            "epoch: 452, acc: 0.792, loss: 2.562 (data_loss: 2.519, reg_loss: 0.043), lr: 0.004998870 validation, acc: 0.806, loss: 2.133 \n",
            "epoch: 453, acc: 0.792, loss: 2.453 (data_loss: 2.410, reg_loss: 0.043), lr: 0.004998868 validation, acc: 0.804, loss: 2.100 \n",
            "epoch: 454, acc: 0.792, loss: 2.383 (data_loss: 2.341, reg_loss: 0.043), lr: 0.004998865 validation, acc: 0.806, loss: 2.049 \n",
            "epoch: 455, acc: 0.793, loss: 2.337 (data_loss: 2.294, reg_loss: 0.042), lr: 0.004998863 validation, acc: 0.808, loss: 2.021 \n",
            "epoch: 456, acc: 0.612, loss: 3.789 (data_loss: 3.747, reg_loss: 0.042), lr: 0.004998860 validation, acc: 0.664, loss: 2.933 \n",
            "epoch: 457, acc: 0.648, loss: 3.509 (data_loss: 3.468, reg_loss: 0.041), lr: 0.004998858 validation, acc: 0.724, loss: 2.473 \n",
            "epoch: 458, acc: 0.705, loss: 3.055 (data_loss: 3.014, reg_loss: 0.041), lr: 0.004998855 validation, acc: 0.758, loss: 2.044 \n",
            "epoch: 459, acc: 0.741, loss: 2.465 (data_loss: 2.424, reg_loss: 0.041), lr: 0.004998853 validation, acc: 0.800, loss: 1.650 \n",
            "epoch: 460, acc: 0.789, loss: 2.101 (data_loss: 2.061, reg_loss: 0.040), lr: 0.004998850 validation, acc: 0.811, loss: 1.563 \n",
            "epoch: 461, acc: 0.694, loss: 3.189 (data_loss: 3.149, reg_loss: 0.040), lr: 0.004998848 validation, acc: 0.718, loss: 2.091 \n",
            "epoch: 462, acc: 0.693, loss: 2.730 (data_loss: 2.690, reg_loss: 0.040), lr: 0.004998845 validation, acc: 0.722, loss: 1.799 \n",
            "epoch: 463, acc: 0.694, loss: 2.566 (data_loss: 2.526, reg_loss: 0.040), lr: 0.004998843 validation, acc: 0.774, loss: 1.663 \n",
            "epoch: 464, acc: 0.746, loss: 1.609 (data_loss: 1.569, reg_loss: 0.040), lr: 0.004998840 validation, acc: 0.811, loss: 1.018 \n",
            "epoch: 465, acc: 0.797, loss: 1.295 (data_loss: 1.255, reg_loss: 0.040), lr: 0.004998838 validation, acc: 0.808, loss: 0.829 \n",
            "epoch: 466, acc: 0.804, loss: 1.020 (data_loss: 0.980, reg_loss: 0.039), lr: 0.004998835 validation, acc: 0.838, loss: 0.609 \n",
            "epoch: 467, acc: 0.833, loss: 0.750 (data_loss: 0.710, reg_loss: 0.039), lr: 0.004998833 validation, acc: 0.867, loss: 0.437 \n",
            "epoch: 468, acc: 0.854, loss: 0.583 (data_loss: 0.543, reg_loss: 0.039), lr: 0.004998830 validation, acc: 0.892, loss: 0.392 \n",
            "epoch: 469, acc: 0.853, loss: 0.581 (data_loss: 0.542, reg_loss: 0.039), lr: 0.004998828 validation, acc: 0.901, loss: 0.241 \n",
            "epoch: 470, acc: 0.896, loss: 0.268 (data_loss: 0.229, reg_loss: 0.039), lr: 0.004998825 validation, acc: 0.981, loss: 0.184 \n",
            "epoch: 471, acc: 0.983, loss: 0.188 (data_loss: 0.149, reg_loss: 0.039), lr: 0.004998823 validation, acc: 0.993, loss: 0.094 \n",
            "epoch: 472, acc: 0.994, loss: 0.098 (data_loss: 0.060, reg_loss: 0.038), lr: 0.004998820 validation, acc: 0.997, loss: 0.043 \n",
            "epoch: 473, acc: 0.980, loss: 0.112 (data_loss: 0.074, reg_loss: 0.038), lr: 0.004998818 validation, acc: 0.996, loss: 0.047 \n",
            "epoch: 474, acc: 0.997, loss: 0.067 (data_loss: 0.029, reg_loss: 0.038), lr: 0.004998815 validation, acc: 0.996, loss: 0.039 \n",
            "epoch: 475, acc: 0.992, loss: 0.097 (data_loss: 0.060, reg_loss: 0.037), lr: 0.004998813 validation, acc: 0.991, loss: 0.071 \n",
            "epoch: 476, acc: 0.994, loss: 0.086 (data_loss: 0.049, reg_loss: 0.037), lr: 0.004998810 validation, acc: 0.993, loss: 0.056 \n",
            "epoch: 477, acc: 0.991, loss: 0.084 (data_loss: 0.048, reg_loss: 0.036), lr: 0.004998808 validation, acc: 0.993, loss: 0.049 \n",
            "epoch: 478, acc: 0.995, loss: 0.067 (data_loss: 0.032, reg_loss: 0.036), lr: 0.004998805 validation, acc: 0.993, loss: 0.038 \n",
            "epoch: 479, acc: 0.996, loss: 0.062 (data_loss: 0.027, reg_loss: 0.035), lr: 0.004998803 validation, acc: 0.994, loss: 0.036 \n",
            "epoch: 480, acc: 0.997, loss: 0.056 (data_loss: 0.021, reg_loss: 0.035), lr: 0.004998800 validation, acc: 0.997, loss: 0.030 \n",
            "epoch: 481, acc: 0.997, loss: 0.052 (data_loss: 0.017, reg_loss: 0.034), lr: 0.004998798 validation, acc: 0.998, loss: 0.026 \n",
            "epoch: 482, acc: 0.998, loss: 0.049 (data_loss: 0.015, reg_loss: 0.034), lr: 0.004998795 validation, acc: 0.999, loss: 0.024 \n",
            "epoch: 483, acc: 0.960, loss: 0.170 (data_loss: 0.136, reg_loss: 0.033), lr: 0.004998793 validation, acc: 0.990, loss: 0.082 \n",
            "epoch: 484, acc: 0.952, loss: 0.180 (data_loss: 0.148, reg_loss: 0.033), lr: 0.004998790 validation, acc: 0.998, loss: 0.055 \n",
            "epoch: 485, acc: 0.997, loss: 0.071 (data_loss: 0.038, reg_loss: 0.032), lr: 0.004998788 validation, acc: 1.000, loss: 0.028 \n",
            "epoch: 486, acc: 0.998, loss: 0.061 (data_loss: 0.029, reg_loss: 0.032), lr: 0.004998785 validation, acc: 0.999, loss: 0.032 \n",
            "epoch: 487, acc: 0.999, loss: 0.048 (data_loss: 0.017, reg_loss: 0.031), lr: 0.004998783 validation, acc: 1.000, loss: 0.024 \n",
            "epoch: 488, acc: 0.999, loss: 0.045 (data_loss: 0.014, reg_loss: 0.031), lr: 0.004998780 validation, acc: 1.000, loss: 0.020 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNnDUP_U8Xn",
        "outputId": "96f1abc8-b3e9-4e4b-9069-9ca7004fd74d"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7862929447852761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbXMisqQKqF",
        "outputId": "f8bbd75e-1040-4c28-d5fd-6a09a26b38cc"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 22 epochs\n",
            "Model hit 85% validation accuracy in 22 epochs\n",
            "Model hit 90% validation accuracy in 36 epochs\n",
            "Model hit 95% validation accuracy in 46 epochs\n",
            "Model hit 97.5% validation accuracy in 67 epochs\n",
            "Model hit 100% validation accuracy in 485 epochs\n",
            "Max accuracy was 100.0% at epoch 485.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwSXsZVU8Xo",
        "outputId": "b2d6ad26-accd-4bb8-c7c0-3bede4576371"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "index = 27\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {fashion_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {fashion_mnist_labels[y_test[index]]}')\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.655% Confident True is T-shirt/top. True is actually T-shirt/top\n",
            "Accuracy: 0.999, loss: 0.023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k0Ve2M0bPG3"
      },
      "source": [
        "training_diff = []\n",
        "testing_diff = []\n",
        "combined_diff = []"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByL_RwvlIx3"
      },
      "source": [
        "Individual Training Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTOnqnDXa0ME",
        "outputId": "0767d1ba-df85-45eb-b54d-bf9b24bd35a1"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x, sorted_y)):\n",
        "  accuracy = Accuracy_Categorical()\n",
        "\n",
        "  y = sorted_y[y_sorted_lists]\n",
        "  X = sorted_x[X_sorted_lists]\n",
        "  accuracy.init(y)\n",
        "\n",
        "  dense1.forward(X)\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "  train_train_mean = activation1.output\n",
        "\n",
        "  dropout1.infrence(activation1.output,y)\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, y)\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, y)\n",
        "  print(f'{fashion_mnist_labels[classes]} Train Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')\n",
        "\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T-shirt/top Train Accuracy: 0.995, loss: 0.029\n",
            "Trouser Train Accuracy: 1.000, loss: 0.003\n",
            "Pullover Train Accuracy: 1.000, loss: 0.009\n",
            "Dress Train Accuracy: 1.000, loss: 0.000\n",
            "Coat Train Accuracy: 1.000, loss: 0.024\n",
            "Sandal Train Accuracy: 1.000, loss: 0.063\n",
            "Shirt Train Accuracy: 0.996, loss: 0.042\n",
            "Sneaker Train Accuracy: 1.000, loss: 0.000\n",
            "Bag Train Accuracy: 1.000, loss: 0.002\n",
            "Ankle boot Train Accuracy: 1.000, loss: 0.050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scjb7Wh_sn6b",
        "outputId": "6d5dcae5-91fa-4496-8098-dd6d3765e52e"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x_test, sorted_y_test)):\n",
        "  accuracy.init(y_sorted_lists)\n",
        "  #print(sorted_y[y_sorted_lists].shape)\n",
        "  #print(sorted_x[X_sorted_lists].shape)\n",
        "  dense1.forward(sorted_x_test[X_sorted_lists])\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  testmean = np.mean(activation1.output, axis=0)\n",
        "  testing_diff.append(testmean)\n",
        "  dropout1.infrence(activation1.output,sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, sorted_y_test[y_sorted_lists])\n",
        "\n",
        "  print(f'{fashion_mnist_labels[classes]} Test Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T-shirt/top Test Accuracy: 1.000, loss: 0.020\n",
            "Trouser Test Accuracy: 1.000, loss: 0.001\n",
            "Pullover Test Accuracy: 1.000, loss: 0.008\n",
            "Dress Test Accuracy: 1.000, loss: 0.000\n",
            "Coat Test Accuracy: 1.000, loss: 0.029\n",
            "Sandal Test Accuracy: 1.000, loss: 0.060\n",
            "Shirt Test Accuracy: 1.000, loss: 0.032\n",
            "Sneaker Test Accuracy: 1.000, loss: 0.000\n",
            "Bag Test Accuracy: 1.000, loss: 0.002\n",
            "Ankle boot Test Accuracy: 1.000, loss: 0.048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u-O8oNZ0qA"
      },
      "source": [
        "# Full mnist test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbD4KrLMnTcR"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMfBGUHeZ4L5",
        "outputId": "4ea5fb65-e552-41bc-c18a-61863e8e308d"
      },
      "source": [
        "(input, label), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(input.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "input = input[keys]\n",
        "label = label[keys]\n",
        "\n",
        "\n",
        "# Scale and reshape samples\n",
        "input = (input.reshape(input.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(label)\n",
        "\n",
        "dense1.forward(input)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,label)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, label)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, label)\n",
        "\n",
        "print(f'Full Training Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full Training Accuracy: 0.999, loss: 0.023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aat-uVF6nYu7"
      },
      "source": [
        "Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hyU1tBDna8x",
        "outputId": "663b0ae1-4561-420d-fa5d-6ecd6dd77192"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Full Testing Accuracy: {testaccuracy:.5f}, loss: {loss:.3f}')"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full Testing Accuracy: 0.99890, loss: 0.023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbIMZ7Pk_Tnp"
      },
      "source": [
        "Change idex to get confidence of different samples of testing data. Index values 0-1600 were refrenced in training. Anything past was never seen during training. Lowest confidence is at index 2732 when trained with 488 epochs and numpy seed set to 22."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "JaxWcRIr_BCV",
        "outputId": "34993bec-bb29-4732-9c2e-59a234439a4a"
      },
      "source": [
        "index = 5000\n",
        "\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {fashion_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {fashion_mnist_labels[y_test[index]]}')\n",
        "\n",
        "X_test.resize(X_test.shape[0],28,28)\n",
        "image = X_test[index]\n",
        "fig = plt.figure\n",
        "plt.title(f'{fashion_mnist_labels[y_test[index]]}')\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.951% Confident True is Pullover. True is actually Pullover\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUW0lEQVR4nO3dfZBUVXoG8OdhYEA+ZYTFAQYRpCgNRrQQV9ciuKsrUeMHf5AliWJiHFO1araylVpirFr+IWVF3XXjVq2LpTLo6kZrFSk1cVk0fhbCQI2I6AIiyOeAysfwzQxv/uiLO+rc94x9u+c2c55f1dT03LdPz+keHu7tPvfcQzODiHR/PfLugIh0DYVdJBIKu0gkFHaRSCjsIpFQ2EUiobBHiuTNJN9s97ORPCvPPkl5KezdAMmNJA+R3E+ymeR8kv3z7pdUFoW9+/grM+sP4AIAkwDcnXN/XCR75t2H2Cjs3YyZbQXwPwAmJIfmX4SK5P+R/MfQY5AcRHIByV0kN5G8m2QPkr1J7iE5od19hyZHFd9Kfr6GZFNyv7dJ/nm7+24k+ROSqwAcUOC7lsLezZCsA3AVgN0ZHuZBAIMAjAHwFwBuAvD3ZnYEwLMAZra77wwAr5nZTpLnA3gUwG0ATgPwawCLSPZud/+ZAK4GcKqZtWboo3xDCnv3sZDkHgBvAngNwH8U8yAkqwD8AMC/mVmLmW0EcD+AG5O7PJnUT/ibZBsA1AP4tZm9Y2ZtZtYA4AiAb7e7/3+Z2WYzO1RM/6R4OozqPq43sz+c+IHk6CIfZwiAXgA2tdu2CcCI5ParAPqSvAhAM4CJAJ5LamcAmEXyjnZtqwEMb/fz5iL7JRkp7N3XgeR7XwD7ktund6LdpwCOoRDcNcm2UQC2AoCZtZF8GoXD8WYAL5hZS3K/zQDmmtlc5/E1zTInOozvpsxsFwoB/TuSVST/AcDYTrRrA/A0gLkkB5A8A8C/AHii3d2eBPDXAP4WfzqEB4CHAfwTyYtY0I/k1SQHlOhpSQYKe/d2K4B/BfAZgD8D8HYn292BwpHBBhQ+A3gShQ/eAABm9k5SH47CJ/8ntjcmv/OXKHxAuB7AzRmfg5QIdfEKkThozy4SCYVdJBIKu0gkFHaRSHTpODtJfRooUmZmxo62Z9qzk5xG8o8k15OcneWxyo2k+yXdT+hvHtu/h6KH3pJzqNcCuALAFgDLAcw0szVOm9z27KE/oIYgu58soT2Z/z2UY88+GcB6M9tgZkcB/BbAdRkeT0TKKEvYR+DLkxq24E+TJb5Asp5kI8nGDL9LRDIq+wd0ZjYPwDxAH9CJ5CnLnn0rgLp2P49MtolIBcoS9uUAxpE8k2Q1Chc0WFSabolIqRV9GG9mrSRvB/AygCoAj5rZ+yXrWYnl+elqjx7+/6k33XSTW58zZ45bf+ihh1Jrb7zxhts2pK2tza337t3brU+dOjW1NnPmzNQaAMyYMcOtr1q1yq1n+Zt3x9GbTO/ZzewlAC+VqC8iUkY6XVYkEgq7SCQUdpFIKOwikVDYRSKhsItEoksvONldT5dtaGhw69OmTXPr+/fvd+tHjx5162PGjHHrnr1797r10L+PQYMGuXWv7+vWrXPbDh8+3K2vX7/erd99d/ralq+99prb9mRWlvnsInLyUNhFIqGwi0RCYReJhMIuEgmFXSQSGnrrpIULF6bWrr76arft5s3+kuRVVVVufdeuXW593759qbV+/fq5baurq916aKrnoUOH3PqxY8dSa6GhtdD02T59+rj1vn37ptamT5/utn355ZfdeiVPgdXQm0jkFHaRSCjsIpFQ2EUiobCLREJhF4mEwi4SCY2zJ0JTNdeuXZtaO3LkSKbfHfobDBw40K17Y74tLS1u29A4eajujWUDwJAhQ1Jroam9x48fz1SvqalJrb3yyitu2xtuuMGtVzKNs4tETmEXiYTCLhIJhV0kEgq7SCQUdpFIKOwikci0imt3MmXKFLd+6qmnptaam5vdtj17+i9zaJw9NB59+PDh1NrSpUvdtqE54aG5+qHnvnv37tRaaE54aD576HXzzn+YMGGC27Y7yhR2khsBtABoA9BqZpNK0SkRKb1S7NkvM7NPS/A4IlJGes8uEomsYTcAvye5gmR9R3cgWU+ykWRjxt8lIhlkPYy/1My2kvwWgMUkPzSz19vfwczmAZgHVPZEGJHuLtOe3cy2Jt93AngOwORSdEpESq/osJPsR3LAidsAvg9gdak6JiKlleUwfhiA55Kx0p4AnjSz/y1Jr3IQGmdva2tLrfXq1ctt613XHQDWrFnj1kNLG3tj2aG58KG+h84RCM3lP+WUU9y6x3vNgXDfvfYjR450206e7B+kLlu2zK1XoqLDbmYbAJxXwr6ISBlp6E0kEgq7SCQUdpFIKOwikVDYRSKhKa6JqVOnunVvGCc0FfOjjz7KVPemsAL+ktChoa8777zTrYeGDUNLQh89ejS1Fpri2qOHvy/KsmxyaKnq0NTek3HoTXt2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSWrI50dra6tb37t2bWgu9hqeddppbX7JkiVt/++233frw4cNTa+PHj3fbXnzxxW7dmz4LAJ9+6l9r1JtKGjp/ILQkc//+/d36wYMHU2sDBgxw265cudKtX3LJJW49T1qyWSRyCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMbZEx9++KFbHzp0aGot9Bpu2bLFrZ93nn+RXm+8GPDnrIcu9bxhwwa3PmrUKLf+zDPPuPXLL788tRZaLjo059ybKx9qX1VV5bbdtWuXWx8zZoxbz5PG2UUip7CLREJhF4mEwi4SCYVdJBIKu0gkFHaRSOi68YnQ0sbeNcqPHTvmtn3iiSfcemic/fPPP3frntCyxgcOHHDrd9xxh1tftGiRW7/22mtTa4899pjbNnQ9/fvuu8+tP/jgg6m10PMO1U9GwT07yUdJ7iS5ut22GpKLSa5Lvg8ubzdFJKvOHMbPBzDtK9tmA1hiZuMALEl+FpEKFgy7mb0O4KvHkdcBaEhuNwC4vsT9EpESK/Y9+zAz257c3gFgWNodSdYDqC/y94hIiWT+gM7MzJvgYmbzAMwDKnsijEh3V+zQWzPJWgBIvu8sXZdEpByKDfsiALOS27MAPF+a7ohIuQQP40k+BWAqgCEktwD4KYB7ADxN8hYAmwDMKGcnu4K3/jrgr0O+YsUKt22onpW3jnlozveFF17o1ocMGeLWQ+cA3Hrrram1Tz75xG0burZ7Y2OjW+/ZM/2fd+gaBIMGDXLrJ6Ng2M1sZkrpeyXui4iUkU6XFYmEwi4SCYVdJBIKu0gkFHaRSEQzxbW2tjZTe2+qaGgq5quvvprpd/ft29ete0sXHzp0yG3b3Nzs1u+99163PnfuXLfuDWl6l8AGwktVh4besvCGWk9W2rOLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpGIZpz9u9/9rluvqalx69548eLFi4vqU2d99tlnbr2hoSG1ds0117ht6+rq3PqePXvceu/evd363r17U2ve5blDbbPypgUD4b4NG5Z6JTYA4fMX8qA9u0gkFHaRSCjsIpFQ2EUiobCLREJhF4mEwi4SiWjG2UPj6MePH3fr3rjru+++67Y966yz3HpIaJx9165dqbX169e7bUOXgg69Lt7lmgFg9OjRqbXQWHbo3IgsQr+7urrarV900UVuPbSUdR60ZxeJhMIuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIhHNOLt3bXUgvIRvVVVVai0073r69OluPWTgwIFu/Zxzzkmtha7N3tTU5NavuOIKt/7AAw+49fr6+tRanz593LahJZtHjBjh1j2h8wNCQucfVKLgnp3koyR3klzdbtsckltJNiVfV5W3myKSVWcO4+cDmNbB9p+b2cTk66XSdktESi0YdjN7HYB/TqWIVLwsH9DdTnJVcpg/OO1OJOtJNpIs38JcIhJUbNh/BWAsgIkAtgO4P+2OZjbPzCaZ2aQif5eIlEBRYTezZjNrM7PjAB4GMLm03RKRUisq7CTbr398A4DVafcVkcoQHGwk+RSAqQCGkNwC4KcAppKcCMAAbARwWxn7WBJjx44t22N7Y/AAcOWVV2Z6/JEjR7r1F154oejHDs21X7lypVufMmWKW58wYUJqbdOmTW7b0Dj7xIkT3brX93PPPddtG5rvPn78eLee5W9SLsGwm9nMDjY/Uoa+iEgZ6XRZkUgo7CKRUNhFIqGwi0RCYReJRDRTXPv16+fWQ0MtntDyvGeeeaZbb21tdeuh5YUvu+yy1Npbb73ltl27dq1bHzVqlFufNWuWW/cug53lNQfCU3+XLl2aWrvgggvctocPH3brQ4cOdeuVSHt2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSGmcvgayXPG5ra3PrLS0tbt2b6nn22We7bb3lngF/yWUg25LPWS/n7J1fAAA7duwo+rFD5z7U1dUV/dh50Z5dJBIKu0gkFHaRSCjsIpFQ2EUiobCLREJhF4lENOPsobnToctBe0JjrjU1NW49NNYd6rs3nlxdXe22HTw4deUuAMDWrVvdeuh185bCDo1lHzt2zK1PmuQvMvT444+7dU9oCe/Q61aJtGcXiYTCLhIJhV0kEgq7SCQUdpFIKOwikVDYRSLRmSWb6wAsADAMhSWa55nZL0jWAPhvAKNRWLZ5hpntLl9Xyys0Xrx7d/pTq62tTa115rGzzrXv27dvai00Xhyqh+bqh9p7Dh065NZ79erl1kNz7UPXfvd48/CB7HPx89CZPXsrgB+b2TkAvg3ghyTPATAbwBIzGwdgSfKziFSoYNjNbLuZrUxutwD4AMAIANcBaEju1gDg+nJ1UkSy+0bv2UmOBnA+gHcADDOz7UlpBwqH+SJSoTr9xoNkfwC/A/AjM9vX/nxtMzOSHb55I1kPoD5rR0Ukm07t2Un2QiHovzGzZ5PNzSRrk3otgJ0dtTWzeWY2ycz8WQsiUlbBsLOwC38EwAdm9rN2pUUATizhOQvA86XvnoiUSmcO478D4EYA75FsSrbdBeAeAE+TvAXAJgAzytPF0ggNEYWmkXrDONu3b0+tAcDy5cvdemiKa2gIyhvaCy33HHreoXqW1zV0Ce3Q8962bZtb37hxo1v3ZH3elSgYdjN7E0DaM/9eabsjIuWiM+hEIqGwi0RCYReJhMIuEgmFXSQSCrtIJE6+eXpFCl2WODSuunfv3tTauHHj3LYjR4506/v27XPrWS5zHXpeWcfhswg99tChQ9366aef7tYXLlyYWtu/f7/btpzPOy/as4tEQmEXiYTCLhIJhV0kEgq7SCQUdpFIKOwikYhmnD00lh2an+zNZ3/xxRfdtrNn+xfePXr0qFsP9c27rHFoHL1Pnz5uPbTkc+iSy1nm2ofms4fmq3vXCQg974MHD7r10Dh9JdKeXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJRDTj7KFx0dCYbu/evVNrobnyCxYscOtSHt5y06Fr1mddRrsSac8uEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0QiOM5Osg7AAgDDABiAeWb2C5JzANwK4MSk4bvM7KVydTSrZcuWufXdu3e79dAa7FmExvhD89lD7T2h66OHziEop9B89yNHjrj1lpaW1NqaNWvctt4YPQDMnz/frVeizpxU0wrgx2a2kuQAACtILk5qPzez+8rXPREplWDYzWw7gO3J7RaSHwAYUe6OiUhpfaP37CRHAzgfwDvJpttJriL5KMnBKW3qSTaSbMzUUxHJpNNhJ9kfwO8A/MjM9gH4FYCxACaisOe/v6N2ZjbPzCaZ2aQS9FdEitSpsJPshULQf2NmzwKAmTWbWZuZHQfwMIDJ5eumiGQVDDsLH9c+AuADM/tZu+217e52A4DVpe+eiJRKZz6N/w6AGwG8R7Ip2XYXgJkkJ6IwHLcRwG1l6WGJfPzxx269qanJrW/btq2U3fmSrMNbra2tJepJZQkNvYV4l9g+cOCA2zb0NwkN1Vaiznwa/yaAjgZjK3ZMXUS+TmfQiURCYReJhMIuEgmFXSQSCrtIJBR2kUhEcynpPXv2uHVvOmRn2mcRmmYamuLaXWV93t75B42N/lSN2tpat75jx46i+pQn7dlFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUiwK8dwSe4CsKndpiEAPu2yDnwzldq3Su0XoL4Vq5R9O8PMhnZU6NKwf+2Xk42Vem26Su1bpfYLUN+K1VV902G8SCQUdpFI5B32eTn/fk+l9q1S+wWob8Xqkr7l+p5dRLpO3nt2EekiCrtIJHIJO8lpJP9Icj3J2Xn0IQ3JjSTfI9mU9/p0yRp6O0mubrethuRikuuS7x2usZdT3+aQ3Jq8dk0kr8qpb3UkXyW5huT7JP852Z7ra+f0q0tety5/z06yCsBaAFcA2AJgOYCZZuYvmN1FSG4EMMnMcj8Bg+QUAPsBLDCzCcm2/wTwuZndk/xHOdjMflIhfZsDYH/ey3gnqxXVtl9mHMD1AG5Gjq+d068Z6ILXLY89+2QA681sg5kdBfBbANfl0I+KZ2avA/j8K5uvA9CQ3G5A4R9Ll0vpW0Uws+1mtjK53QLgxDLjub52Tr+6RB5hHwFgc7uft6Cy1ns3AL8nuYJkfd6d6cAwM9ue3N4BYFienelAcBnvrvSVZcYr5rUrZvnzrPQB3dddamYXAPhLAD9MDlcrkhXeg1XS2GmnlvHuKh0sM/6FPF+7Ypc/zyqPsG8FUNfu55HJtopgZluT7zsBPIfKW4q6+cQKusn3nTn35wuVtIx3R8uMowJeuzyXP88j7MsBjCN5JslqAD8AsCiHfnwNyX7JBycg2Q/A91F5S1EvAjAruT0LwPM59uVLKmUZ77RlxpHza5f78udm1uVfAK5C4RP5jwD8ex59SOnXGADvJl/v5903AE+hcFh3DIXPNm4BcBqAJQDWAfgDgJoK6tvjAN4DsAqFYNXm1LdLUThEXwWgKfm6Ku/XzulXl7xuOl1WJBL6gE4kEgq7SCQUdpFIKOwikVDYRSKhsItEQmEXicT/A76Xj+jkLMpOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "h5c5xUTNXk2v",
        "outputId": "91241f55-661f-43f0-b3b8-eb38f3847e2f"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}