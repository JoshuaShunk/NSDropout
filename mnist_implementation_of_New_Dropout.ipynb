{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_implementation_of_New_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_implementation_of_New_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYgI3SFHqm4"
      },
      "source": [
        "# MNIST Numbers Implementation of My New Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HD9nTuEVHD"
      },
      "source": [
        "np.random.seed(seed=22) #Random seed used for comparison between old dropout"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cag8ZraxEZbF",
        "outputId": "8e533a25-48b2-4ad1-cf4d-e2fba59db0cf"
      },
      "source": [
        "print(np.random.random(size=3)) #Check that seeds line up"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20846054 0.48168106 0.42053804]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers (Credit to Harrison Kinsley & Daniel Kukiela for raw python implementation)\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Og9ZrbKtV"
      },
      "source": [
        "# New Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXBWsHDIUSfh"
      },
      "source": [
        "class Layer_BinaryNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = 1 - rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, inputs, val_inputs):\n",
        "        self.inputs = inputs\n",
        "        self.val_inputs = val_inputs\n",
        "        nummask = round(len(self.inputs[0]) * self.rate)\n",
        "        \n",
        "        #Averaging Values\n",
        "        self.meanarray1 = np.mean(inputs, axis=0)\n",
        "        self.meanarray2 = np.mean(val_inputs, axis=0)\n",
        "\n",
        "        if self.iterations != 0:\n",
        "            # Calculating value\n",
        "            self.difference = self.meanarray1 - self.meanarray2\n",
        "            ind = np.argpartition(self.difference, -nummask)[-nummask:]\n",
        "            mask = np.ones(self.meanarray1.shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            self.difference[~mask] = 1\n",
        "            self.difference[mask] = 0. \n",
        "            self.binary_mask = self.difference / self.rate\n",
        "\n",
        "        else:\n",
        "            self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                                  size=inputs.shape) / self.rate\n",
        "        self.output = inputs * self.binary_mask\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiuCzwWxbRl0"
      },
      "source": [
        "class Layer_CatagoricalNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, X_test, y_test, X, y):        \n",
        "        if self.iterations != 0:\n",
        "          #Sorting data into classes\n",
        "          idx = np.argsort(y_test)\n",
        "          X_test_sorted = X_test[idx]\n",
        "          y_test_sorted = y_test[idx]\n",
        "\n",
        "          idx2 = np.argsort(y)\n",
        "          X_train_sorted = X[idx2]\n",
        "          y_train_sorted = y[idx2]\n",
        "\n",
        "          #Adding sorted data into dictionaries \n",
        "          sorted_x = {}\n",
        "          sorted_y = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x[\"class_{0}\".format(classes)] = X[y == classes]\n",
        "            sorted_y[\"label_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "          sorted_x_test = {}\n",
        "          sorted_y_test = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x_test[\"class_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "            sorted_y_test[\"label_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "          #Averaging sorted data from each class then finding the difference between the averaged train and test inputs\n",
        "          differnce_classes = {}\n",
        "          for i, classes, test_classes in zip(range(len(set(y))), sorted_x, sorted_x_test):\n",
        "            differnce_classes[\"diff_{0}\".format(i)] = np.mean(sorted_x[classes], axis=0) - np.mean(sorted_x_test[classes], axis=0)\n",
        "\n",
        "          #Masking the data taking the high values(greatest difference between train and test) and setting their values to 0\n",
        "          self.diff_mask = {}\n",
        "          for i, classes, test_classes, diff in zip(range(len(set(y))), sorted_x, sorted_x_test, differnce_classes):\n",
        "            ind = np.argpartition(differnce_classes[diff], -round(len(X[0]) * self.rate))[-round(len(X[0]) * self.rate):]\n",
        "            mask = np.ones(np.mean(sorted_x[classes],axis=0).shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            differnce_classes[diff][~mask] = 0.\n",
        "            differnce_classes[diff][mask] = 1\n",
        "            self.diff_mask[\"mask_{0}\".format(i)] = differnce_classes[diff]\n",
        "\n",
        "          #Goes through each input values and applies the apprioprite mask based on what the true output should be.\n",
        "          binary_mask = np.empty(shape=X.shape)\n",
        "          #for i, input, label in zip(range(len(X)), X, y):\n",
        "          for i, (input, label) in enumerate(zip(X,y)): \n",
        "            for true, diff in enumerate(self.diff_mask):\n",
        "              if label == true:\n",
        "                self.binary_mask[i] = self.diff_mask[diff]\n",
        "        else:\n",
        "          self.binary_mask = np.random.binomial(1, (1-self.rate), size=X.shape)\n",
        "        \n",
        "        self.output = (self.binary_mask/(1-self.rate)) * X\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def infrence(self, input, label):\n",
        "        self.input = input\n",
        "        self.label = label\n",
        "        idx = np.argsort(self.label)\n",
        "        input_sorted = input[idx]\n",
        "        label_sorted = label[idx]\n",
        "        self.infrence_binary_mask = np.empty(shape=self.input.shape)\n",
        "        for i, (input, label) in enumerate(zip(self.input, self.label)):\n",
        "          #for true, diff in zip(range(len(set(self.label))),self.diff_mask):\n",
        "          for true, diff in enumerate(self.diff_mask):\n",
        "            if label == true:\n",
        "              self.infrence_binary_mask[i] = self.diff_mask[diff]\n",
        "\n",
        "        self.output = self.infrence_binary_mask * self.input\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "dbc59c37-c73b-47ef-efbb-07145a89c017"
      },
      "source": [
        "(X, y), (X_val, y_val) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "input = X\n",
        "label = y\n",
        "\n",
        "X = X[:9600,:,:]\n",
        "#X_test = X_test[:1600,:,:]\n",
        "y = y[:9600]\n",
        "#y_test  = y_test[:1600]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1/6))\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_train = (X_train.reshape(X_train.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_val = (X_val.reshape(X_val.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "input = (input.reshape(input.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 784)\n",
            "(8000,)\n",
            "(1600, 784)\n",
            "(1600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nW7mqGTnem"
      },
      "source": [
        "Sorting Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtvA9y81TpGf",
        "outputId": "65094265-563e-44e0-82d9-fdb5230e6234"
      },
      "source": [
        "idx = np.argsort(y_train)\n",
        "X_sorted = X_train[idx]\n",
        "y_sorted = y_train[idx]\n",
        "\n",
        "sorted_x = {}\n",
        "sorted_y = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x[\"X_{0}\".format(classes)] = X_train[y_train == classes]\n",
        "  sorted_y[\"y_{0}\".format(classes)] = y_train[y_train == classes]\n",
        "\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x[sorted_lists].shape[0]}')\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_0: 773\n",
            "Number of Samples for X_1: 768\n",
            "Number of Samples for X_2: 831\n",
            "Number of Samples for X_3: 808\n",
            "Number of Samples for X_4: 781\n",
            "Number of Samples for X_5: 763\n",
            "Number of Samples for X_6: 853\n",
            "Number of Samples for X_7: 784\n",
            "Number of Samples for X_8: 846\n",
            "Number of Samples for X_9: 793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaNaUO3kP2G"
      },
      "source": [
        "Sorting Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBLFeGAUkSOs",
        "outputId": "04248573-c350-41ef-e844-29c8d3246262"
      },
      "source": [
        "idx = np.argsort(y_test)\n",
        "X_test_sorted = X_test[idx]\n",
        "y_test_sorted = y_test[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_test = {}\n",
        "sorted_y_test = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_test[\"X_test_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "  sorted_y_test[\"y_test_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_test:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_test[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_test[sorted_lists].shape[0])\n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_test_0: 141\n",
            "Number of Samples for X_test_1: 168\n",
            "Number of Samples for X_test_2: 173\n",
            "Number of Samples for X_test_3: 170\n",
            "Number of Samples for X_test_4: 156\n",
            "Number of Samples for X_test_5: 164\n",
            "Number of Samples for X_test_6: 139\n",
            "Number of Samples for X_test_7: 183\n",
            "Number of Samples for X_test_8: 148\n",
            "Number of Samples for X_test_9: 158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSOlE-g40VbR",
        "outputId": "b728b892-0f61-4fd2-a7e6-714b2b589d61"
      },
      "source": [
        "idx = np.argsort(y_val)\n",
        "X_val_sorted = X_val[idx]\n",
        "y_val_sorted = y_val[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_val = {}\n",
        "sorted_y_val = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_val[\"X_val_{0}\".format(classes)] = X_val[y_val == classes]\n",
        "  sorted_y_val[\"y_val_{0}\".format(classes)] = y_val[y_val == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_val:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_val[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_val[sorted_lists].shape[0])"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_val_0: 1000\n",
            "Number of Samples for X_val_1: 1000\n",
            "Number of Samples for X_val_2: 1000\n",
            "Number of Samples for X_val_3: 1000\n",
            "Number of Samples for X_val_4: 1000\n",
            "Number of Samples for X_val_5: 1000\n",
            "Number of Samples for X_val_6: 1000\n",
            "Number of Samples for X_val_7: 1000\n",
            "Number of Samples for X_val_8: 1000\n",
            "Number of Samples for X_val_9: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbnq4TJp1cl",
        "outputId": "f914411a-b614-4f19-bab9-8bb668bbd0c3"
      },
      "source": [
        "print(f'Found {X.shape[0]} images belonging to {len(set(y))} unique classes')"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9600 images belonging to 10 unique classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_CatagoricalNSDropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,len(set(y)))\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7,learning_rate=0.005)\n",
        "#optimizer = Optimizer_SGD(learning_rate=0.01)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "aef2dab3-c938-4aa1-929f-6127f1ae431a"
      },
      "source": [
        "epochs = 109\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X_train)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    if epoch != 0:\n",
        "      cached_val_inputs = cached_val_inputs\n",
        "      cached_train_inputs = activation1.output\n",
        "    else:\n",
        "      cached_val_inputs = np.random.random(size=128) #Never used just needed to pass to dropout\n",
        "      cached_train_inputs = activation1.output\n",
        "\n",
        "\n",
        "    dropout1.forward(X=activation1.output, y=y_train, X_test=cached_val_inputs, y_test=y_test)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y_train)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y_train)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "    dropout1.post_update_params()\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    if epoch == 0:\n",
        "      dense2.forward(activation1.output)\n",
        "    else:\n",
        "      dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "      dense2.forward(dropout1.output)\n",
        "    \n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Unseen Validaiton Accuracy\n",
        "    dense1.forward(X_val)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    if epoch == 0:\n",
        "      dense2.forward(activation1.output)\n",
        "    else:\n",
        "      dropout1.infrence(activation1.output,y_val)\n",
        "\n",
        "      dense2.forward(dropout1.output)\n",
        "    \n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    testloss = loss_function.calculate(activation4.output, y_val)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    testaccuracy = accuracy.calculate(predictions, y_val)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        "    test_acc_cache.append(testaccuracy)\n",
        "    test_loss_cache.append(testloss)\n",
        "    \n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ' +\n",
        "              f'Unseen, acc: {testaccuracy:.3f}, loss: {testloss:.3f} ')"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.112, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.005000000 validation, acc: 0.087, loss: 2.302 Unseen, acc: 0.100, loss: 2.302 \n",
            "epoch: 1, acc: 0.107, loss: 2.304 (data_loss: 2.302, reg_loss: 0.002), lr: 0.004999998 validation, acc: 0.155, loss: 2.295 Unseen, acc: 0.169, loss: 2.295 \n",
            "epoch: 2, acc: 0.202, loss: 2.297 (data_loss: 2.296, reg_loss: 0.001), lr: 0.004999995 validation, acc: 0.279, loss: 2.256 Unseen, acc: 0.287, loss: 2.258 \n",
            "epoch: 3, acc: 0.274, loss: 2.251 (data_loss: 2.250, reg_loss: 0.001), lr: 0.004999993 validation, acc: 0.246, loss: 2.113 Unseen, acc: 0.252, loss: 2.120 \n",
            "epoch: 4, acc: 0.245, loss: 2.109 (data_loss: 2.107, reg_loss: 0.002), lr: 0.004999990 validation, acc: 0.271, loss: 1.882 Unseen, acc: 0.273, loss: 1.897 \n",
            "epoch: 5, acc: 0.284, loss: 1.855 (data_loss: 1.852, reg_loss: 0.003), lr: 0.004999988 validation, acc: 0.223, loss: 1.739 Unseen, acc: 0.201, loss: 1.763 \n",
            "epoch: 6, acc: 0.203, loss: 1.788 (data_loss: 1.784, reg_loss: 0.004), lr: 0.004999985 validation, acc: 0.333, loss: 1.671 Unseen, acc: 0.325, loss: 1.691 \n",
            "epoch: 7, acc: 0.326, loss: 1.674 (data_loss: 1.669, reg_loss: 0.005), lr: 0.004999983 validation, acc: 0.256, loss: 1.719 Unseen, acc: 0.253, loss: 1.740 \n",
            "epoch: 8, acc: 0.292, loss: 1.691 (data_loss: 1.685, reg_loss: 0.006), lr: 0.004999980 validation, acc: 0.407, loss: 1.620 Unseen, acc: 0.390, loss: 1.648 \n",
            "epoch: 9, acc: 0.394, loss: 1.594 (data_loss: 1.587, reg_loss: 0.007), lr: 0.004999978 validation, acc: 0.434, loss: 1.460 Unseen, acc: 0.420, loss: 1.492 \n",
            "epoch: 10, acc: 0.435, loss: 1.446 (data_loss: 1.438, reg_loss: 0.008), lr: 0.004999975 validation, acc: 0.492, loss: 1.350 Unseen, acc: 0.476, loss: 1.380 \n",
            "epoch: 11, acc: 0.469, loss: 1.348 (data_loss: 1.339, reg_loss: 0.009), lr: 0.004999973 validation, acc: 0.460, loss: 1.252 Unseen, acc: 0.455, loss: 1.272 \n",
            "epoch: 12, acc: 0.456, loss: 1.271 (data_loss: 1.261, reg_loss: 0.010), lr: 0.004999970 validation, acc: 0.573, loss: 1.154 Unseen, acc: 0.558, loss: 1.181 \n",
            "epoch: 13, acc: 0.533, loss: 1.224 (data_loss: 1.213, reg_loss: 0.011), lr: 0.004999968 validation, acc: 0.593, loss: 1.123 Unseen, acc: 0.577, loss: 1.150 \n",
            "epoch: 14, acc: 0.553, loss: 1.150 (data_loss: 1.139, reg_loss: 0.012), lr: 0.004999965 validation, acc: 0.603, loss: 1.075 Unseen, acc: 0.600, loss: 1.091 \n",
            "epoch: 15, acc: 0.523, loss: 1.162 (data_loss: 1.150, reg_loss: 0.013), lr: 0.004999963 validation, acc: 0.605, loss: 1.091 Unseen, acc: 0.601, loss: 1.095 \n",
            "epoch: 16, acc: 0.560, loss: 1.146 (data_loss: 1.132, reg_loss: 0.014), lr: 0.004999960 validation, acc: 0.584, loss: 1.141 Unseen, acc: 0.567, loss: 1.145 \n",
            "epoch: 17, acc: 0.581, loss: 1.198 (data_loss: 1.184, reg_loss: 0.015), lr: 0.004999958 validation, acc: 0.654, loss: 0.964 Unseen, acc: 0.647, loss: 0.969 \n",
            "epoch: 18, acc: 0.577, loss: 0.984 (data_loss: 0.968, reg_loss: 0.016), lr: 0.004999955 validation, acc: 0.693, loss: 0.917 Unseen, acc: 0.688, loss: 0.925 \n",
            "epoch: 19, acc: 0.666, loss: 0.950 (data_loss: 0.933, reg_loss: 0.017), lr: 0.004999953 validation, acc: 0.662, loss: 0.846 Unseen, acc: 0.656, loss: 0.863 \n",
            "epoch: 20, acc: 0.653, loss: 0.859 (data_loss: 0.842, reg_loss: 0.018), lr: 0.004999950 validation, acc: 0.718, loss: 0.774 Unseen, acc: 0.703, loss: 0.797 \n",
            "epoch: 21, acc: 0.687, loss: 0.864 (data_loss: 0.845, reg_loss: 0.019), lr: 0.004999948 validation, acc: 0.762, loss: 0.732 Unseen, acc: 0.747, loss: 0.749 \n",
            "epoch: 22, acc: 0.632, loss: 0.854 (data_loss: 0.834, reg_loss: 0.019), lr: 0.004999945 validation, acc: 0.692, loss: 0.776 Unseen, acc: 0.696, loss: 0.776 \n",
            "epoch: 23, acc: 0.658, loss: 0.849 (data_loss: 0.828, reg_loss: 0.020), lr: 0.004999943 validation, acc: 0.661, loss: 0.773 Unseen, acc: 0.668, loss: 0.765 \n",
            "epoch: 24, acc: 0.684, loss: 0.872 (data_loss: 0.850, reg_loss: 0.021), lr: 0.004999940 validation, acc: 0.696, loss: 0.738 Unseen, acc: 0.705, loss: 0.737 \n",
            "epoch: 25, acc: 0.655, loss: 0.960 (data_loss: 0.937, reg_loss: 0.023), lr: 0.004999938 validation, acc: 0.720, loss: 0.818 Unseen, acc: 0.718, loss: 0.820 \n",
            "epoch: 26, acc: 0.641, loss: 0.973 (data_loss: 0.950, reg_loss: 0.024), lr: 0.004999935 validation, acc: 0.470, loss: 1.759 Unseen, acc: 0.471, loss: 1.728 \n",
            "epoch: 27, acc: 0.262, loss: 3.503 (data_loss: 3.478, reg_loss: 0.025), lr: 0.004999933 validation, acc: 0.320, loss: 2.711 Unseen, acc: 0.322, loss: 2.592 \n",
            "epoch: 28, acc: 0.265, loss: 4.081 (data_loss: 4.055, reg_loss: 0.026), lr: 0.004999930 validation, acc: 0.346, loss: 2.956 Unseen, acc: 0.340, loss: 2.776 \n",
            "epoch: 29, acc: 0.268, loss: 3.363 (data_loss: 3.336, reg_loss: 0.027), lr: 0.004999928 validation, acc: 0.440, loss: 2.332 Unseen, acc: 0.453, loss: 2.189 \n",
            "epoch: 30, acc: 0.421, loss: 2.802 (data_loss: 2.774, reg_loss: 0.028), lr: 0.004999925 validation, acc: 0.368, loss: 2.541 Unseen, acc: 0.384, loss: 2.391 \n",
            "epoch: 31, acc: 0.397, loss: 2.589 (data_loss: 2.561, reg_loss: 0.029), lr: 0.004999923 validation, acc: 0.370, loss: 2.215 Unseen, acc: 0.391, loss: 2.093 \n",
            "epoch: 32, acc: 0.412, loss: 3.324 (data_loss: 3.295, reg_loss: 0.029), lr: 0.004999920 validation, acc: 0.397, loss: 2.545 Unseen, acc: 0.415, loss: 2.406 \n",
            "epoch: 33, acc: 0.502, loss: 2.396 (data_loss: 2.366, reg_loss: 0.030), lr: 0.004999918 validation, acc: 0.574, loss: 1.862 Unseen, acc: 0.587, loss: 1.797 \n",
            "epoch: 34, acc: 0.581, loss: 2.045 (data_loss: 2.014, reg_loss: 0.031), lr: 0.004999915 validation, acc: 0.648, loss: 1.792 Unseen, acc: 0.653, loss: 1.757 \n",
            "epoch: 35, acc: 0.625, loss: 2.283 (data_loss: 2.251, reg_loss: 0.032), lr: 0.004999913 validation, acc: 0.702, loss: 1.720 Unseen, acc: 0.699, loss: 1.684 \n",
            "epoch: 36, acc: 0.693, loss: 2.325 (data_loss: 2.292, reg_loss: 0.033), lr: 0.004999910 validation, acc: 0.711, loss: 1.879 Unseen, acc: 0.707, loss: 1.834 \n",
            "epoch: 37, acc: 0.714, loss: 2.400 (data_loss: 2.366, reg_loss: 0.033), lr: 0.004999908 validation, acc: 0.716, loss: 1.891 Unseen, acc: 0.716, loss: 1.839 \n",
            "epoch: 38, acc: 0.695, loss: 2.421 (data_loss: 2.387, reg_loss: 0.034), lr: 0.004999905 validation, acc: 0.713, loss: 1.985 Unseen, acc: 0.717, loss: 1.917 \n",
            "epoch: 39, acc: 0.694, loss: 2.255 (data_loss: 2.220, reg_loss: 0.035), lr: 0.004999903 validation, acc: 0.771, loss: 2.068 Unseen, acc: 0.775, loss: 2.005 \n",
            "epoch: 40, acc: 0.781, loss: 2.282 (data_loss: 2.247, reg_loss: 0.035), lr: 0.004999900 validation, acc: 0.749, loss: 2.202 Unseen, acc: 0.747, loss: 2.149 \n",
            "epoch: 41, acc: 0.646, loss: 2.624 (data_loss: 2.588, reg_loss: 0.036), lr: 0.004999898 validation, acc: 0.672, loss: 2.054 Unseen, acc: 0.678, loss: 1.990 \n",
            "epoch: 42, acc: 0.688, loss: 2.359 (data_loss: 2.322, reg_loss: 0.037), lr: 0.004999895 validation, acc: 0.701, loss: 1.806 Unseen, acc: 0.709, loss: 1.750 \n",
            "epoch: 43, acc: 0.702, loss: 1.987 (data_loss: 1.950, reg_loss: 0.037), lr: 0.004999893 validation, acc: 0.695, loss: 1.704 Unseen, acc: 0.693, loss: 1.653 \n",
            "epoch: 44, acc: 0.711, loss: 1.868 (data_loss: 1.830, reg_loss: 0.037), lr: 0.004999890 validation, acc: 0.702, loss: 1.650 Unseen, acc: 0.697, loss: 1.600 \n",
            "epoch: 45, acc: 0.703, loss: 2.045 (data_loss: 2.007, reg_loss: 0.038), lr: 0.004999888 validation, acc: 0.724, loss: 1.709 Unseen, acc: 0.725, loss: 1.661 \n",
            "epoch: 46, acc: 0.694, loss: 2.020 (data_loss: 1.982, reg_loss: 0.038), lr: 0.004999885 validation, acc: 0.724, loss: 1.704 Unseen, acc: 0.726, loss: 1.655 \n",
            "epoch: 47, acc: 0.709, loss: 1.786 (data_loss: 1.747, reg_loss: 0.038), lr: 0.004999883 validation, acc: 0.728, loss: 1.571 Unseen, acc: 0.727, loss: 1.521 \n",
            "epoch: 48, acc: 0.663, loss: 1.991 (data_loss: 1.952, reg_loss: 0.039), lr: 0.004999880 validation, acc: 0.677, loss: 1.693 Unseen, acc: 0.684, loss: 1.627 \n",
            "epoch: 49, acc: 0.692, loss: 1.996 (data_loss: 1.958, reg_loss: 0.039), lr: 0.004999878 validation, acc: 0.682, loss: 1.656 Unseen, acc: 0.690, loss: 1.588 \n",
            "epoch: 50, acc: 0.693, loss: 1.770 (data_loss: 1.731, reg_loss: 0.039), lr: 0.004999875 validation, acc: 0.680, loss: 1.392 Unseen, acc: 0.689, loss: 1.334 \n",
            "epoch: 51, acc: 0.653, loss: 1.608 (data_loss: 1.569, reg_loss: 0.039), lr: 0.004999873 validation, acc: 0.657, loss: 1.309 Unseen, acc: 0.665, loss: 1.265 \n",
            "epoch: 52, acc: 0.658, loss: 1.462 (data_loss: 1.423, reg_loss: 0.039), lr: 0.004999870 validation, acc: 0.677, loss: 1.284 Unseen, acc: 0.686, loss: 1.234 \n",
            "epoch: 53, acc: 0.694, loss: 1.388 (data_loss: 1.349, reg_loss: 0.039), lr: 0.004999868 validation, acc: 0.677, loss: 1.258 Unseen, acc: 0.688, loss: 1.214 \n",
            "epoch: 54, acc: 0.693, loss: 1.395 (data_loss: 1.356, reg_loss: 0.039), lr: 0.004999865 validation, acc: 0.686, loss: 1.114 Unseen, acc: 0.696, loss: 1.075 \n",
            "epoch: 55, acc: 0.699, loss: 1.320 (data_loss: 1.281, reg_loss: 0.039), lr: 0.004999863 validation, acc: 0.683, loss: 0.993 Unseen, acc: 0.694, loss: 0.956 \n",
            "epoch: 56, acc: 0.682, loss: 1.090 (data_loss: 1.052, reg_loss: 0.038), lr: 0.004999860 validation, acc: 0.671, loss: 0.905 Unseen, acc: 0.683, loss: 0.876 \n",
            "epoch: 57, acc: 0.688, loss: 0.911 (data_loss: 0.873, reg_loss: 0.038), lr: 0.004999858 validation, acc: 0.679, loss: 0.811 Unseen, acc: 0.690, loss: 0.789 \n",
            "epoch: 58, acc: 0.698, loss: 0.853 (data_loss: 0.815, reg_loss: 0.038), lr: 0.004999855 validation, acc: 0.710, loss: 0.779 Unseen, acc: 0.722, loss: 0.765 \n",
            "epoch: 59, acc: 0.726, loss: 0.759 (data_loss: 0.722, reg_loss: 0.037), lr: 0.004999853 validation, acc: 0.776, loss: 0.732 Unseen, acc: 0.783, loss: 0.724 \n",
            "epoch: 60, acc: 0.772, loss: 0.695 (data_loss: 0.658, reg_loss: 0.037), lr: 0.004999850 validation, acc: 0.738, loss: 0.696 Unseen, acc: 0.741, loss: 0.692 \n",
            "epoch: 61, acc: 0.732, loss: 0.696 (data_loss: 0.659, reg_loss: 0.037), lr: 0.004999848 validation, acc: 0.725, loss: 0.697 Unseen, acc: 0.719, loss: 0.694 \n",
            "epoch: 62, acc: 0.715, loss: 0.690 (data_loss: 0.654, reg_loss: 0.036), lr: 0.004999845 validation, acc: 0.784, loss: 0.681 Unseen, acc: 0.784, loss: 0.676 \n",
            "epoch: 63, acc: 0.769, loss: 0.680 (data_loss: 0.644, reg_loss: 0.036), lr: 0.004999843 validation, acc: 0.747, loss: 0.666 Unseen, acc: 0.748, loss: 0.657 \n",
            "epoch: 64, acc: 0.721, loss: 0.771 (data_loss: 0.735, reg_loss: 0.036), lr: 0.004999840 validation, acc: 0.713, loss: 0.735 Unseen, acc: 0.718, loss: 0.737 \n",
            "epoch: 65, acc: 0.724, loss: 0.611 (data_loss: 0.576, reg_loss: 0.035), lr: 0.004999838 validation, acc: 0.787, loss: 0.603 Unseen, acc: 0.798, loss: 0.596 \n",
            "epoch: 66, acc: 0.777, loss: 0.731 (data_loss: 0.696, reg_loss: 0.035), lr: 0.004999835 validation, acc: 0.746, loss: 0.686 Unseen, acc: 0.759, loss: 0.687 \n",
            "epoch: 67, acc: 0.750, loss: 0.667 (data_loss: 0.632, reg_loss: 0.035), lr: 0.004999833 validation, acc: 0.755, loss: 0.634 Unseen, acc: 0.765, loss: 0.634 \n",
            "epoch: 68, acc: 0.765, loss: 0.621 (data_loss: 0.586, reg_loss: 0.034), lr: 0.004999830 validation, acc: 0.761, loss: 0.596 Unseen, acc: 0.769, loss: 0.599 \n",
            "epoch: 69, acc: 0.740, loss: 0.618 (data_loss: 0.584, reg_loss: 0.034), lr: 0.004999828 validation, acc: 0.806, loss: 0.548 Unseen, acc: 0.809, loss: 0.550 \n",
            "epoch: 70, acc: 0.810, loss: 0.515 (data_loss: 0.482, reg_loss: 0.033), lr: 0.004999825 validation, acc: 0.859, loss: 0.490 Unseen, acc: 0.862, loss: 0.491 \n",
            "epoch: 71, acc: 0.801, loss: 0.480 (data_loss: 0.447, reg_loss: 0.033), lr: 0.004999823 validation, acc: 0.887, loss: 0.462 Unseen, acc: 0.889, loss: 0.464 \n",
            "epoch: 72, acc: 0.898, loss: 0.479 (data_loss: 0.446, reg_loss: 0.033), lr: 0.004999820 validation, acc: 0.939, loss: 0.425 Unseen, acc: 0.946, loss: 0.426 \n",
            "epoch: 73, acc: 0.927, loss: 0.409 (data_loss: 0.376, reg_loss: 0.033), lr: 0.004999818 validation, acc: 0.955, loss: 0.373 Unseen, acc: 0.958, loss: 0.372 \n",
            "epoch: 74, acc: 0.960, loss: 0.338 (data_loss: 0.305, reg_loss: 0.033), lr: 0.004999815 validation, acc: 0.969, loss: 0.312 Unseen, acc: 0.969, loss: 0.312 \n",
            "epoch: 75, acc: 0.968, loss: 0.270 (data_loss: 0.238, reg_loss: 0.032), lr: 0.004999813 validation, acc: 0.984, loss: 0.237 Unseen, acc: 0.985, loss: 0.240 \n",
            "epoch: 76, acc: 0.955, loss: 0.260 (data_loss: 0.227, reg_loss: 0.032), lr: 0.004999810 validation, acc: 0.989, loss: 0.229 Unseen, acc: 0.986, loss: 0.234 \n",
            "epoch: 77, acc: 0.903, loss: 0.282 (data_loss: 0.249, reg_loss: 0.032), lr: 0.004999808 validation, acc: 0.941, loss: 0.258 Unseen, acc: 0.941, loss: 0.269 \n",
            "epoch: 78, acc: 0.946, loss: 0.261 (data_loss: 0.229, reg_loss: 0.032), lr: 0.004999805 validation, acc: 0.957, loss: 0.193 Unseen, acc: 0.956, loss: 0.205 \n",
            "epoch: 79, acc: 0.961, loss: 0.189 (data_loss: 0.156, reg_loss: 0.032), lr: 0.004999803 validation, acc: 0.979, loss: 0.156 Unseen, acc: 0.976, loss: 0.166 \n",
            "epoch: 80, acc: 0.951, loss: 0.210 (data_loss: 0.178, reg_loss: 0.033), lr: 0.004999800 validation, acc: 0.975, loss: 0.145 Unseen, acc: 0.972, loss: 0.153 \n",
            "epoch: 81, acc: 0.922, loss: 0.212 (data_loss: 0.179, reg_loss: 0.033), lr: 0.004999798 validation, acc: 0.973, loss: 0.149 Unseen, acc: 0.967, loss: 0.159 \n",
            "epoch: 82, acc: 0.970, loss: 0.157 (data_loss: 0.124, reg_loss: 0.033), lr: 0.004999795 validation, acc: 0.979, loss: 0.129 Unseen, acc: 0.973, loss: 0.138 \n",
            "epoch: 83, acc: 0.976, loss: 0.143 (data_loss: 0.111, reg_loss: 0.033), lr: 0.004999793 validation, acc: 0.988, loss: 0.103 Unseen, acc: 0.989, loss: 0.108 \n",
            "epoch: 84, acc: 0.987, loss: 0.113 (data_loss: 0.080, reg_loss: 0.033), lr: 0.004999790 validation, acc: 0.990, loss: 0.093 Unseen, acc: 0.989, loss: 0.095 \n",
            "epoch: 85, acc: 0.957, loss: 0.186 (data_loss: 0.153, reg_loss: 0.033), lr: 0.004999788 validation, acc: 0.966, loss: 0.146 Unseen, acc: 0.967, loss: 0.148 \n",
            "epoch: 86, acc: 0.980, loss: 0.123 (data_loss: 0.090, reg_loss: 0.033), lr: 0.004999785 validation, acc: 0.970, loss: 0.138 Unseen, acc: 0.972, loss: 0.132 \n",
            "epoch: 87, acc: 0.889, loss: 0.334 (data_loss: 0.301, reg_loss: 0.033), lr: 0.004999783 validation, acc: 0.980, loss: 0.105 Unseen, acc: 0.981, loss: 0.110 \n",
            "epoch: 88, acc: 0.980, loss: 0.128 (data_loss: 0.096, reg_loss: 0.033), lr: 0.004999780 validation, acc: 0.927, loss: 0.176 Unseen, acc: 0.915, loss: 0.194 \n",
            "epoch: 89, acc: 0.915, loss: 0.237 (data_loss: 0.204, reg_loss: 0.033), lr: 0.004999778 validation, acc: 0.979, loss: 0.085 Unseen, acc: 0.979, loss: 0.088 \n",
            "epoch: 90, acc: 0.865, loss: 0.486 (data_loss: 0.453, reg_loss: 0.033), lr: 0.004999775 validation, acc: 0.927, loss: 0.234 Unseen, acc: 0.921, loss: 0.250 \n",
            "epoch: 91, acc: 0.955, loss: 0.196 (data_loss: 0.164, reg_loss: 0.033), lr: 0.004999773 validation, acc: 0.957, loss: 0.192 Unseen, acc: 0.949, loss: 0.194 \n",
            "epoch: 92, acc: 0.838, loss: 0.444 (data_loss: 0.411, reg_loss: 0.033), lr: 0.004999770 validation, acc: 0.897, loss: 0.332 Unseen, acc: 0.895, loss: 0.328 \n",
            "epoch: 93, acc: 0.866, loss: 0.476 (data_loss: 0.444, reg_loss: 0.033), lr: 0.004999768 validation, acc: 0.872, loss: 0.413 Unseen, acc: 0.868, loss: 0.424 \n",
            "epoch: 94, acc: 0.828, loss: 0.625 (data_loss: 0.592, reg_loss: 0.033), lr: 0.004999765 validation, acc: 0.901, loss: 0.297 Unseen, acc: 0.900, loss: 0.299 \n",
            "epoch: 95, acc: 0.919, loss: 0.277 (data_loss: 0.245, reg_loss: 0.033), lr: 0.004999763 validation, acc: 0.944, loss: 0.198 Unseen, acc: 0.938, loss: 0.201 \n",
            "epoch: 96, acc: 0.929, loss: 0.257 (data_loss: 0.224, reg_loss: 0.033), lr: 0.004999760 validation, acc: 0.917, loss: 0.252 Unseen, acc: 0.912, loss: 0.259 \n",
            "epoch: 97, acc: 0.768, loss: 1.483 (data_loss: 1.450, reg_loss: 0.033), lr: 0.004999758 validation, acc: 0.871, loss: 0.798 Unseen, acc: 0.864, loss: 0.871 \n",
            "epoch: 98, acc: 0.882, loss: 0.819 (data_loss: 0.786, reg_loss: 0.033), lr: 0.004999755 validation, acc: 0.911, loss: 0.457 Unseen, acc: 0.903, loss: 0.485 \n",
            "epoch: 99, acc: 0.952, loss: 0.171 (data_loss: 0.138, reg_loss: 0.033), lr: 0.004999753 validation, acc: 0.973, loss: 0.105 Unseen, acc: 0.965, loss: 0.121 \n",
            "epoch: 100, acc: 0.972, loss: 0.126 (data_loss: 0.093, reg_loss: 0.033), lr: 0.004999750 validation, acc: 0.980, loss: 0.090 Unseen, acc: 0.974, loss: 0.102 \n",
            "epoch: 101, acc: 0.979, loss: 0.112 (data_loss: 0.079, reg_loss: 0.033), lr: 0.004999748 validation, acc: 0.973, loss: 0.094 Unseen, acc: 0.971, loss: 0.106 \n",
            "epoch: 102, acc: 0.970, loss: 0.133 (data_loss: 0.099, reg_loss: 0.034), lr: 0.004999745 validation, acc: 0.973, loss: 0.104 Unseen, acc: 0.971, loss: 0.117 \n",
            "epoch: 103, acc: 0.973, loss: 0.136 (data_loss: 0.102, reg_loss: 0.034), lr: 0.004999743 validation, acc: 0.976, loss: 0.106 Unseen, acc: 0.974, loss: 0.119 \n",
            "epoch: 104, acc: 0.946, loss: 0.180 (data_loss: 0.146, reg_loss: 0.034), lr: 0.004999740 validation, acc: 0.982, loss: 0.094 Unseen, acc: 0.979, loss: 0.105 \n",
            "epoch: 105, acc: 0.955, loss: 0.167 (data_loss: 0.133, reg_loss: 0.034), lr: 0.004999738 validation, acc: 0.978, loss: 0.104 Unseen, acc: 0.979, loss: 0.109 \n",
            "epoch: 106, acc: 0.977, loss: 0.153 (data_loss: 0.119, reg_loss: 0.034), lr: 0.004999735 validation, acc: 0.974, loss: 0.110 Unseen, acc: 0.974, loss: 0.116 \n",
            "epoch: 107, acc: 0.979, loss: 0.123 (data_loss: 0.089, reg_loss: 0.034), lr: 0.004999733 validation, acc: 0.981, loss: 0.085 Unseen, acc: 0.981, loss: 0.092 \n",
            "epoch: 108, acc: 0.984, loss: 0.104 (data_loss: 0.070, reg_loss: 0.034), lr: 0.004999730 validation, acc: 0.988, loss: 0.064 Unseen, acc: 0.989, loss: 0.071 \n",
            "epoch: 109, acc: 0.988, loss: 0.090 (data_loss: 0.056, reg_loss: 0.034), lr: 0.004999728 validation, acc: 0.993, loss: 0.054 Unseen, acc: 0.991, loss: 0.059 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNnDUP_U8Xn",
        "outputId": "ccd41e71-194a-41d9-e618-616dc9f5d090"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7040374999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbXMisqQKqF",
        "outputId": "f0d4d4d8-b7d4-4a2b-c131-1ddb7e9da980"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 69 epochs\n",
            "Model hit 85% validation accuracy in 70 epochs\n",
            "Model hit 90% validation accuracy in 72 epochs\n",
            "Model hit 95% validation accuracy in 73 epochs\n",
            "Model hit 97.5% validation accuracy in 75 epochs\n",
            "Max accuracy was 99.3125% at epoch 109.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwSXsZVU8Xo",
        "outputId": "65ff4794-01cd-4f03-e7e6-af89de319c14"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "index = 27\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {fashion_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {fashion_mnist_labels[y_test[index]]}')\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.457% Confident True is Sneaker. True is actually Sneaker\n",
            "Accuracy: 0.993, loss: 0.054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k0Ve2M0bPG3"
      },
      "source": [
        "training_diff = []\n",
        "testing_diff = []\n",
        "combined_diff = []"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByL_RwvlIx3"
      },
      "source": [
        "Individual Training Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTOnqnDXa0ME",
        "outputId": "12691657-0c9e-43b6-93c2-0fb90a49dce5"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x, sorted_y)):\n",
        "  accuracy = Accuracy_Categorical()\n",
        "\n",
        "  y = sorted_y[y_sorted_lists]\n",
        "  X = sorted_x[X_sorted_lists]\n",
        "  accuracy.init(y)\n",
        "\n",
        "  dense1.forward(X)\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "  train_train_mean = activation1.output\n",
        "\n",
        "  dropout1.infrence(activation1.output,y)\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, y)\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, y)\n",
        "  print(f'{fashion_mnist_labels[classes]} Train Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')\n",
        "\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T-shirt/top Train Accuracy: 0.997, loss: 0.023\n",
            "Trouser Train Accuracy: 0.996, loss: 0.016\n",
            "Pullover Train Accuracy: 1.000, loss: 0.004\n",
            "Dress Train Accuracy: 0.998, loss: 0.117\n",
            "Coat Train Accuracy: 1.000, loss: 0.076\n",
            "Sandal Train Accuracy: 1.000, loss: 0.001\n",
            "Shirt Train Accuracy: 0.996, loss: 0.053\n",
            "Sneaker Train Accuracy: 1.000, loss: 0.006\n",
            "Bag Train Accuracy: 0.959, loss: 0.197\n",
            "Ankle boot Train Accuracy: 0.986, loss: 0.055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scjb7Wh_sn6b",
        "outputId": "7a521669-042c-4312-afd1-321181f8b4e8"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x_val, sorted_y_val)):\n",
        "  accuracy.init(sorted_y_val[y_sorted_lists])\n",
        "  #print(sorted_y[y_sorted_lists].shape)\n",
        "  #print(sorted_x[X_sorted_lists].shape)\n",
        "  dense1.forward(sorted_x_val[X_sorted_lists])\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  testmean = np.mean(activation1.output, axis=0)\n",
        "  testing_diff.append(testmean)\n",
        "  dropout1.infrence(activation1.output,sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  print(f'{fashion_mnist_labels[classes]} Test Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T-shirt/top Test Accuracy: 0.994, loss: 0.025\n",
            "Trouser Test Accuracy: 0.999, loss: 0.013\n",
            "Pullover Test Accuracy: 0.998, loss: 0.010\n",
            "Dress Test Accuracy: 0.994, loss: 0.126\n",
            "Coat Test Accuracy: 0.995, loss: 0.080\n",
            "Sandal Test Accuracy: 1.000, loss: 0.002\n",
            "Shirt Test Accuracy: 0.998, loss: 0.048\n",
            "Sneaker Test Accuracy: 1.000, loss: 0.005\n",
            "Bag Test Accuracy: 0.944, loss: 0.223\n",
            "Ankle boot Test Accuracy: 0.989, loss: 0.057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u-O8oNZ0qA"
      },
      "source": [
        "# Full mnist test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbD4KrLMnTcR"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMfBGUHeZ4L5",
        "outputId": "1088ce3f-dc39-4dba-ee76-28aa76b8ea38"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(label)\n",
        "\n",
        "dense1.forward(input)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,label)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, label)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, label)\n",
        "\n",
        "print(f'Found {input.shape[0]} images belonging to {len(set(label))} unique classes')\n",
        "\n",
        "print(f'Full Training Accuracy: {testaccuracy:.5f}, loss: {loss:.3f}')"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 60000 images belonging to 10 unique classes\n",
            "Full Training Accuracy: 0.99162, loss: 0.059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aat-uVF6nYu7"
      },
      "source": [
        "Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hyU1tBDna8x",
        "outputId": "0d38456a-ea2d-49a2-ba9a-1a496588e78b"
      },
      "source": [
        "\n",
        "(X, y), (X_val, y_val) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_val = (X_val.reshape(X_val.shape[0], -1).astype(np.float32) - 127.5) / 127.5 # Reshape X_val if cell below was already ran\n",
        "\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_val)\n",
        "\n",
        "dense1.forward(X_val)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dropout1.infrence(activation1.output,y_val)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_val)\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_val)\n",
        "\n",
        "print(f'Found {X_val.shape[0]} images belonging to {len(set(y_val))} unique classes')\n",
        "print(f'Full Testing Accuracy: {testaccuracy:.5f}, loss: {loss:.3f}')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 10 unique classes\n",
            "Full Testing Accuracy: 0.99110, loss: 0.059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbIMZ7Pk_Tnp"
      },
      "source": [
        "Change idex to get confidence of different samples of testing data. Index values 0-1600 were refrenced in training. Anything past was never seen during training. Lowest confidence is at index 2732 when trained with 488 epochs and numpy seed set to 22."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "JaxWcRIr_BCV",
        "outputId": "25a84c43-ae28-474e-e2a8-b64a4f4b041c"
      },
      "source": [
        "index = 6619\n",
        "\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {fashion_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {fashion_mnist_labels[y_val[index]]}')\n",
        "\n",
        "X_val.resize(X_val.shape[0],28,28)\n",
        "image = X_val[index]\n",
        "fig = plt.figure\n",
        "plt.title(f'{fashion_mnist_labels[y_val[index]]}')\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18.183% Confident True is Bag. True is actually Bag\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWVklEQVR4nO3de4yc5XXH8e8J2Nis8XVde20cY4yRcFBMKsetGgulophLlBj+gbgVcaMIp21A0BCFKFUBNUIiVS6NqpLGaVAcAklzDxWIBlBUlD8gMdS1je1ycZbYZu31/YLXAZvTP3bcbsm+5yzz7s5M8vw+kuX1nHlmnn1njudy3vM85u6IyO++t7V7AiLSGkp2kUIo2UUKoWQXKYSSXaQQSnaRQijZRQqhZC+QmfWa2YCZHTOzg2b2sJnNa/e8ZGwp2cv1fnefBPQAe4B/bPN8ZIwp2Qvn7ieA7wGLAczsfWb2n2Z2xMx2mNldQ69vZh8ys5fNbL+Z/W3jXcKftGHq8hYp2QtnZmcD1wNPNS56FfgQMBV4H/CXZnZN47qLgXuBP2PwHcEUYG6r5yzNMZ0bXx4z6wW6gZNAF7AXuMLdNw1z3X8A3N3/2szuAC5y91WN2NnAIeBqd3+8VfOX5uiVvVzXuPtUYAJwE/AfZjbbzP7AzH5qZnvN7DDwFwz+xwAwB9hx+gbc/Tiwv9UTl+Yo2Qvn7qfc/QfAKWA58CDwEDDP3acA/wxY4+p9wLmnx5rZRGBGa2cszVKyF84GrQSmAVuBc4AD7n7CzJYBfzrk6t8D3m9mf2Rm44G7+L//CKTDKdnL9W9mdgw4AtwNrHb354C/Av7OzI4CdwDfOT2gEb8Z+DaDr/LHgH7g1y2euzRBX9BJ08xsEoNf0C1y91+2ez4S0yu7vCVm9n4zO9vMuoDPAZuA3vbOSkZCyS5v1UrglcafRcAHXW8PfyvobbxIIfTKLlKIM1t5Z2amtxHDeNvb4v9zx48fH8bPOuuspmIAJ06cCOMnT54M42eccUYYf/311ytjr732Wjj2zDPjp2c2tzfeeCOM/65y92HLobWS3cyuBL4EnAH8i7vfU+f2StXV1RXG586NTz9fuHBhZezCCy8Mxz733HNh/NChQ2E8m3tfX19lbNeuXeHYmTNnhvF9+/aF8SNHjoTx0jT9Nt7MzgD+CbiKwY6pVY1GCRHpQHU+sy8DXnT37e7+GoMnWqwcnWmJyGirk+xzGdIUAexkmHZHM1tjZuvNbH2N+xKRmsb8Czp3XwusBX1BJ9JOdV7ZdwFD1y07t3GZiHSgOsn+C2CRmS1odEB9kMHWSBHpQE2/jXf3k2Z2E/DvDJbe7mt0RTUtqzdHNd2onjsaLr300srYqlWrwrEf+MAHwvi0adPC+J49e8J4VI/O6uDTp08P4729vWG8znGfOnVqGM/q7Fu3bg3jUZ09K8v19PSE8RdffDGMf+ITnwjj2WM6Fmp9Znf3R4BHRmkuIjKGdLqsSCGU7CKFULKLFELJLlIIJbtIIZTsIoVo6Uo1ZuZR3ffUqVNjdt/nnXdeGF+3bl0Yj+rRWa05axPN+rqz29+7d29lLOv5njhxYhgfN25cGN+/P94jIrr/KVOmhGOz8w+yOrxZ9SrX2e+V5UU2t4GBgTAenQPwy1/Ga3d++MMfDuNV/ex6ZRcphJJdpBBKdpFCKNlFCqFkFymEkl2kEC0vvUXxK664IhwflVoefvjhcGy2Eumzzz4bxqMlmbMyy4QJE2rFs+Weo9Jc9vhmJais7JctVR3d/8GDB8Oxmey41Wn9zeK//nW8l+Xhw4fDeH9/f2UsaqcGuO222ypjTz31FIcPH1bpTaRkSnaRQijZRQqhZBcphJJdpBBKdpFCKNlFCtHSLZszt99+exj/+c9/XhnL6ppHjx4N49nSvvPmzauMZfXixYvj/S6zJbSzdsqoJpxtWxy1gUJep8/q0dF201n7bdb6m8VfffXVylhWJ8+O26ZNm8L4tm3bwnjUxnrBBReEYz/+8Y9Xxm6++ebKmF7ZRQqhZBcphJJdpBBKdpFCKNlFCqFkFymEkl2kEB1VZ89q3YsWLaqM/ehHPwrHbtmyJYxnNd9ofLbtcbZEdtYzntWEozp9ttxyVifPjkt2jsCOHTsqY9m5Edlx6evrC+PROgDZGgHR+QGQn59w/vnnh/Fly5ZVxhYsWBCOjZYOD58L4a0mzKwXOAqcAk66+9I6tyciY2c0Xtn/2N3jZWBEpO30mV2kEHWT3YGfmNkzZrZmuCuY2RozW29m62vel4jUUPdt/HJ332Vmvwc8Zmbb3P3JoVdw97XAWsgXnBSRsVPrld3ddzX+7gd+CFR/xSgibdV0sptZl5mdc/pnYAWwebQmJiKjq87b+FnADxv1xjOBB9390WjA9OnTueqqqyrjWU33He94R2Wsbl00642OaukzZ84Mx2Y94Vk9uc4a59l9Z33b2dyyWvnOnTubvu3JkyeH8WzL52g76pdeeikcm53bUGeNAYifT9ltHzt2rDI2JnV2d98OLGl2vIi0lkpvIoVQsosUQskuUgglu0ghlOwihWhpi2t3dzc33nhjZfziiy8Ox0+dOrUyduDAgXBsVmLKWjWj8bt37w7HZvFs2+M6JaisJJmVO7PyWHZco22Vo6WeAbZv317rvqOyY9biGrWRQr6UdNb2PH/+/MpYnW22oxKzXtlFCqFkFymEkl2kEEp2kUIo2UUKoWQXKYSSXaQQLa2z7969m89+9rOV8W9+85vh+A0bNlTGsm1us3pyVo+OlrHO2mePHz8exrPlnrNWzqgOH9W5Ia83Z3X2GTNmhPGo/Ters2fnTmTbcEeP+Zw5c8KxWYtr1trb3d0dxi+88MKm7zs6J0R1dhFRsouUQskuUgglu0ghlOwihVCyixRCyS5SiJbW2QcGBti4cWNlPOsBjmq6hw4dCsdmPeFnn312GP/ud79bGbv33nvDsVl83754X8zsHIFItqRxdo5AJjtHINquOrvvrMaf9X1H9ersmGa/V7b0eJ1e+0x0Tojq7CKiZBcphZJdpBBKdpFCKNlFCqFkFymEkl2kEC2ts7/++uvs2rVrTG77gQceCOM33HBDGO/r6wvjK1asqIz19vaGY3t6esJ4Vm/OauXN9jdDXg/Oxmf16DqyWnTWix/Vo7PbjrZ7hngPA8jP2+jq6qqMRVsyQ7xufCR9ZTez+8ys38w2D7lsupk9ZmYvNP6ON5QWkbYbydv4rwNXvumyTwFPuPsi4InGv0Wkg6XJ7u5PAm9eH2glsK7x8zrgmlGel4iMsmY/cM1y99MfcncDs6quaGZrgDVN3o+IjJLa3664u5tZ5bcd7r4WWAsQXU9Exlazpbc9ZtYD0Pi7f/SmJCJjodlkfwhY3fh5NfDj0ZmOiIyV9G28mX0LeC/QbWY7gTuBe4DvmNlHgJeB68ZykqdF9eZs3fdsD/T169eH8eXLl1fGvvKVr4Rjs1r1008/HcYXL14cxqOab9RPDnm9uW7PeVQLz9ZHzx7TbN35qOc8W98gOy779+8P49lxi9ZfyGr0UR5E95smu7uvqghdlo0Vkc6h02VFCqFkFymEkl2kEEp2kUIo2UUK0dIW17qiskJWKqmzHDPEbYfPP/98OHb27NlhPCvj9PfH5yxFJaysHTIrf2XHNVtSeWBgoDKWlc6isZBvhZ2Nr3PbWdtytiV0tB31pEmTwrFROTV6vPTKLlIIJbtIIZTsIoVQsosUQskuUgglu0ghlOwihfitqrNHbYlHjx4Nx2YtrhMmTAjjUW0za8WMlnqGfKnobG7R/WdLRWdzy+rs2VLSUc04+72zWvc555wTxqNW0cOHD4dj58+fH8ZnzpwZxrNzCKLzH6JlpiFuj40eb72yixRCyS5SCCW7SCGU7CKFULKLFELJLlIIJbtIIVpaZx83blxYn8yWJa6zzW1WT86W74166bNadlbjnzJlShjPbj86bnWXis6Woq6zTkB2fkK23HO2ZXOdPv/suZitAxD1q0P8nKh77kPl7TY1SkR+6yjZRQqhZBcphJJdpBBKdpFCKNlFCqFkFylES+vs48eP5+1vf3tlPOuNjvqfs/7hrJ6c1XSjWvfEiRPDsVnfdSarq0b16qxeXLcOnz1m0e1nc8vq8FmdPZr7vn37wrHZ3LJa+MGDB8P4ueeeWxnLzgGInuu1+tnN7D4z6zezzUMuu8vMdpnZhsafq7PbEZH2Gsnb+K8DVw5z+Rfd/ZLGn0dGd1oiMtrSZHf3J4H43D8R6Xh1vqC7ycw2Nt7mT6u6kpmtMbP1ZrY++xwkImOn2WT/MrAQuAToAz5fdUV3X+vuS919afbFg4iMnaaS3d33uPspd38D+CqwbHSnJSKjralkN7Oh+9VeC2yuuq6IdIa0zm5m3wLeC3Sb2U7gTuC9ZnYJ4EAv8NGR3NnEiRNZsmRJdF/h+KjnPOsvzmRrdUd11Wx/9awnPPsuI1tfPdojPaujZ/Gslz57zKK51+lHh/wxi+47O/chO+bZ+gnZGgbRug7ZMW+2nz1NdndfNczFX2vq3kSkbXS6rEghlOwihVCyixRCyS5SCCW7SCFa2uI6efJkLrvssqbHT58+vTKWtbhmsjJPVGLasWNHOLa3tzeMZ9sD1znzMGvFzJaCzspAdWTtsVlZLytZRsdt4cKFtW67v78/jGfbbEeluaxUGz1XtWWziCjZRUqhZBcphJJdpBBKdpFCKNlFCqFkFylEy7dsnj17dmX88OHD4fhoa+Os5TCT1UV7enrCeCRbanrSpElhPJtb1o4ZyerwWTyrw0fxbN7Zccnq9AMDA5WxZttET8uWuc4cP368MlZ3C+8qemUXKYSSXaQQSnaRQijZRQqhZBcphJJdpBBKdpFCtLTO7u5hjTDr277ooosqY88880zT84J8i92ovzmrRUfb80Jeh6+znHPWG531jGe/W/aYRfcfLYENeR0++92i8xOy+86Wmo6WNQfYvXt3GI+eT9nz4cCB6q0Xo/UJ9MouUgglu0ghlOwihVCyixRCyS5SCCW7SCGU7CKFGMmWzfOAbwCzGNyiea27f8nMpgP/CpzH4LbN17l7WKw+depUuL1xVk9+5zvfWRl7/PHHw7GZrN4cza27uzscm9Vks77sOmu7Z7Xq7PfOatlZX3hUp88e72xL52x8VEvPzqvI6uTZYxat2wBxnX3y5Mnh2G3btlXGomM2klf2k8Bt7r4Y+EPgY2a2GPgU8IS7LwKeaPxbRDpUmuzu3ufuzzZ+PgpsBeYCK4F1jautA64Zq0mKSH1v6TO7mZ0HvAt4Gpjl7n2N0G4G3+aLSIcacbKb2STg+8Ct7n5kaMwHP7gN++HNzNaY2XozW5+tMSciY2dEyW5m4xhM9Afc/QeNi/eYWU8j3gMMu9Odu69196XuvjRbSE9Exk6a7Db4de3XgK3u/oUhoYeA1Y2fVwM/Hv3pichoGUmL63uAG4BNZrahcdmngXuA75jZR4CXgeuyGzpy5EhYIluxYkU4fsmSJSOY7vCyEtK0adPC+Jw5cypj2XbRR48eDeNdXV1hPGszzcpAkax0lsWzrY2j0l/d9tk6Zb9o+++R3Hb2kXTnzp1hfN68eZWxqDwNsH379spYtJ1z+ixx958BVcXY5jdbF5GW0hl0IoVQsosUQskuUgglu0ghlOwihVCyixSipUtJHzhwgAcffLAyfscdd4Tjo9rkZz7zmXDsr371qzCe1YtfeeWVythZZ50Vjs3aHetuyRy1wEZ1Vxj7LZuj45rVsrNzI7J4NPdsqehsu+iszt7b2xvGo7ll20HPmDGjMhadc6FXdpFCKNlFCqFkFymEkl2kEEp2kUIo2UUKoWQXKURL6+ynTp0K65P3339/OP7WW2+tjF1++eXh2BdeeCGMZ3XXqJ48a1a8/N7UqVPDeLacc7a9cFRnz2q22fkFdbaLzuJ1t5POxh8/frzpsdlxy8ydOzeMDwwMVMayFZ0effTRyliUX3plFymEkl2kEEp2kUIo2UUKoWQXKYSSXaQQSnaRQrS0zp755Cc/Gca3bNlSGbvzzjvDsVldNRP1CUf9xdlYyHvOM1FvdN1adiYbH639nh2XrNZdJ54d82yNgky2l0D0mB04cCAcu3nz5qbmpFd2kUIo2UUKoWQXKYSSXaQQSnaRQijZRQqhZBcphGVrd5vZPOAbwCzAgbXu/iUzuwu4EdjbuOqn3f2R5LbiOxtDd999dxi//vrrw/ixY8cqY9ne7idOnAjjmTo95dm67+1Ut1e+zv7t2fkHWR0+O0cg+92iOnz2mL373e+ujLk77j7sgRvJSTUngdvc/VkzOwd4xswea8S+6O6fG8FtiEibpcnu7n1AX+Pno2a2FYiX4RCRjvOW3uOZ2XnAu4CnGxfdZGYbzew+Mxv2vayZrTGz9Wa2vtZMRaSWESe7mU0Cvg/c6u5HgC8DC4FLGHzl//xw49x9rbsvdfelozBfEWnSiJLdzMYxmOgPuPsPANx9j7ufcvc3gK8Cy8ZumiJSV5rsNviV6NeAre7+hSGX9wy52rVAc604ItISI/k2/j3ADcAmM9vQuOzTwCozu4TBclwv8NExmeEQUdthVirp7u4O4wsWLAjj27dvr4x1dXWFYydOnFgrnt1+3TbVEtUpjUG8TDXk5dZoefE5c+aEY7NyeZWRfBv/M2C4Z1NYUxeRztK5Z1yIyKhSsosUQskuUgglu0ghlOwihVCyixSi5UtJR+17We0z27o4csstt4TxCy64IIzPnDmzMnbkyJFwbFZz7e/vrxU/dOhQZSxrl4y2Dob6bah1ZPXkrMV18uTJlbFsqeisbTnbkjmr00fPiWuvvTYc2yy9sosUQskuUgglu0ghlOwihVCyixRCyS5SCCW7SCHSpaRH9c7M9gIvD7moG9jXsgm8NZ06t06dF2huzRrNuc1392FPCmlpsv/GnZut79S16Tp1bp06L9DcmtWqueltvEghlOwihWh3sq9t8/1HOnVunTov0Nya1ZK5tfUzu4i0Trtf2UWkRZTsIoVoS7Kb2ZVm9t9m9qKZfaodc6hiZr1mtsnMNrR7f7rGHnr9ZrZ5yGXTzewxM3uh8XfceN3aud1lZrsax26DmV3dprnNM7OfmtkWM3vOzG5pXN7WYxfMqyXHreWf2c3sDOB54HJgJ/ALYJW7b2npRCqYWS+w1N3bfgKGmV0KHAO+4e4XNy77e+CAu9/T+I9ymrvf3iFzuws41u5tvBu7FfUM3WYcuAb4c9p47IJ5XUcLjls7XtmXAS+6+3Z3fw34NrCyDfPoeO7+JHDgTRevBNY1fl7H4JOl5Srm1hHcvc/dn238fBQ4vc14W49dMK+WaEeyzwV2DPn3Tjprv3cHfmJmz5jZmnZPZhiz3L2v8fNuYFY7JzOMdBvvVnrTNuMdc+ya2f68Ln1B95uWu/vvA1cBH2u8Xe1IPvgZrJNqpyPaxrtVhtlm/H+189g1u/15Xe1I9l3AvCH/PrdxWUdw912Nv/uBH9J5W1HvOb2DbuPveDXKFuqkbbyH22acDjh27dz+vB3J/gtgkZktMLPxwAeBh9owj99gZl2NL04wsy5gBZ23FfVDwOrGz6uBH7dxLv9Pp2zjXbXNOG0+dm3f/tzdW/4HuJrBb+RfAv6mHXOomNf5wH81/jzX7rkB32Lwbd3rDH638RFgBvAE8ALwODC9g+Z2P7AJ2MhgYvW0aW7LGXyLvhHY0PhzdbuPXTCvlhw3nS4rUgh9QSdSCCW7SCGU7CKFULKLFELJLlIIJbtIIZTsIoX4H5DQJ0d4QUriAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6KeoLkg7k0u",
        "outputId": "374c5e20-1d2a-4aa4-b802-1ce7c96a9b3d"
      },
      "source": [
        "confidence_list = []\n",
        "for index in range(10000):\n",
        "  confidence_list.append(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0])\n",
        "\n",
        "print(confidence_list.index(min(confidence_list)))\n",
        "\n",
        "a = confidence_list[:] \n",
        "a.sort()\n",
        "print(confidence_list.index(a[1]))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6619\n",
            "6619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "h5c5xUTNXk2v",
        "outputId": "ac66bf5d-0599-4afd-9eea-5ba915b21f79"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3wUdf7H8dcnIfTeu/QmTQkdgoVqQ8GCemcXK1LO86xXPO9OzztQFAtW9OyIgEi3EEBBgvQmRRAQBKT39vn9scv9IpeEQLKZ7Ob9fDz2QXZmdvYzGc175zuznzF3R0REJD1xQRcgIiK5m4JCREQypKAQEZEMKShERCRDCgoREcmQgkJERDKkoBDJRmbW0cxWBF2HSHZSUEjMMLO1ZtY5yBrcfbq714/Eus3sKzM7aGZ7zWybmY0ys0qZfO15ZrYhEnVJ7FNQiJwGM4sPuIR73b0oUAcoCvwr4HokD1BQSMwzszgze9DMVpvZL2b2oZmVTjX/IzPbbGa7zCzZzM5ONe9NM3vRzMab2T7g/PCRy/1mtjD8mg/MrGB4+V99cs9o2fD8B8xsk5n9ZGa3mZmbWZ1TbZO77wRGA81TretmM1tmZnvMbI2Z3RGeXgSYAFQOH43sNbPKp/q9iJygoJC8oB9wOdAJqAzsAIalmj8BqAuUB74D3jnp9dcBfwOKATPC064GugM1gabATRm8f5rLmll3YBDQmdARwnmZ3SAzKwP0AlalmrwFuAQoDtwMDDGzc919H9AD+Mndi4YfP3Hq34sIEMNBYWavm9kWM1ucDes638zmp3ocNLPLM/na88KfJE+89o/ZUM+dZrYovL4ZZtYoq+uMcXcCj7j7Bnc/BPwZuNLM8gG4++vuvifVvGZmViLV68e4+0x3P+7uB8PThrr7T+6+HfiUVJ/s05DeslcDb7j7EnffH37vUxlqZruAbUBZQn/sCW/HZ+6+2kOmAZOBjhmsK8Pfi8gJMRsUwJuEPsVlmbt/6e7N3b05cAGwn9D/hL9iZmvTWcX0E69398ezoaR33b1JuJ5/AoOzYZ2x7CzgEzPbaWY7gWXAMaCCmcWb2ZPh4ZfdwNrwa8qmev36NNa5OdXP+wmdL0hPestWPmndab3Pye5z9xKEjkxKAVVPzDCzHmY2y8y2h7fzIn69HSdL9/eSiTokD4nZoHD3ZGB76mlmVtvMJprZXDObbmYNzmDVVwITwp8As8TMfmNm34aPDF7O7IlSd9+d6mkRQC2AM7Ye6OHuJVM9Crr7RkLDSj0JDf+UAGqEX2OpXh+p3+8mUv2hB6pl9oXuvgh4AhhmIQWAjwmd3K7g7iWB8fz/dqS1DRn9XkT+K2aDIh3DgX7u3gK4H3jhDNbRB3jvNF/T1swWmNmEEydKzawhcA3QPnxkcAy4PrMrNLN7zGw1oSOK+06znliWYGYFUz3yAS8BfzOzswDMrJyZ9QwvXww4BPwCFAb+noO1fgjcbGYNzaww8Nhpvn4EoU//lwH5gQLAVuComfUAuqZa9megzElDahn9XkT+K88EhZkVBdoBH5nZfOBloFJ4Xi8zW5zGY9JJ66gENAEmpZo27MT5B0JXlZw4F/FIeJHvgLPcvRnwHKErVQAuBFoAc8KvvRCoFV7nW+nUc/eJ93X3Ye5eG/gD8Gj2/rai2njgQKrHn4FngbHAZDPbA8wCWoeXfwtYB2wElobn5Qh3nwAMBb4kdFL6xHsfyuTrDxPatsfcfQ+hDwwfEjopfR2hbT6x7HJCH3DWhIeaKpPx70XkvyyWb1xkZjWAce7e2MyKAyvcPVNfUEpnff2Bs929bzrz17p7jVOsYy2QCFwLVHb3h860nvD64oAd4XFriWLho8zFQAF3Pxp0PSIn5JkjivC4/g9mdhVAeFy32Wmu5lpOc9jJzCqamYV/bkXod/4L8DmhK0zKh+eVPjEEkIl11k319GJg5enUJLmHmV1hZgXMrBTwFPCpQkJym5gNCjN7D/gGqG9mG8zsVkLnAG41swXAEkInMTO7vhqETjZOO81SrgQWh99zKNAnfPniUkJDRpPNbCEwhfBQWCbca2ZLwkNWg4AbT7MmyT3uIPT9h9WEzlPdFWw5Iv8rpoeeREQk62L2iEJERLJHTH4Ds2zZsl6jRo2gyxARiRpz587d5u7l0poXk0FRo0YNUlJSgi5DRCRqmNm69OZp6ElERDKkoBARkQwpKEREJEMKChERyZCCQkREMhRoUJhZdzNbYWarzOzBNOYXsNCtI1eZ2ezwt6NFRCQHBRYU4XsvDCN0i8ZGwLVp3KntVkIN7+oAQwj1whERkRwU5BFFK2CVu68Jt0t+n//tvdSTUM99gJHAhSca7EXC0M9XsmD9zkitXkQkKgUZFFX49a0fN4SnpblMuKPmLqBMWiszs75mlmJmKVu3bj3tYnbuP8y7s3/kihdm8vfxyzhw+Nhpr0NEJBbFzMlsdx/u7onunliuXJrfQs9QycL5mTwoiWtaVmd48hp6PJvMrDW/RKBSEZHoEmRQbOTX9wiuGp6W5jLhW1qWIHQvh4goXjCBf/Rqwru3t+a4Q5/hs3j4k0XsPngkUm8pIpLrBRkUc4C6ZlbTzPITuhf12JOWGcv/32vhSuALz4G+6O1ql2XSgCRu71iT97/9ka6Dk/l82c+RflsRkVwpsKAIn3O4l9D9p5cBH7r7EjN73MwuCy/2GqEbwq8idIOe/7mENlIK5Y/nkYsbMeru9pQolMCtI1Lo//48ftmbqdsZi4jEjJi8cVFiYqJnZ/fYw0eP8+JXq3n+y5UUK5jAny5txGXNKhPBC7BERHKUmc1198S05sXMyexIyp8vjv6d6zKuX0eqlS5M//fnc9uIFDbtOhB0aSIiEaegOA31KxZj1F3tePTihsxcvY2ug5N5d/aPHD8ee0dlIiInKChOU3yccVvHWkwakETjKiV4+JNFXPfqLNZu2xd0aSIiEaGgOENnlSnCu7e35sleTViycTfdn03mleQ1HD12POjSRESylYIiC8yMPq2qM2VQJzrUKcffxi+j94tfs3zz7qBLExHJNgqKbFCxREFeuaEFz117Dht2HOCSoTMYPOV7Dh1VGxARiX4KimxiZlzarDJTBnXi0maVGfr5Si59bgbzftwRdGkiIlmioMhmpYvkZ8g1zXn9pkT2HDxKrxe/5q/jlqrJoIhELQVFhFzQoAKTByZxfevqvDbjB7o9k8zXq7YFXZaIyGlTUERQsYIJPHF5E97v24Y4g+tenc2DHy9k1wE1GRSR6KGgyAFtapVh4oAk7uhUiw9T1tN1yDSmLFWTQRGJDgqKHFIwIZ6HejRk9D3tKVU4P7e/lcK9737HNjUZFJFcTkGRw5pWLcmn/Trwuy71mLzkZzoPnsYn8zYQi80ZRSQ2KCgCkBAfR78L6zK+fwdqlS3CwA8WcPObc9i4U00GRST3UVAEqE75Ynx0Zzv+dGkjZq/ZTtfB03h71jo1GRSRXEVBEbD4OOPm9jWZPDCJc6qX4rHRi+nzyix+UJNBEcklFBS5RLXShXn71lb8s3dTlm/aTfdnknlp2mo1GRSRwCkochEz4+qW1Zg6qBPn1S/HkxOWc8ULX7P0JzUZFJHgKChyofLFC/LSb1ow7Lpz2bTrAJc9P4N/T16hJoMiEggFRS5lZlzctBJTBnbisuaVee6LVVw8dAZz16nJoIjkLAVFLleqSH4GX92cN29uyYHDx7jypa/5y6dL2H/4aNCliUgeoaCIEufVL8+kgUn8ts1ZvDFzLV2HJDNjpZoMikjkKSiiSNEC+Xi8Z2M+vKMt+ePj+M1rs3lg5AJ27VeTQRGJHAVFFGpVszTj+3fk7vNq8/F3G+k8ZBoTF28OuiwRiVEKiihVMCGeB7o3YMw97SlXtAB3/mcud78zly17DgZdmojEGAVFlGtcpQRj7m3P77vVZ+qyLXQZnMzIuWoyKCLZR0ERAxLi47jn/DqMv68jdcsX5f6PFnDjG3PYsGN/0KWJSAwIJCjMrLSZTTGzleF/S6Wz3DEzmx9+jM3pOqNNnfJF+fCOtvzlsrNJWbudrkOSGfH1WjUZFJEsCeqI4kHgc3evC3wefp6WA+7ePPy4LOfKi15xccaN7WoweWASLc4qxZ/GLuHql79h9da9QZcmIlEqqKDoCYwI/zwCuDygOmJW1VKFeeuWVjx9ZVNWbtlLj2en88JXqziiJoMicpqCCooK7r4p/PNmoEI6yxU0sxQzm2VmCpPTZGZclViNKYOSuLBBef45cQWXD5vJ4o27gi5NRKKIRerqGDObClRMY9YjwAh3L5lq2R3u/j/nKcysirtvNLNawBfAhe6+Op336wv0BahevXqLdevWZcdmxJSJizfx6Ogl7Nh/mDuSanHfhXUpmBAfdFkikguY2Vx3T0xzXhCXUZrZCuA8d99kZpWAr9y9/ile8yYwzt1Hnmr9iYmJnpKSkj3Fxphd+4/wxGdL+WjuBmqVK8JTvZvSskbpoMsSkYBlFBRBDT2NBW4M/3wjMObkBcyslJkVCP9cFmgPLM2xCmNUicIJPH1VM966pRWHjhznqpe+4Y9jFrP3kJoMikjaggqKJ4EuZrYS6Bx+jpklmtmr4WUaAilmtgD4EnjS3RUU2SSpXjkmD0zipnY1eHvWOroNSWba91uDLktEcqFAhp4iTUNPpydl7Xb+8PFCVm/dR+9zq/LYJQ0pWTh/0GWJSA7KjUNPkosk1ijNZ/d15N7z6zBm/kY6D57G+EWbTv1CEckTFBQChJoM3t+tPmPubU/FEgW5+53vuOPtFLbsVpNBkbxOQSG/cnblEoy+uz1/6N6AL1dspfPgaXw4Z72aDIrkYQoK+R/54uO467zaTOzfkQYVi/PAxwu54fVvWb9dTQZF8iIFhaSrVrmivN+3DX/teTbfrdtBt2eSeWPmD2oyKJLHKCgkQ3Fxxm/b1mDyoE60rFGav3y6lKte/oZVW/YEXZqI5BAFhWRKlZKFePPmlgy+uhmrt+7lomdn8PwXK9VkUCQPUFBIppkZvc6typSBnehydgX+Nfl7Lnt+Jos2qMmgSCxTUMhpK1esAMOuO5eXf9uCX/Ye4vIXZvKPCcs4eORY0KWJSAQoKOSMdTu7IlMGdeLKc6vy8rQ19Hh2OrPX/BJ0WSKSzRQUkiUlCiXw1JVNeee21hw9fpxrhs/i0dGL2HPwSNCliUg2UVBItmhfpyyTBiRxa4eavDP7R7oNSebLFVuCLktEsoGCQrJN4fz5eOySRnx8VzuKFMjHzW/MYeAH89m+73DQpYlIFigoJNudW70U4+7rwH0X1OHTBT/RZfA0Pl3wk9qAiEQpBYVERIF88QzqWp9P+3WgSqlC9HtvHn3fnsvPajIoEnUUFBJRDSsVZ9Rd7Xj4ogYkfx9qMvj+tz/q6EIkiigoJOLyxcfRN6k2kwYk0ahScR4ctYjrX53Nj7+oyaBINFBQSI6pUbYI793ehr9d0ZiFG3bR9ZlpvDp9DcfUZFAkV1NQSI6KizOub30WUwYl0a52WZ74bBm9X/ya739Wk0GR3EpBIYGoVKIQr92YyLN9mrPul31cPHQ6z05dyeGjajIoktsoKCQwZkbP5lWYOqgT3RtXYsjU77ns+RksWL8z6NJEJBUFhQSuTNECPHftObx6QyI79x/hihdm8vfxyzhwWE0GRXIDBYXkGp0bVWDyoCSuaVmd4clr6P5sMt+sVpNBkaApKCRXKV4wgX/0asK7t7cG4NpXZvHQqEXsVpNBkcAoKCRXale7LBP7J9E3qRYfzPmRroOT+XzZz0GXJZInnTIozKyemX1uZovDz5ua2aORL03yukL543n4ooaMurs9JQolcOuIFPq/P49f9h4KujSRPCUzRxSvAA8BRwDcfSHQJ5JFiaTWvFpJPu3XgQGd6zJ+0Sa6DElmzPyNagMikkMyExSF3f3bk6YdjUQxIunJny+OAZ3rMa5fR6qVLkz/9+dz+1spbNp1IOjSRGJeZoJim5nVBhzAzK4ENmXlTc3sKjNbYmbHzSwxg+W6m9kKM1tlZg9m5T0lNtSvWIxRd7Xj0YsbMmPVNroOTubd2T9yXG1ARCImM0FxD/Ay0MDMNgIDgDuz+L6LgV5AcnoLmFk8MAzoATQCrjWzRll8X4kB8XHGbR1rMWlAEk2qluDhTxZx3auzWLttX9ClicSkzASFu3tnoBzQwN07ZPJ1Ga1wmbuvOMVirYBV7r7G3Q8D7wM9s/K+ElvOKlOEd25rzZO9mrBk4266PZPM8OTVHD2mNiAi2Skzf/A/BnD3fe5+onPbyMiV9F9VgPWpnm8IT0uTmfU1sxQzS9m6dWvEi5Pcwczo06o6UwZ1omPdcvx9/HJ6v/g1yzfvDro0kZiRL70ZZtYAOBsoYWa9Us0qDhQ81YrNbCpQMY1Zj7j7mNMt9FTcfTgwHCAxMVED1nlMxRIFeeWGFoxbuIk/j13Cpc/N4O7z6nD3+bUpkC8+6PJEolq6QQHUBy4BSgKXppq+B7j9VCsOD1dlxUagWqrnVcPTRNJkZlzarDLt65Tlr+OW8uznK5mweBNP9W7KOdVLBV2eSNSyU12LbmZt3f2biLy52VfA/e6eksa8fMD3wIWEAmIOcJ27LznVehMTEz0l5X9WKXnMl8u38PAni9i8+yC3tK/J77rWo3D+jD4bieRdZjbX3dO8CjUz5yjmmdk9ZvaCmb1+4pHFgq4wsw1AW+AzM5sUnl7ZzMYDuPtR4F5gErAM+DAzISFywvkNyjN5YBLXt67OazN+oNszycxctS3oskSiTmaOKD4ClgPXAY8D1wPL3L1/5Ms7MzqikJPNXvMLD45axA/b9tGnZTUeuqghJQolBF2WSK6R1SOKOu7+GLDP3UcAFwOts7NAkUhrXasME/p35I5OtfgwZT1dBk9j8pLNQZclEhUyExQn+jvvNLPGQAmgfORKEomMggnxPNSjIaPvaU/pIvnp+/Zc7n33O7apyaBIhjITFMPNrBTwKDAWWAo8FdGqRCKoadVQk8HfdanH5CU/02XwNEbPU5NBkfSc8hxFmi8yq+7uP0agnmyhcxSSWSt/3sMDHy9k3o87Ob9+Of52RRMqlywUdFkiOe6Mz1GYWVszu9LMyoefNzWzd4GZEahTJMfVrVCMkXe244+XNGLWmu10HZLM27PWqcmgSCrpBoWZPQ28DvQmdAnrE8BkYDZQN2fKE4m8+Djjlg41mTwwiebVSvLY6MX0GT6LNVv3Bl2aSK6Q7tCTmS0FznX3g+FzFOuBxu6+NgfrOyMaepIz5e58NHcDT4xbyqGjxxnYpR63dahJvnjdNVhi25kOPR1094MA7r4DWBkNISGSFWbG1YnVmDqoE+fVL8eTE5Zz+QszWfqTmgxK3pXREcVOfn2/iKTUz939ssiWduZ0RCHZZcKiTTw2Zgk79x/mzk616XdhHTUZlJiU0RFFRo1vTr73w7+zrySR6NCjSSXa1i7D4+OW8vyXq5i4ZDNP9W5Ki7PUZFDyjjO6PDa30xGFRMJXK7bw8KhFbNp9kJva1eD+rvUpUkBNBiU2ZLWFh4gA59Uvz+RBnfhtm7N4Y+Zauj2TzPSVukmWxD4FhchpKFogH4/3bMyHd7Qlf3wcv33tWx4YuYBd+4+c+sUiUUpBIXIGWtUszfj+Hbn7vNp8/N1GOg+ZxsTFajIosSkzbcY/BU5eaBeQArx84hLa3ETnKCQnLd64iwdGLmTppt1c1KQif7msMeWKFQi6LJHTktVzFGuAvcAr4cduQrdDrRd+LpKnNa5SgjH3tuf33eozddkWOg+exsdzN6jJoMSMzBxRzHH3lmlNM7Ml7n52RCs8AzqikKCs2rKXBz9eSMq6HSTVK8ffr2hM1VKFgy5L5JSyekRR1Myqp1pZdaBo+OnhbKhPJGbUKV+UD+9oy18uO5uUtdvpNiSZt75ZqyaDEtUyExS/A2aY2Zdm9hUwHbjfzIoAIyJZnEg0ioszbmxXg8kDk2hRozR/HLOEq1/+hlVb1GRQolOmvnBnZgWABuGnK3LjCezUNPQkuYW7M+q7jTw+bikHjhyj/4V16ZtUiwQ1GZRcJju+cNcCOBtoBlxtZjdkV3EisczM6N2iKlMGJdG5YXmenrSCns/PZPHGXUGXJpJppwwKM3sb+BfQAWgZfqSZOiKStvLFCvLC9S146TfnsnXvIXoOm8k/Jy7n4JFjQZcmckqZaVSTCDRyXesnkmXdG1eiba2yPPHZUl74ajUTl2zmn72bklijdNCliaQrM0NPi4GKkS5EJK8oUTiBp69qxlu3tOLQkeNc9fI3/GnMYvYeOhp0aSJpykxQlAWWmtkkMxt74hHpwkRiXVK9ckwemMSNbWvw1qx1dBuSzLTv1WRQcp/MfOGuU1rT3X1aRCrKBrrqSaLN3HXbeWDkQlZv3Uevc6vwx0saUbJw/qDLkjwko6uedD8KkVzi4JFjPP/FKl6atpqShRN4vGdjejSuiJkFXZrkAWd0eayZzQj/u8fMdqd67DGzLN1A2MyuMrMlZnbczNK9gsrM1prZIjObb2b6yy8xrWBCPPd3q8/YeztQqUQh7n7nO+78z1y27M7VX1uSPCDdoHD3DuF/i7l78VSPYu5ePIvvuxjoxa/vyZ2e8929eXpJJxJrGlUuzid3t+PBHg34asVWOg+exocp69VkUAKTqS/cmVm8mVU2s+onHll5U3df5u4rsrIOkViWLz6OOzvVZkL/jjSoWJwHRi7khte/Zf32/UGXJnlQZr5w1w/4GZgCfBZ+jItwXSc4MNnM5ppZ34wWNLO+ZpZiZilbt+rKEYkNtcoV5f2+bfhrz7P5bt0Ouj2TzBszf+CYmgxKDsrMVU+rgNbu/stprdhsKml//+IRdx8TXuYr4H53T/P8g5lVcfeNZlaeUFD1c/dTDlfpZLbEoo07D/DIJ4v4asVWzq1ekn9e2ZQ65YsFXZbEiKz2elpP6I52p8XdO7t74zQeY05jHRvD/24BPgFanW4dIrGiSslCvHFTSwZf3Yw12/Zx0bMzeP6LlRw5djzo0iTGZaaFxxrgKzP7DDh0YqK7D45YVUC4jXmcu+8J/9wVeDyS7ymS25kZvc6tSlK9cvxp7BL+Nfl7xi3cxNNXNqNJ1RJBlycxKjNHFD8SGvbJDxRL9ThjZnaFmW0A2gKfmdmk8PTKZjY+vFgFQvfBWAB8C3zm7hOz8r4isaJs0QIMu+5chv+2Bdv3HebyF2byjwnL1GRQIiLDcxRmFg+85e7X51xJWadzFJKX7DpwhH+MX8b7c9ZTs2wRnuzVhNa1ygRdlkSZMz5H4e7HgLPMTL0ERHKpEoUSeLJ3U965rTVHjx/nmuGzeHT0IvYcPBJ0aRIjMnuOYma4EeC+ExMjfY5CRE5P+zplmTQgiX9P/p7XZ/7AF8u28LcrmnB+g/JBlyZRLjPnKFYT+t5EHNl0jkJEIqNw/nw8dkkjPr6rHUUK5OPmN+cw8IP5bN93OOjSJIqpKaBIjDp09BjDvlzNC1+uokShBP582dlc0rSSmgxKmrL0PQozK2dmT5vZeDP74sQj+8sUkexUIF88g7rUY9x9HahSqhD93pvH7W/NZfMuNRmU05OZoad3gOVATeAvwFpgTgRrEpFs1KBicUbd1Y6HL2rA9JVb6TJkGu9/+6OaDEqmZSYoyrj7a8ARd5/m7rcAF0S4LhHJRvni4+ibVJtJA5JoVKk4D45axHWvzGbdL/tO/WLJ8zITFCeusdtkZheb2TmA7gQvEoVqlC3Ce7e34W9XNGbRxl10eyaZV6evUZNByVBmguIJMysB/A64H3gVGBjRqkQkYuLijOtbn8WUQUm0q12WJz5bRu8Xv+b7n/cEXZrkUrrqSSQPc3fGLviJv3y6lD0Hj3Dv+XW567za5M+XqVvVSAzJ6lVP9czsczNbHH7e1Mweze4iRSTnmRk9m1dhysAkejSuxJCp33PpczNYsH5n0KVJLpKZjw2vAA8RPlfh7guBPpEsSkRyVpmiBRh67Tm8ekMiuw4c4YoXZvK3z5Zy4LCaDErmgqKwu3970rSjkShGRILVuVEFJg9K4pqW1Xll+g90fzaZr1dvC7osCVhmgmKbmdUmdFtSzOxKYFNEqxKRwBQvmMA/ejXhvdvbAHDdK7N5aNQidqvJYJ6VmaC4B3gZaGBmG4EBwJ0RrUpEAte2dhkm9k+ib1ItPpjzI10HJ/P5sp+DLksCcMqgcPc17t4ZKAc0cPcOwBURr0xEAlcofzwPX9SQT+5uT8nCCdw6IoX73pvHL3sPnfrFEjMyfQ2cu+9z9xMXWg+KUD0ikgs1q1aSsfd2YGDnekxYvIkuQ5IZM3+j2oDkEWd6sbTaT4rkMfnzxdG/c13G9etItdKF6f/+fG4bkcKmXQeCLk0i7EyDQh8jRPKo+hWLMequdjx6cUNmrt5Gl8HJvDN7HcfVBiRmpRsUZrbHzHan8dgDVM7BGkUkl4mPM27rWIvJAzrRtGoJHvlkMde9Oou129RkMBalGxTuXszdi6fxKObumbmFqojEuOplCvPOba15slcTlvy0m27PJDM8eTVHjx0PujTJRmroIiJZYmb0aVWdqYM6kVSvHH8fv5xeL37N8s27gy5NsomCQkSyRYXiBRn+2xY8f905bNxxgEuGzmDwlO85dFRtQKKdgkJEso2ZcUnTykwd1IlLm1Vm6OcrufS5Gcz7cUfQpUkWKChEJNuVKpKfIdc0542bWrLn4FF6vfg1j3+6lP2H1SYuGikoRCRizm9QnskDk7i+dXVen/kD3Z5JZuYqNRmMNgoKEYmoYgUTeOLyJnzQtw354uK4/tXZ/GHkQnYdUJPBaBFIUJjZ02a23MwWmtknZlYyneW6m9kKM1tlZg/mdJ0ikn1a1yrDhP4dubNTbUZ+t4Eug6cxacnmoMuSTAjqiGIK0NjdmwLfE7ox0q+YWTwwDOgBNAKuNbNGOVqliGSrggnxPNijAaPvbk+ZogW44+253PPOd2zdoyaDuVkgQeHuk939xFmtWUDVNBZrBawKd689DLwP9MypGkUkcppULcHYe9tzf9d6TFn6M12GTOOTeRvUZDCXyg3nKG4BJqQxvQqwPqjFU2UAAA4xSURBVNXzDeFpaTKzvmaWYmYpW7duzeYSRSS7JcTHce8FdRnfvwO1yhZh4AcLuOXNOfy0U00Gc5uIBYWZTTWzxWk8eqZa5hFCt1V9J6vv5+7D3T3R3RPLlSuX1dWJSA6pU74YH93Zjj9d2ohZa7bTZfA03p6lJoO5ScR6NoVvdpQuM7sJuAS40NM+3twIVEv1vGp4mojEmPg44+b2NencsAIPjVrEY6MX8+n8n3iydxNqlSsadHl5XlBXPXUHHgAuc/f96Sw2B6hrZjXNLD/QBxibUzWKSM6rVrowb9/ain9e2ZTlm3fT/dnpvPiVmgwGLahzFM8DxYApZjbfzF4CMLPKZjYeIHyy+15gErAM+NDdlwRUr4jkEDPj6sRqTB3UifPrl+Opicu5/IWZLPlpV9Cl5VkWi1cZJCYmekpKStBliEg2mLBoE4+NWcLO/Ye5o1Mt+l1Ql4IJ8UGXFXPMbK67J6Y1Lzdc9SQikq4eTSoxdVASPZtXYdiXq7l46HTmrtsedFl5ioJCRHK9koXz8++rmzHillYcPHKcK1/6hj+PXcK+Q2oymBMUFCISNTrVK8ekgUnc0OYsRnyzlq5Dkkn+Xt+bijQFhYhElaIF8vGXno358I62FEiI44bXv+X+jxawa7+aDEaKgkJEolLLGqUZf19H7jm/Np/M20jnIdOYuHhT0GXFJAWFiEStggnx/L5bA8bc057yxQpw53++467/zGXLnoNBlxZTFBQiEvUaVynB6Hva80D3+ny+fAtdBifzUcp6NRnMJgoKEYkJCfFx3H1eHSb070i9CkX5/ciF3PjGHDbsSK/5g2SWgkJEYkrtckX5oG9bHu95NnPXbqfrkGRGfL1WTQazQEEhIjEnLs64oW0NJg1MIrFGaf40dglXv/wNq7bsDbq0qKSgEJGYVbVUYUbc3JJ/X9WMlVv2ctGz0xn25SqOqMngaVFQiEhMMzN6t6jK1EGd6NKoAk9PWkHP52eyeKOaDGaWgkJE8oRyxQow7Ppzeek3Ldi69xA9h83kqYnLOXjkWNCl5XoKChHJU7o3rsjUgZ248tyqvPjVai56djrf/qAmgxlRUIhInlOicAJPXdmU/9zamiPHj3P1y9/wxzGL2asmg2lSUIhIntWhblkmDUji5vY1eHvWOroOnsZXK7YEXVauo6AQkTytcP58/OnSsxl5ZzsK5Y/npjfmMOjD+ezYdzjo0nINBYWICNDirFJ8dl9H+l1Qh7Hzf6LLkGl8tnCT2oCgoBAR+a+CCfH8rmt9xt7bgYolCnLPu99x53/msmV33m4yqKAQETlJo8rFGX13ex7q0YCvVmzlwsHT+HBO3m0yqKAQEUlDvvg47uhUm4kDkmhYqTgPfLyQ3772Leu3570mgwoKEZEM1CxbhPdvb8MTlzdm/vqddB2SzGszfuBYHmoyqKAQETmFuDjjN23OYvLAJNrUKs1fxy2l94tfs/LnPUGXliMUFCIimVS5ZCFev6klz1zTnHW/7OPioTMY+vlKDh+N7SaDCgoRkdNgZlx+ThWmDOpEt8YVGTzley57fgYLN+wMurSIUVCIiJyBskUL8Ny15/DKDYns2H+Yy4fN5B/jl8Vkk0EFhYhIFnRpVIHJAztxTctqvJy8hu7PJDNrzS9Bl5WtAgkKM3vazJab2UIz+8TMSqaz3FozW2Rm880sJafrFBHJjBKFEvhHr6a8e1trjjv0GT6LRz5ZxJ6DR4IuLVsEdUQxBWjs7k2B74GHMlj2fHdv7u6JOVOaiMiZaVenLBMHdOS2DjV579sf6TokmS+W/xx0WVkWSFC4+2R3P9HPdxZQNYg6RESyW+H8+Xj0kkZ8fFc7ihXMxy1vpjDg/Xlsj+Img7nhHMUtwIR05jkw2czmmlnfHKxJRCRLzqleinH9OjKgc10+W7SJzoOnMXbBT1HZBsQiVbSZTQUqpjHrEXcfE17mESAR6OVpFGJmVdx9o5mVJzRc1c/dk9N5v75AX4Dq1au3WLduXTZtiYhI1qzYvIcHRi5gwYZddG5YgScub0zFEgWDLutXzGxuekP8EQuKUzGzm4A7gAvd/ZTNU8zsz8Bed//XqZZNTEz0lBSd+xaR3OPYcef1GT/w7ykrSIiL4+GLG9KnZTXMLOjSgIyDIqirnroDDwCXpRcSZlbEzIqd+BnoCizOuSpFRLJPfJxxe1ItJvZP4uwqxXlo1CKue2U2637ZF3RppxTUOYrngWLAlPClry8BmFllMxsfXqYCMMPMFgDfAp+5+8RgyhURyR41yhbhvdvb8I9eTVi8cRfdnknm1elrcnWTwcCGniJJQ08iEg027zrIo6MXMXXZFppVK8k/ezelfsVigdSS64aeREQEKpYoyCs3JDL02nPYsH0/lzw3nSFTvs91TQYVFCIiATIzLmtWmSmDOnFxk0o8+/lKLnluOvN+3BF0af+loBARyQVKF8nPM33O4fWbEtlz8Ci9X/yaJ8Yt5cDh4JsMKihERHKRCxpUYPLAJPq0qs6rM36g2zPJfL16W6A1KShERHKZYgUT+PsVTXjv9jaYwXWvzOahUQvZHVCTQQWFiEgu1bZ2GSb2T+KOpFp8MGc9XQZPY+rSnG8yqKAQEcnFCuWP56GLGjL6nvaUKpyf295Kod9789i291CO1aCgEBGJAk2rlmTsvR0Y1KUekxZvpsvgaYyetzFHmgwqKEREokT+fHHcd2FdPruvAzXKFmHAB/O55c05/LTzQETfV0EhIhJl6lYoxsg72/HYJY2YtWY7XYck859Z6zgeoTYgCgoRkSgUH2fc2qEmkwcm0bxaSR4dvZg+r8xi/+Gjp37xacqX7WsUEZEcU610Yd6+tRUfpWxg7rodFM6f/X/WFRQiIlHOzLi6ZTWublktIuvX0JOIiGRIQSEiIhlSUIiISIYUFCIikiEFhYiIZEhBISIiGVJQiIhIhhQUIiKSIcuJzoM5zcy2AuvO8OVlgWBvJxVZsbx9sbxtoO2Ldrl9+85y93JpzYjJoMgKM0tx98Sg64iUWN6+WN420PZFu2jePg09iYhIhhQUIiKSIQXF/xoedAERFsvbF8vbBtq+aBe126dzFCIikiEdUYiISIYUFCIikiEFRZiZdTezFWa2ysweDLqerDKzamb2pZktNbMlZtY/PL20mU0xs5Xhf0sFXWtWmFm8mc0zs3Hh5zXNbHZ4P35gZvmDrvFMmVlJMxtpZsvNbJmZtY2l/WdmA8P/bS42s/fMrGA07z8ze93MtpjZ4lTT0txfFjI0vJ0Lzezc4Co/NQUFoT82wDCgB9AIuNbMGgVbVZYdBX7n7o2ANsA94W16EPjc3esCn4efR7P+wLJUz58Chrh7HWAHcGsgVWWPZ4GJ7t4AaEZoO2Ni/5lZFeA+INHdGwPxQB+ie/+9CXQ/aVp6+6sHUDf86Au8mEM1nhEFRUgrYJW7r3H3w8D7QM+Aa8oSd9/k7t+Ff95D6I9MFULbNSK82Ajg8mAqzDozqwpcDLwafm7ABcDI8CJRu31mVgJIAl4DcPfD7r6TGNp/hG7FXMjM8gGFgU1E8f5z92Rg+0mT09tfPYG3PGQWUNLMKuVMpadPQRFSBVif6vmG8LSYYGY1gHOA2UAFd98UnrUZqBBQWdnhGeAB4Hj4eRlgp7sfDT+P5v1YE9gKvBEeWnvVzIoQI/vP3TcC/wJ+JBQQu4C5xM7+OyG9/RVVf3MUFDHOzIoCHwMD3H136nkeujY6Kq+PNrNLgC3uPjfoWiIkH3Au8KK7nwPs46Rhpijff6UIfaquCVQGivC/wzYxJZr3l4IiZCNQLdXzquFpUc3MEgiFxDvuPio8+ecTh7jhf7cEVV8WtQcuM7O1hIYKLyA0pl8yPJQB0b0fNwAb3H12+PlIQsERK/uvM/CDu2919yPAKEL7NFb23wnp7a+o+pujoAiZA9QNX3GRn9BJtbEB15Ql4fH614Bl7j441ayxwI3hn28ExuR0bdnB3R9y96ruXoPQ/vrC3a8HvgSuDC8Wzdu3GVhvZvXDky4ElhIj+4/QkFMbMysc/m/1xPbFxP5LJb39NRa4IXz1UxtgV6ohqlxH38wOM7OLCI15xwOvu/vfAi4pS8ysAzAdWMT/j+E/TOg8xYdAdUKt2K9295NPwEUVMzsPuN/dLzGzWoSOMEoD84DfuPuhIOs7U2bWnNCJ+vzAGuBmQh/uYmL/mdlfgGsIXaE3D7iN0Dh9VO4/M3sPOI9QO/GfgT8Bo0ljf4XD8XlCw237gZvdPSWIujNDQSEiIhnS0JOIiGRIQSEiIhlSUIiISIYUFCIikiEFhYiIZEhBIXIGzOyYmc1P9ci25nxmViN1B1KRoOU79SIikoYD7t486CJEcoKOKESykZmtNbN/mtkiM/vWzOqEp9cwsy/C9x743Myqh6dXMLNPzGxB+NEuvKp4M3slfL+GyWZWKLCNkjxPQSFyZgqdNPR0Tap5u9y9CaFv3j4TnvYcMMLdmwLvAEPD04cC09y9GaFeTkvC0+sCw9z9bGAn0DvC2yOSLn0zW+QMmNledy+axvS1wAXuvibclHGzu5cxs21AJXc/Ep6+yd3LmtlWoGrqNhXhtvBTwje7wcz+ACS4+xOR3zKR/6UjCpHs5+n8fDpS9zc6hs4nSoAUFCLZ75pU/34T/vlrQl1uAa4n1LARQrfHvAv+e//vEjlVpEhm6VOKyJkpZGbzUz2f6O4nLpEtZWYLCR0VXBue1o/Q3ep+T+jOdTeHp/cHhpvZrYSOHO4idMc3kVxD5yhEslH4HEWiu28LuhaR7KKhJxERyZCOKEREJEM6ohARkQwpKEREJEMKChERyZCCQkREMqSgEBGRDP0fwwYK3lSqzcIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}