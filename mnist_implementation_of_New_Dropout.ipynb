{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_implementation_of_New_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_implementation_of_New_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYgI3SFHqm4"
      },
      "source": [
        "# MNIST Numbers Implementation of My New Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HD9nTuEVHD"
      },
      "source": [
        "np.random.seed(seed=22) #Random seed used for comparison between old dropout"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cag8ZraxEZbF",
        "outputId": "9fb7357e-17bf-435b-aeb2-4e5c190a69dd"
      },
      "source": [
        "print(np.random.random(size=3)) #Check that seeds line up"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20846054 0.48168106 0.42053804]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers (Credit to Harrison Kinsley & Daniel Kukiela for raw python implementation)\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Og9ZrbKtV"
      },
      "source": [
        "# New Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXBWsHDIUSfh"
      },
      "source": [
        "class Layer_BinaryNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = 1 - rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, inputs, val_inputs):\n",
        "        self.inputs = inputs\n",
        "        self.val_inputs = val_inputs\n",
        "        nummask = round(len(self.inputs[0]) * self.rate)\n",
        "        \n",
        "        #Averaging Values\n",
        "        self.meanarray1 = np.mean(inputs, axis=0)\n",
        "        self.meanarray2 = np.mean(val_inputs, axis=0)\n",
        "\n",
        "        if self.iterations != 0:\n",
        "            # Calculating value\n",
        "            self.difference = self.meanarray1 - self.meanarray2\n",
        "            ind = np.argpartition(self.difference, -nummask)[-nummask:]\n",
        "            mask = np.ones(self.meanarray1.shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            self.difference[~mask] = 1\n",
        "            self.difference[mask] = 0. \n",
        "            self.binary_mask = self.difference / self.rate\n",
        "\n",
        "        else:\n",
        "            self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                                  size=inputs.shape) / self.rate\n",
        "        self.output = inputs * self.binary_mask\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiuCzwWxbRl0"
      },
      "source": [
        "class Layer_CatagoricalNSDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, X_test, y_test, X, y):        \n",
        "        if self.iterations != 0:\n",
        "          #Adding sorted data into dictionaries \n",
        "          sorted_x = {}\n",
        "          sorted_y = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x[\"class_{0}\".format(classes)] = X[y == classes]\n",
        "            sorted_y[\"label_{0}\".format(classes)] = y[y == classes]\n",
        "\n",
        "          sorted_x_test = {}\n",
        "          sorted_y_test = {}\n",
        "          for classes in range(len(set(y))):\n",
        "            sorted_x_test[\"class_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "            sorted_y_test[\"label_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "          #Averaging sorted data from each class then finding the difference between the averaged train and test inputs\n",
        "          differnce_classes = {}\n",
        "          for i, classes, test_classes in zip(range(len(set(y))), sorted_x, sorted_x_test):\n",
        "            differnce_classes[\"diff_{0}\".format(i)] = np.mean(sorted_x[classes], axis=0) - np.mean(sorted_x_test[classes], axis=0)\n",
        "\n",
        "          #Masking the data taking the high values(greatest difference between train and test) and setting their values to 0\n",
        "          self.diff_mask = {}\n",
        "          for i, classes, test_classes, diff in zip(range(len(set(y))), sorted_x, sorted_x_test, differnce_classes):\n",
        "            ind = np.argpartition(differnce_classes[diff], -round(len(X[0]) * self.rate))[-round(len(X[0]) * self.rate):]\n",
        "            mask = np.ones(np.mean(sorted_x[classes],axis=0).shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            differnce_classes[diff][~mask] = 0.\n",
        "            differnce_classes[diff][mask] = 1\n",
        "            self.diff_mask[\"mask_{0}\".format(i)] = differnce_classes[diff]\n",
        "\n",
        "          #Goes through each input values and applies the apprioprite mask based on what the true output should be.\n",
        "          binary_mask = np.empty(shape=X.shape)\n",
        "          for i, (input, label) in enumerate(zip(X,y)): \n",
        "            for true, diff in enumerate(self.diff_mask):\n",
        "              if label == true:\n",
        "                self.binary_mask[i] = self.diff_mask[diff]\n",
        "        else:\n",
        "          self.binary_mask = np.random.binomial(1, (1-self.rate), size=X.shape)\n",
        "        \n",
        "        self.output = (self.binary_mask/(1-self.rate)) * X\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def infrence(self, input, label):\n",
        "        self.input = input\n",
        "        self.label = label\n",
        "        idx = np.argsort(self.label)\n",
        "        input_sorted = input[idx]\n",
        "        label_sorted = label[idx]\n",
        "        self.infrence_binary_mask = np.empty(shape=self.input.shape)\n",
        "        for i, (input, label) in enumerate(zip(self.input, self.label)):\n",
        "          #for true, diff in zip(range(len(set(self.label))),self.diff_mask):\n",
        "          for true, diff in enumerate(self.diff_mask):\n",
        "            if label == true:\n",
        "              self.infrence_binary_mask[i] = self.diff_mask[diff]\n",
        "\n",
        "        self.output = self.infrence_binary_mask * self.input\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "33f80348-0c42-4fd3-da11-1c335b44090a"
      },
      "source": [
        "(X, y), (X_val, y_val) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "input = X\n",
        "label = y\n",
        "\n",
        "X = X[:10000,:,:]\n",
        "#X_test = X_test[:1600,:,:]\n",
        "y = y[:10000]\n",
        "#y_test  = y_test[:1600]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(.2))\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_train = (X_train.reshape(X_train.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_val = (X_val.reshape(X_val.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "input = (input.reshape(input.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 784)\n",
            "(8000,)\n",
            "(2000, 784)\n",
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nW7mqGTnem"
      },
      "source": [
        "Sorting Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtvA9y81TpGf",
        "outputId": "8bf1c638-2660-41c9-e6db-fd8e0149d84a"
      },
      "source": [
        "idx = np.argsort(y_train)\n",
        "X_sorted = X_train[idx]\n",
        "y_sorted = y_train[idx]\n",
        "\n",
        "sorted_x = {}\n",
        "sorted_y = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x[\"X_{0}\".format(classes)] = X_train[y_train == classes]\n",
        "  sorted_y[\"y_{0}\".format(classes)] = y_train[y_train == classes]\n",
        "\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x[sorted_lists].shape[0]}')\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_0: 766\n",
            "Number of Samples for X_1: 800\n",
            "Number of Samples for X_2: 830\n",
            "Number of Samples for X_3: 798\n",
            "Number of Samples for X_4: 808\n",
            "Number of Samples for X_5: 745\n",
            "Number of Samples for X_6: 858\n",
            "Number of Samples for X_7: 799\n",
            "Number of Samples for X_8: 815\n",
            "Number of Samples for X_9: 781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaNaUO3kP2G"
      },
      "source": [
        "Sorting Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBLFeGAUkSOs",
        "outputId": "0ed2b9fd-d23f-46c4-8fea-cdf1c75b0685"
      },
      "source": [
        "idx = np.argsort(y_test)\n",
        "X_test_sorted = X_test[idx]\n",
        "y_test_sorted = y_test[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_test = {}\n",
        "sorted_y_test = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_test[\"X_test_{0}\".format(classes)] = X_test[y_test == classes]\n",
        "  sorted_y_test[\"y_test_{0}\".format(classes)] = y_test[y_test == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_test:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_test[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_test[sorted_lists].shape[0])\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_test_0: 197\n",
            "Number of Samples for X_test_1: 176\n",
            "Number of Samples for X_test_2: 207\n",
            "Number of Samples for X_test_3: 210\n",
            "Number of Samples for X_test_4: 177\n",
            "Number of Samples for X_test_5: 214\n",
            "Number of Samples for X_test_6: 177\n",
            "Number of Samples for X_test_7: 211\n",
            "Number of Samples for X_test_8: 220\n",
            "Number of Samples for X_test_9: 211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSOlE-g40VbR",
        "outputId": "2ae8dc01-8e60-4b44-e7f3-3224ac9a4a21"
      },
      "source": [
        "idx = np.argsort(y_val)\n",
        "X_val_sorted = X_val[idx]\n",
        "y_val_sorted = y_val[idx]\n",
        "\n",
        "class_list = []\n",
        "\n",
        "sorted_x_val = {}\n",
        "sorted_y_val = {}\n",
        "for classes in range(len(set(y))):\n",
        "  sorted_x_val[\"X_val_{0}\".format(classes)] = X_val[y_val == classes]\n",
        "  sorted_y_val[\"y_val_{0}\".format(classes)] = y_val[y_val == classes]\n",
        "\n",
        "\n",
        "for sorted_lists in sorted_x_val:\n",
        "  print(f'Number of Samples for {sorted_lists}: {sorted_x_val[sorted_lists].shape[0]}')\n",
        "  class_list.append(sorted_x_val[sorted_lists].shape[0])"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples for X_val_0: 1000\n",
            "Number of Samples for X_val_1: 1000\n",
            "Number of Samples for X_val_2: 1000\n",
            "Number of Samples for X_val_3: 1000\n",
            "Number of Samples for X_val_4: 1000\n",
            "Number of Samples for X_val_5: 1000\n",
            "Number of Samples for X_val_6: 1000\n",
            "Number of Samples for X_val_7: 1000\n",
            "Number of Samples for X_val_8: 1000\n",
            "Number of Samples for X_val_9: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbnq4TJp1cl",
        "outputId": "1382b438-50fc-43e7-d69b-20aeb3e204ec"
      },
      "source": [
        "print(f'Found {X.shape[0]} images belonging to {len(set(y))} unique classes')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 10 unique classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_CatagoricalNSDropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,len(set(y)))\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7,learning_rate=0.005)\n",
        "#optimizer = Optimizer_SGD(learning_rate=0.01)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "5a78a3cf-28ce-497c-896c-d49b774025ae"
      },
      "source": [
        "epochs = 223\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X_train)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    if epoch != 0:\n",
        "      cached_val_inputs = cached_val_inputs\n",
        "      cached_train_inputs = activation1.output\n",
        "    else:\n",
        "      cached_val_inputs = np.random.random(size=128) #Never used just needed to pass to dropout\n",
        "      cached_train_inputs = activation1.output\n",
        "\n",
        "\n",
        "    dropout1.forward(X=activation1.output, y=y_train, X_test=cached_val_inputs, y_test=y_test)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y_train)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y_train)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "    dropout1.post_update_params()\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    if epoch == 0:\n",
        "      dense2.forward(activation1.output)\n",
        "    else:\n",
        "      dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "      dense2.forward(dropout1.output)\n",
        "    \n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Unseen Validaiton Accuracy\n",
        "    dense1.forward(X_val)\n",
        "    activation1.forward(dense1.output)\n",
        "    \n",
        "    if epoch == 0:\n",
        "      dense2.forward(activation1.output)\n",
        "    else:\n",
        "      dropout1.infrence(activation1.output,y_val)\n",
        "\n",
        "      dense2.forward(dropout1.output)\n",
        "    \n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    testloss = loss_function.calculate(activation4.output, y_val)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    testaccuracy = accuracy.calculate(predictions, y_val)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        "    test_acc_cache.append(testaccuracy)\n",
        "    test_loss_cache.append(testloss)\n",
        "    \n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ' +\n",
        "              f'Unseen, acc: {testaccuracy:.3f}, loss: {testloss:.3f} ')"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.112, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.005000000 validation, acc: 0.150, loss: 2.302 Unseen, acc: 0.173, loss: 2.302 \n",
            "epoch: 1, acc: 0.122, loss: 2.304 (data_loss: 2.302, reg_loss: 0.002), lr: 0.004999998 validation, acc: 0.285, loss: 2.298 Unseen, acc: 0.278, loss: 2.298 \n",
            "epoch: 2, acc: 0.200, loss: 2.300 (data_loss: 2.299, reg_loss: 0.001), lr: 0.004999995 validation, acc: 0.333, loss: 2.276 Unseen, acc: 0.330, loss: 2.276 \n",
            "epoch: 3, acc: 0.337, loss: 2.278 (data_loss: 2.277, reg_loss: 0.001), lr: 0.004999993 validation, acc: 0.302, loss: 2.184 Unseen, acc: 0.302, loss: 2.189 \n",
            "epoch: 4, acc: 0.304, loss: 2.175 (data_loss: 2.173, reg_loss: 0.001), lr: 0.004999990 validation, acc: 0.385, loss: 1.961 Unseen, acc: 0.369, loss: 1.974 \n",
            "epoch: 5, acc: 0.334, loss: 1.936 (data_loss: 1.933, reg_loss: 0.003), lr: 0.004999988 validation, acc: 0.386, loss: 1.743 Unseen, acc: 0.372, loss: 1.766 \n",
            "epoch: 6, acc: 0.375, loss: 1.810 (data_loss: 1.805, reg_loss: 0.005), lr: 0.004999985 validation, acc: 0.356, loss: 1.674 Unseen, acc: 0.341, loss: 1.707 \n",
            "epoch: 7, acc: 0.370, loss: 1.727 (data_loss: 1.721, reg_loss: 0.007), lr: 0.004999983 validation, acc: 0.371, loss: 1.601 Unseen, acc: 0.356, loss: 1.627 \n",
            "epoch: 8, acc: 0.375, loss: 1.610 (data_loss: 1.602, reg_loss: 0.008), lr: 0.004999980 validation, acc: 0.468, loss: 1.423 Unseen, acc: 0.454, loss: 1.452 \n",
            "epoch: 9, acc: 0.451, loss: 1.432 (data_loss: 1.423, reg_loss: 0.009), lr: 0.004999978 validation, acc: 0.459, loss: 1.433 Unseen, acc: 0.445, loss: 1.461 \n",
            "epoch: 10, acc: 0.420, loss: 1.451 (data_loss: 1.441, reg_loss: 0.010), lr: 0.004999975 validation, acc: 0.526, loss: 1.381 Unseen, acc: 0.505, loss: 1.416 \n",
            "epoch: 11, acc: 0.494, loss: 1.381 (data_loss: 1.370, reg_loss: 0.011), lr: 0.004999973 validation, acc: 0.475, loss: 1.346 Unseen, acc: 0.456, loss: 1.382 \n",
            "epoch: 12, acc: 0.445, loss: 1.340 (data_loss: 1.328, reg_loss: 0.012), lr: 0.004999970 validation, acc: 0.519, loss: 1.219 Unseen, acc: 0.496, loss: 1.257 \n",
            "epoch: 13, acc: 0.473, loss: 1.233 (data_loss: 1.220, reg_loss: 0.013), lr: 0.004999968 validation, acc: 0.502, loss: 1.157 Unseen, acc: 0.488, loss: 1.191 \n",
            "epoch: 14, acc: 0.512, loss: 1.146 (data_loss: 1.132, reg_loss: 0.014), lr: 0.004999965 validation, acc: 0.581, loss: 1.064 Unseen, acc: 0.561, loss: 1.095 \n",
            "epoch: 15, acc: 0.562, loss: 1.068 (data_loss: 1.052, reg_loss: 0.016), lr: 0.004999963 validation, acc: 0.638, loss: 0.982 Unseen, acc: 0.621, loss: 1.010 \n",
            "epoch: 16, acc: 0.616, loss: 0.990 (data_loss: 0.973, reg_loss: 0.017), lr: 0.004999960 validation, acc: 0.628, loss: 0.968 Unseen, acc: 0.617, loss: 0.990 \n",
            "epoch: 17, acc: 0.592, loss: 1.000 (data_loss: 0.982, reg_loss: 0.018), lr: 0.004999958 validation, acc: 0.697, loss: 0.844 Unseen, acc: 0.680, loss: 0.874 \n",
            "epoch: 18, acc: 0.686, loss: 0.842 (data_loss: 0.823, reg_loss: 0.019), lr: 0.004999955 validation, acc: 0.727, loss: 0.777 Unseen, acc: 0.724, loss: 0.794 \n",
            "epoch: 19, acc: 0.720, loss: 0.794 (data_loss: 0.774, reg_loss: 0.020), lr: 0.004999953 validation, acc: 0.733, loss: 0.736 Unseen, acc: 0.724, loss: 0.749 \n",
            "epoch: 20, acc: 0.721, loss: 0.732 (data_loss: 0.712, reg_loss: 0.020), lr: 0.004999950 validation, acc: 0.819, loss: 0.645 Unseen, acc: 0.815, loss: 0.657 \n",
            "epoch: 21, acc: 0.807, loss: 0.650 (data_loss: 0.629, reg_loss: 0.021), lr: 0.004999948 validation, acc: 0.812, loss: 0.639 Unseen, acc: 0.811, loss: 0.649 \n",
            "epoch: 22, acc: 0.814, loss: 0.619 (data_loss: 0.597, reg_loss: 0.022), lr: 0.004999945 validation, acc: 0.812, loss: 0.570 Unseen, acc: 0.811, loss: 0.570 \n",
            "epoch: 23, acc: 0.808, loss: 0.565 (data_loss: 0.542, reg_loss: 0.023), lr: 0.004999943 validation, acc: 0.822, loss: 0.487 Unseen, acc: 0.824, loss: 0.487 \n",
            "epoch: 24, acc: 0.820, loss: 0.495 (data_loss: 0.472, reg_loss: 0.024), lr: 0.004999940 validation, acc: 0.870, loss: 0.474 Unseen, acc: 0.869, loss: 0.474 \n",
            "epoch: 25, acc: 0.840, loss: 0.503 (data_loss: 0.479, reg_loss: 0.024), lr: 0.004999938 validation, acc: 0.908, loss: 0.408 Unseen, acc: 0.910, loss: 0.411 \n",
            "epoch: 26, acc: 0.899, loss: 0.420 (data_loss: 0.395, reg_loss: 0.025), lr: 0.004999935 validation, acc: 0.896, loss: 0.418 Unseen, acc: 0.913, loss: 0.408 \n",
            "epoch: 27, acc: 0.823, loss: 0.493 (data_loss: 0.467, reg_loss: 0.025), lr: 0.004999933 validation, acc: 0.911, loss: 0.405 Unseen, acc: 0.913, loss: 0.409 \n",
            "epoch: 28, acc: 0.906, loss: 0.392 (data_loss: 0.366, reg_loss: 0.026), lr: 0.004999930 validation, acc: 0.933, loss: 0.350 Unseen, acc: 0.931, loss: 0.356 \n",
            "epoch: 29, acc: 0.884, loss: 0.412 (data_loss: 0.385, reg_loss: 0.027), lr: 0.004999928 validation, acc: 0.953, loss: 0.292 Unseen, acc: 0.950, loss: 0.289 \n",
            "epoch: 30, acc: 0.916, loss: 0.336 (data_loss: 0.309, reg_loss: 0.027), lr: 0.004999925 validation, acc: 0.925, loss: 0.340 Unseen, acc: 0.924, loss: 0.345 \n",
            "epoch: 31, acc: 0.780, loss: 0.546 (data_loss: 0.519, reg_loss: 0.028), lr: 0.004999923 validation, acc: 0.909, loss: 0.368 Unseen, acc: 0.906, loss: 0.375 \n",
            "epoch: 32, acc: 0.850, loss: 0.460 (data_loss: 0.432, reg_loss: 0.028), lr: 0.004999920 validation, acc: 0.870, loss: 0.388 Unseen, acc: 0.876, loss: 0.401 \n",
            "epoch: 33, acc: 0.879, loss: 0.454 (data_loss: 0.425, reg_loss: 0.029), lr: 0.004999918 validation, acc: 0.885, loss: 0.407 Unseen, acc: 0.890, loss: 0.425 \n",
            "epoch: 34, acc: 0.773, loss: 0.668 (data_loss: 0.639, reg_loss: 0.029), lr: 0.004999915 validation, acc: 0.900, loss: 0.371 Unseen, acc: 0.897, loss: 0.386 \n",
            "epoch: 35, acc: 0.750, loss: 0.742 (data_loss: 0.712, reg_loss: 0.030), lr: 0.004999913 validation, acc: 0.833, loss: 0.525 Unseen, acc: 0.829, loss: 0.541 \n",
            "epoch: 36, acc: 0.770, loss: 0.707 (data_loss: 0.677, reg_loss: 0.031), lr: 0.004999910 validation, acc: 0.798, loss: 0.525 Unseen, acc: 0.797, loss: 0.545 \n",
            "epoch: 37, acc: 0.865, loss: 0.482 (data_loss: 0.451, reg_loss: 0.031), lr: 0.004999908 validation, acc: 0.919, loss: 0.323 Unseen, acc: 0.918, loss: 0.336 \n",
            "epoch: 38, acc: 0.927, loss: 0.338 (data_loss: 0.307, reg_loss: 0.032), lr: 0.004999905 validation, acc: 0.944, loss: 0.292 Unseen, acc: 0.941, loss: 0.299 \n",
            "epoch: 39, acc: 0.947, loss: 0.295 (data_loss: 0.263, reg_loss: 0.032), lr: 0.004999903 validation, acc: 0.949, loss: 0.277 Unseen, acc: 0.946, loss: 0.285 \n",
            "epoch: 40, acc: 0.849, loss: 0.480 (data_loss: 0.447, reg_loss: 0.033), lr: 0.004999900 validation, acc: 0.962, loss: 0.245 Unseen, acc: 0.957, loss: 0.252 \n",
            "epoch: 41, acc: 0.961, loss: 0.251 (data_loss: 0.218, reg_loss: 0.034), lr: 0.004999898 validation, acc: 0.959, loss: 0.212 Unseen, acc: 0.957, loss: 0.217 \n",
            "epoch: 42, acc: 0.948, loss: 0.322 (data_loss: 0.288, reg_loss: 0.034), lr: 0.004999895 validation, acc: 0.957, loss: 0.224 Unseen, acc: 0.953, loss: 0.231 \n",
            "epoch: 43, acc: 0.957, loss: 0.239 (data_loss: 0.204, reg_loss: 0.035), lr: 0.004999893 validation, acc: 0.982, loss: 0.118 Unseen, acc: 0.979, loss: 0.125 \n",
            "epoch: 44, acc: 0.982, loss: 0.129 (data_loss: 0.093, reg_loss: 0.036), lr: 0.004999890 validation, acc: 0.984, loss: 0.098 Unseen, acc: 0.982, loss: 0.099 \n",
            "epoch: 45, acc: 0.985, loss: 0.118 (data_loss: 0.081, reg_loss: 0.037), lr: 0.004999888 validation, acc: 0.976, loss: 0.113 Unseen, acc: 0.976, loss: 0.114 \n",
            "epoch: 46, acc: 0.961, loss: 0.171 (data_loss: 0.134, reg_loss: 0.037), lr: 0.004999885 validation, acc: 0.986, loss: 0.083 Unseen, acc: 0.983, loss: 0.086 \n",
            "epoch: 47, acc: 0.931, loss: 0.282 (data_loss: 0.245, reg_loss: 0.038), lr: 0.004999883 validation, acc: 0.964, loss: 0.136 Unseen, acc: 0.960, loss: 0.147 \n",
            "epoch: 48, acc: 0.947, loss: 0.235 (data_loss: 0.197, reg_loss: 0.038), lr: 0.004999880 validation, acc: 0.952, loss: 0.188 Unseen, acc: 0.953, loss: 0.202 \n",
            "epoch: 49, acc: 0.951, loss: 0.227 (data_loss: 0.188, reg_loss: 0.038), lr: 0.004999878 validation, acc: 0.977, loss: 0.116 Unseen, acc: 0.969, loss: 0.135 \n",
            "epoch: 50, acc: 0.946, loss: 0.247 (data_loss: 0.208, reg_loss: 0.038), lr: 0.004999875 validation, acc: 0.976, loss: 0.096 Unseen, acc: 0.968, loss: 0.121 \n",
            "epoch: 51, acc: 0.973, loss: 0.150 (data_loss: 0.111, reg_loss: 0.039), lr: 0.004999873 validation, acc: 0.980, loss: 0.089 Unseen, acc: 0.975, loss: 0.102 \n",
            "epoch: 52, acc: 0.943, loss: 0.211 (data_loss: 0.172, reg_loss: 0.039), lr: 0.004999870 validation, acc: 0.975, loss: 0.106 Unseen, acc: 0.973, loss: 0.112 \n",
            "epoch: 53, acc: 0.964, loss: 0.172 (data_loss: 0.133, reg_loss: 0.039), lr: 0.004999868 validation, acc: 0.976, loss: 0.123 Unseen, acc: 0.970, loss: 0.131 \n",
            "epoch: 54, acc: 0.918, loss: 0.281 (data_loss: 0.242, reg_loss: 0.039), lr: 0.004999865 validation, acc: 0.951, loss: 0.159 Unseen, acc: 0.949, loss: 0.172 \n",
            "epoch: 55, acc: 0.943, loss: 0.223 (data_loss: 0.184, reg_loss: 0.039), lr: 0.004999863 validation, acc: 0.980, loss: 0.100 Unseen, acc: 0.979, loss: 0.108 \n",
            "epoch: 56, acc: 0.889, loss: 0.677 (data_loss: 0.638, reg_loss: 0.039), lr: 0.004999860 validation, acc: 0.903, loss: 0.309 Unseen, acc: 0.892, loss: 0.331 \n",
            "epoch: 57, acc: 0.796, loss: 0.673 (data_loss: 0.633, reg_loss: 0.039), lr: 0.004999858 validation, acc: 0.973, loss: 0.113 Unseen, acc: 0.974, loss: 0.116 \n",
            "epoch: 58, acc: 0.971, loss: 0.170 (data_loss: 0.131, reg_loss: 0.039), lr: 0.004999855 validation, acc: 0.986, loss: 0.084 Unseen, acc: 0.982, loss: 0.091 \n",
            "epoch: 59, acc: 0.890, loss: 0.589 (data_loss: 0.549, reg_loss: 0.039), lr: 0.004999853 validation, acc: 0.926, loss: 0.275 Unseen, acc: 0.918, loss: 0.301 \n",
            "epoch: 60, acc: 0.874, loss: 0.587 (data_loss: 0.547, reg_loss: 0.040), lr: 0.004999850 validation, acc: 0.909, loss: 0.313 Unseen, acc: 0.910, loss: 0.324 \n",
            "epoch: 61, acc: 0.861, loss: 0.526 (data_loss: 0.486, reg_loss: 0.040), lr: 0.004999848 validation, acc: 0.950, loss: 0.197 Unseen, acc: 0.946, loss: 0.201 \n",
            "epoch: 62, acc: 0.943, loss: 0.235 (data_loss: 0.195, reg_loss: 0.040), lr: 0.004999845 validation, acc: 0.967, loss: 0.131 Unseen, acc: 0.966, loss: 0.131 \n",
            "epoch: 63, acc: 0.967, loss: 0.167 (data_loss: 0.126, reg_loss: 0.041), lr: 0.004999843 validation, acc: 0.914, loss: 0.253 Unseen, acc: 0.925, loss: 0.245 \n",
            "epoch: 64, acc: 0.933, loss: 0.249 (data_loss: 0.208, reg_loss: 0.041), lr: 0.004999840 validation, acc: 0.911, loss: 0.265 Unseen, acc: 0.915, loss: 0.261 \n",
            "epoch: 65, acc: 0.820, loss: 0.513 (data_loss: 0.472, reg_loss: 0.042), lr: 0.004999838 validation, acc: 0.930, loss: 0.245 Unseen, acc: 0.924, loss: 0.261 \n",
            "epoch: 66, acc: 0.853, loss: 0.556 (data_loss: 0.514, reg_loss: 0.042), lr: 0.004999835 validation, acc: 0.966, loss: 0.106 Unseen, acc: 0.961, loss: 0.130 \n",
            "epoch: 67, acc: 0.897, loss: 0.260 (data_loss: 0.217, reg_loss: 0.043), lr: 0.004999833 validation, acc: 0.948, loss: 0.156 Unseen, acc: 0.939, loss: 0.186 \n",
            "epoch: 68, acc: 0.937, loss: 0.218 (data_loss: 0.174, reg_loss: 0.044), lr: 0.004999830 validation, acc: 0.858, loss: 0.348 Unseen, acc: 0.845, loss: 0.395 \n",
            "epoch: 69, acc: 0.860, loss: 0.438 (data_loss: 0.394, reg_loss: 0.044), lr: 0.004999828 validation, acc: 0.890, loss: 0.321 Unseen, acc: 0.878, loss: 0.370 \n",
            "epoch: 70, acc: 0.869, loss: 0.455 (data_loss: 0.410, reg_loss: 0.044), lr: 0.004999825 validation, acc: 0.895, loss: 0.281 Unseen, acc: 0.881, loss: 0.316 \n",
            "epoch: 71, acc: 0.878, loss: 0.395 (data_loss: 0.350, reg_loss: 0.045), lr: 0.004999823 validation, acc: 0.931, loss: 0.197 Unseen, acc: 0.925, loss: 0.219 \n",
            "epoch: 72, acc: 0.923, loss: 0.262 (data_loss: 0.217, reg_loss: 0.045), lr: 0.004999820 validation, acc: 0.985, loss: 0.087 Unseen, acc: 0.980, loss: 0.092 \n",
            "epoch: 73, acc: 0.925, loss: 0.357 (data_loss: 0.312, reg_loss: 0.045), lr: 0.004999818 validation, acc: 0.982, loss: 0.085 Unseen, acc: 0.982, loss: 0.084 \n",
            "epoch: 74, acc: 0.978, loss: 0.119 (data_loss: 0.074, reg_loss: 0.045), lr: 0.004999815 validation, acc: 0.995, loss: 0.057 Unseen, acc: 0.995, loss: 0.054 \n",
            "epoch: 75, acc: 0.995, loss: 0.084 (data_loss: 0.039, reg_loss: 0.045), lr: 0.004999813 validation, acc: 0.993, loss: 0.072 Unseen, acc: 0.994, loss: 0.067 \n",
            "epoch: 76, acc: 0.991, loss: 0.100 (data_loss: 0.055, reg_loss: 0.045), lr: 0.004999810 validation, acc: 0.986, loss: 0.076 Unseen, acc: 0.992, loss: 0.072 \n",
            "epoch: 77, acc: 0.992, loss: 0.102 (data_loss: 0.056, reg_loss: 0.045), lr: 0.004999808 validation, acc: 0.991, loss: 0.061 Unseen, acc: 0.993, loss: 0.057 \n",
            "epoch: 78, acc: 0.992, loss: 0.088 (data_loss: 0.042, reg_loss: 0.045), lr: 0.004999805 validation, acc: 0.990, loss: 0.060 Unseen, acc: 0.991, loss: 0.056 \n",
            "epoch: 79, acc: 0.936, loss: 0.187 (data_loss: 0.142, reg_loss: 0.045), lr: 0.004999803 validation, acc: 0.985, loss: 0.065 Unseen, acc: 0.985, loss: 0.063 \n",
            "epoch: 80, acc: 0.981, loss: 0.108 (data_loss: 0.062, reg_loss: 0.045), lr: 0.004999800 validation, acc: 0.988, loss: 0.086 Unseen, acc: 0.985, loss: 0.082 \n",
            "epoch: 81, acc: 0.961, loss: 0.152 (data_loss: 0.107, reg_loss: 0.045), lr: 0.004999798 validation, acc: 0.980, loss: 0.074 Unseen, acc: 0.980, loss: 0.080 \n",
            "epoch: 82, acc: 0.981, loss: 0.116 (data_loss: 0.071, reg_loss: 0.045), lr: 0.004999795 validation, acc: 0.950, loss: 0.154 Unseen, acc: 0.946, loss: 0.166 \n",
            "epoch: 83, acc: 0.945, loss: 0.218 (data_loss: 0.173, reg_loss: 0.045), lr: 0.004999793 validation, acc: 0.988, loss: 0.056 Unseen, acc: 0.988, loss: 0.061 \n",
            "epoch: 84, acc: 0.923, loss: 0.192 (data_loss: 0.147, reg_loss: 0.045), lr: 0.004999790 validation, acc: 0.951, loss: 0.136 Unseen, acc: 0.946, loss: 0.141 \n",
            "epoch: 85, acc: 0.940, loss: 0.206 (data_loss: 0.161, reg_loss: 0.045), lr: 0.004999788 validation, acc: 0.895, loss: 0.172 Unseen, acc: 0.904, loss: 0.168 \n",
            "epoch: 86, acc: 0.819, loss: 0.616 (data_loss: 0.571, reg_loss: 0.045), lr: 0.004999785 validation, acc: 0.920, loss: 0.235 Unseen, acc: 0.921, loss: 0.238 \n",
            "epoch: 87, acc: 0.875, loss: 1.252 (data_loss: 1.207, reg_loss: 0.045), lr: 0.004999783 validation, acc: 0.874, loss: 0.852 Unseen, acc: 0.873, loss: 0.829 \n",
            "epoch: 88, acc: 0.794, loss: 1.729 (data_loss: 1.684, reg_loss: 0.045), lr: 0.004999780 validation, acc: 0.829, loss: 1.109 Unseen, acc: 0.820, loss: 1.105 \n",
            "epoch: 89, acc: 0.774, loss: 2.355 (data_loss: 2.310, reg_loss: 0.045), lr: 0.004999778 validation, acc: 0.785, loss: 1.614 Unseen, acc: 0.783, loss: 1.624 \n",
            "epoch: 90, acc: 0.772, loss: 2.294 (data_loss: 2.249, reg_loss: 0.045), lr: 0.004999775 validation, acc: 0.798, loss: 1.469 Unseen, acc: 0.790, loss: 1.495 \n",
            "epoch: 91, acc: 0.635, loss: 2.908 (data_loss: 2.863, reg_loss: 0.045), lr: 0.004999773 validation, acc: 0.768, loss: 1.640 Unseen, acc: 0.763, loss: 1.681 \n",
            "epoch: 92, acc: 0.713, loss: 2.560 (data_loss: 2.515, reg_loss: 0.045), lr: 0.004999770 validation, acc: 0.766, loss: 1.464 Unseen, acc: 0.760, loss: 1.485 \n",
            "epoch: 93, acc: 0.759, loss: 1.860 (data_loss: 1.815, reg_loss: 0.045), lr: 0.004999768 validation, acc: 0.827, loss: 0.875 Unseen, acc: 0.828, loss: 0.885 \n",
            "epoch: 94, acc: 0.790, loss: 1.320 (data_loss: 1.275, reg_loss: 0.045), lr: 0.004999765 validation, acc: 0.853, loss: 0.483 Unseen, acc: 0.855, loss: 0.479 \n",
            "epoch: 95, acc: 0.822, loss: 0.737 (data_loss: 0.692, reg_loss: 0.045), lr: 0.004999763 validation, acc: 0.956, loss: 0.180 Unseen, acc: 0.956, loss: 0.167 \n",
            "epoch: 96, acc: 0.942, loss: 0.281 (data_loss: 0.236, reg_loss: 0.045), lr: 0.004999760 validation, acc: 0.961, loss: 0.147 Unseen, acc: 0.959, loss: 0.149 \n",
            "epoch: 97, acc: 0.911, loss: 0.256 (data_loss: 0.210, reg_loss: 0.046), lr: 0.004999758 validation, acc: 0.961, loss: 0.135 Unseen, acc: 0.958, loss: 0.144 \n",
            "epoch: 98, acc: 0.959, loss: 0.188 (data_loss: 0.142, reg_loss: 0.046), lr: 0.004999755 validation, acc: 0.988, loss: 0.061 Unseen, acc: 0.982, loss: 0.064 \n",
            "epoch: 99, acc: 0.978, loss: 0.127 (data_loss: 0.081, reg_loss: 0.046), lr: 0.004999753 validation, acc: 0.985, loss: 0.060 Unseen, acc: 0.982, loss: 0.059 \n",
            "epoch: 100, acc: 0.984, loss: 0.103 (data_loss: 0.057, reg_loss: 0.046), lr: 0.004999750 validation, acc: 0.972, loss: 0.096 Unseen, acc: 0.973, loss: 0.093 \n",
            "epoch: 101, acc: 0.974, loss: 0.138 (data_loss: 0.091, reg_loss: 0.046), lr: 0.004999748 validation, acc: 0.971, loss: 0.117 Unseen, acc: 0.972, loss: 0.113 \n",
            "epoch: 102, acc: 0.815, loss: 1.138 (data_loss: 1.092, reg_loss: 0.047), lr: 0.004999745 validation, acc: 0.931, loss: 0.465 Unseen, acc: 0.925, loss: 0.465 \n",
            "epoch: 103, acc: 0.906, loss: 0.649 (data_loss: 0.602, reg_loss: 0.047), lr: 0.004999743 validation, acc: 0.946, loss: 0.314 Unseen, acc: 0.943, loss: 0.301 \n",
            "epoch: 104, acc: 0.861, loss: 1.190 (data_loss: 1.144, reg_loss: 0.047), lr: 0.004999740 validation, acc: 0.921, loss: 0.529 Unseen, acc: 0.915, loss: 0.540 \n",
            "epoch: 105, acc: 0.905, loss: 0.696 (data_loss: 0.649, reg_loss: 0.047), lr: 0.004999738 validation, acc: 0.956, loss: 0.296 Unseen, acc: 0.950, loss: 0.279 \n",
            "epoch: 106, acc: 0.899, loss: 0.604 (data_loss: 0.558, reg_loss: 0.047), lr: 0.004999735 validation, acc: 0.952, loss: 0.240 Unseen, acc: 0.952, loss: 0.229 \n",
            "epoch: 107, acc: 0.924, loss: 0.588 (data_loss: 0.541, reg_loss: 0.047), lr: 0.004999733 validation, acc: 0.957, loss: 0.255 Unseen, acc: 0.953, loss: 0.246 \n",
            "epoch: 108, acc: 0.947, loss: 0.347 (data_loss: 0.299, reg_loss: 0.047), lr: 0.004999730 validation, acc: 0.957, loss: 0.183 Unseen, acc: 0.953, loss: 0.204 \n",
            "epoch: 109, acc: 0.951, loss: 0.266 (data_loss: 0.218, reg_loss: 0.047), lr: 0.004999728 validation, acc: 0.969, loss: 0.139 Unseen, acc: 0.963, loss: 0.161 \n",
            "epoch: 110, acc: 0.960, loss: 0.215 (data_loss: 0.167, reg_loss: 0.048), lr: 0.004999725 validation, acc: 0.974, loss: 0.124 Unseen, acc: 0.968, loss: 0.141 \n",
            "epoch: 111, acc: 0.941, loss: 0.259 (data_loss: 0.211, reg_loss: 0.048), lr: 0.004999723 validation, acc: 0.971, loss: 0.130 Unseen, acc: 0.967, loss: 0.136 \n",
            "epoch: 112, acc: 0.964, loss: 0.216 (data_loss: 0.167, reg_loss: 0.048), lr: 0.004999720 validation, acc: 0.961, loss: 0.171 Unseen, acc: 0.957, loss: 0.181 \n",
            "epoch: 113, acc: 0.930, loss: 0.328 (data_loss: 0.279, reg_loss: 0.049), lr: 0.004999718 validation, acc: 0.971, loss: 0.154 Unseen, acc: 0.970, loss: 0.160 \n",
            "epoch: 114, acc: 0.973, loss: 0.221 (data_loss: 0.172, reg_loss: 0.049), lr: 0.004999715 validation, acc: 0.975, loss: 0.168 Unseen, acc: 0.973, loss: 0.168 \n",
            "epoch: 115, acc: 0.945, loss: 0.339 (data_loss: 0.289, reg_loss: 0.049), lr: 0.004999713 validation, acc: 0.965, loss: 0.198 Unseen, acc: 0.964, loss: 0.206 \n",
            "epoch: 116, acc: 0.955, loss: 0.311 (data_loss: 0.262, reg_loss: 0.049), lr: 0.004999710 validation, acc: 0.972, loss: 0.193 Unseen, acc: 0.969, loss: 0.202 \n",
            "epoch: 117, acc: 0.915, loss: 0.406 (data_loss: 0.357, reg_loss: 0.049), lr: 0.004999708 validation, acc: 0.975, loss: 0.179 Unseen, acc: 0.974, loss: 0.186 \n",
            "epoch: 118, acc: 0.852, loss: 0.676 (data_loss: 0.627, reg_loss: 0.049), lr: 0.004999705 validation, acc: 0.955, loss: 0.206 Unseen, acc: 0.952, loss: 0.220 \n",
            "epoch: 119, acc: 0.901, loss: 0.434 (data_loss: 0.384, reg_loss: 0.049), lr: 0.004999703 validation, acc: 0.949, loss: 0.220 Unseen, acc: 0.949, loss: 0.229 \n",
            "epoch: 120, acc: 0.946, loss: 0.306 (data_loss: 0.256, reg_loss: 0.050), lr: 0.004999700 validation, acc: 0.958, loss: 0.187 Unseen, acc: 0.952, loss: 0.205 \n",
            "epoch: 121, acc: 0.875, loss: 0.442 (data_loss: 0.392, reg_loss: 0.050), lr: 0.004999698 validation, acc: 0.960, loss: 0.175 Unseen, acc: 0.958, loss: 0.191 \n",
            "epoch: 122, acc: 0.965, loss: 0.246 (data_loss: 0.196, reg_loss: 0.050), lr: 0.004999695 validation, acc: 0.958, loss: 0.166 Unseen, acc: 0.961, loss: 0.178 \n",
            "epoch: 123, acc: 0.947, loss: 0.318 (data_loss: 0.268, reg_loss: 0.050), lr: 0.004999693 validation, acc: 0.961, loss: 0.162 Unseen, acc: 0.960, loss: 0.176 \n",
            "epoch: 124, acc: 0.963, loss: 0.235 (data_loss: 0.185, reg_loss: 0.050), lr: 0.004999690 validation, acc: 0.982, loss: 0.089 Unseen, acc: 0.979, loss: 0.101 \n",
            "epoch: 125, acc: 0.900, loss: 0.766 (data_loss: 0.715, reg_loss: 0.051), lr: 0.004999688 validation, acc: 0.955, loss: 0.194 Unseen, acc: 0.943, loss: 0.235 \n",
            "epoch: 126, acc: 0.947, loss: 0.295 (data_loss: 0.244, reg_loss: 0.051), lr: 0.004999685 validation, acc: 0.982, loss: 0.065 Unseen, acc: 0.975, loss: 0.077 \n",
            "epoch: 127, acc: 0.946, loss: 0.183 (data_loss: 0.132, reg_loss: 0.052), lr: 0.004999683 validation, acc: 0.970, loss: 0.078 Unseen, acc: 0.969, loss: 0.085 \n",
            "epoch: 128, acc: 0.931, loss: 0.383 (data_loss: 0.331, reg_loss: 0.052), lr: 0.004999680 validation, acc: 0.958, loss: 0.127 Unseen, acc: 0.957, loss: 0.136 \n",
            "epoch: 129, acc: 0.877, loss: 0.714 (data_loss: 0.662, reg_loss: 0.052), lr: 0.004999678 validation, acc: 0.894, loss: 0.327 Unseen, acc: 0.900, loss: 0.311 \n",
            "epoch: 130, acc: 0.901, loss: 0.408 (data_loss: 0.356, reg_loss: 0.053), lr: 0.004999675 validation, acc: 0.918, loss: 0.209 Unseen, acc: 0.918, loss: 0.211 \n",
            "epoch: 131, acc: 0.889, loss: 0.340 (data_loss: 0.287, reg_loss: 0.053), lr: 0.004999673 validation, acc: 0.976, loss: 0.088 Unseen, acc: 0.973, loss: 0.092 \n",
            "epoch: 132, acc: 0.957, loss: 0.188 (data_loss: 0.135, reg_loss: 0.053), lr: 0.004999670 validation, acc: 0.990, loss: 0.050 Unseen, acc: 0.989, loss: 0.056 \n",
            "epoch: 133, acc: 0.988, loss: 0.117 (data_loss: 0.064, reg_loss: 0.054), lr: 0.004999668 validation, acc: 0.985, loss: 0.048 Unseen, acc: 0.984, loss: 0.055 \n",
            "epoch: 134, acc: 0.961, loss: 0.198 (data_loss: 0.144, reg_loss: 0.054), lr: 0.004999665 validation, acc: 0.984, loss: 0.057 Unseen, acc: 0.980, loss: 0.066 \n",
            "epoch: 135, acc: 0.980, loss: 0.132 (data_loss: 0.078, reg_loss: 0.054), lr: 0.004999663 validation, acc: 0.983, loss: 0.066 Unseen, acc: 0.984, loss: 0.073 \n",
            "epoch: 136, acc: 0.977, loss: 0.164 (data_loss: 0.110, reg_loss: 0.054), lr: 0.004999660 validation, acc: 0.974, loss: 0.104 Unseen, acc: 0.974, loss: 0.117 \n",
            "epoch: 137, acc: 0.945, loss: 0.304 (data_loss: 0.249, reg_loss: 0.054), lr: 0.004999658 validation, acc: 0.977, loss: 0.103 Unseen, acc: 0.974, loss: 0.100 \n",
            "epoch: 138, acc: 0.854, loss: 1.308 (data_loss: 1.253, reg_loss: 0.055), lr: 0.004999655 validation, acc: 0.885, loss: 0.950 Unseen, acc: 0.881, loss: 1.001 \n",
            "epoch: 139, acc: 0.888, loss: 1.187 (data_loss: 1.133, reg_loss: 0.055), lr: 0.004999653 validation, acc: 0.888, loss: 0.979 Unseen, acc: 0.884, loss: 1.034 \n",
            "epoch: 140, acc: 0.890, loss: 1.207 (data_loss: 1.152, reg_loss: 0.055), lr: 0.004999650 validation, acc: 0.887, loss: 0.981 Unseen, acc: 0.883, loss: 1.021 \n",
            "epoch: 141, acc: 0.884, loss: 1.568 (data_loss: 1.513, reg_loss: 0.055), lr: 0.004999648 validation, acc: 0.875, loss: 1.369 Unseen, acc: 0.871, loss: 1.416 \n",
            "epoch: 142, acc: 0.859, loss: 1.105 (data_loss: 1.050, reg_loss: 0.055), lr: 0.004999645 validation, acc: 0.889, loss: 0.618 Unseen, acc: 0.887, loss: 0.646 \n",
            "epoch: 143, acc: 0.825, loss: 1.876 (data_loss: 1.822, reg_loss: 0.054), lr: 0.004999643 validation, acc: 0.861, loss: 1.111 Unseen, acc: 0.860, loss: 1.124 \n",
            "epoch: 144, acc: 0.865, loss: 0.702 (data_loss: 0.648, reg_loss: 0.054), lr: 0.004999640 validation, acc: 0.886, loss: 0.310 Unseen, acc: 0.881, loss: 0.315 \n",
            "epoch: 145, acc: 0.889, loss: 1.401 (data_loss: 1.347, reg_loss: 0.054), lr: 0.004999638 validation, acc: 0.887, loss: 1.025 Unseen, acc: 0.885, loss: 1.068 \n",
            "epoch: 146, acc: 0.891, loss: 1.271 (data_loss: 1.217, reg_loss: 0.053), lr: 0.004999635 validation, acc: 0.894, loss: 0.926 Unseen, acc: 0.890, loss: 0.967 \n",
            "epoch: 147, acc: 0.897, loss: 1.328 (data_loss: 1.275, reg_loss: 0.053), lr: 0.004999633 validation, acc: 0.897, loss: 1.011 Unseen, acc: 0.896, loss: 1.043 \n",
            "epoch: 148, acc: 0.897, loss: 1.247 (data_loss: 1.195, reg_loss: 0.052), lr: 0.004999630 validation, acc: 0.893, loss: 0.949 Unseen, acc: 0.893, loss: 0.976 \n",
            "epoch: 149, acc: 0.896, loss: 1.169 (data_loss: 1.117, reg_loss: 0.052), lr: 0.004999628 validation, acc: 0.896, loss: 0.859 Unseen, acc: 0.895, loss: 0.886 \n",
            "epoch: 150, acc: 0.882, loss: 1.183 (data_loss: 1.132, reg_loss: 0.051), lr: 0.004999625 validation, acc: 0.898, loss: 0.801 Unseen, acc: 0.897, loss: 0.836 \n",
            "epoch: 151, acc: 0.899, loss: 1.013 (data_loss: 0.963, reg_loss: 0.051), lr: 0.004999623 validation, acc: 0.899, loss: 0.697 Unseen, acc: 0.896, loss: 0.736 \n",
            "epoch: 152, acc: 0.892, loss: 0.927 (data_loss: 0.877, reg_loss: 0.050), lr: 0.004999620 validation, acc: 0.897, loss: 0.573 Unseen, acc: 0.894, loss: 0.612 \n",
            "epoch: 153, acc: 0.892, loss: 0.834 (data_loss: 0.784, reg_loss: 0.050), lr: 0.004999618 validation, acc: 0.893, loss: 0.513 Unseen, acc: 0.890, loss: 0.546 \n",
            "epoch: 154, acc: 0.892, loss: 0.694 (data_loss: 0.645, reg_loss: 0.049), lr: 0.004999615 validation, acc: 0.902, loss: 0.300 Unseen, acc: 0.902, loss: 0.331 \n",
            "epoch: 155, acc: 0.905, loss: 0.423 (data_loss: 0.375, reg_loss: 0.048), lr: 0.004999613 validation, acc: 0.978, loss: 0.084 Unseen, acc: 0.973, loss: 0.093 \n",
            "epoch: 156, acc: 0.985, loss: 0.122 (data_loss: 0.074, reg_loss: 0.048), lr: 0.004999610 validation, acc: 0.981, loss: 0.065 Unseen, acc: 0.981, loss: 0.069 \n",
            "epoch: 157, acc: 0.981, loss: 0.124 (data_loss: 0.077, reg_loss: 0.047), lr: 0.004999608 validation, acc: 0.982, loss: 0.060 Unseen, acc: 0.981, loss: 0.065 \n",
            "epoch: 158, acc: 0.984, loss: 0.112 (data_loss: 0.065, reg_loss: 0.047), lr: 0.004999605 validation, acc: 0.991, loss: 0.039 Unseen, acc: 0.989, loss: 0.046 \n",
            "epoch: 159, acc: 0.990, loss: 0.089 (data_loss: 0.043, reg_loss: 0.046), lr: 0.004999603 validation, acc: 0.994, loss: 0.031 Unseen, acc: 0.993, loss: 0.036 \n",
            "epoch: 160, acc: 0.906, loss: 0.220 (data_loss: 0.174, reg_loss: 0.046), lr: 0.004999600 validation, acc: 0.896, loss: 0.233 Unseen, acc: 0.897, loss: 0.245 \n",
            "epoch: 161, acc: 0.899, loss: 0.313 (data_loss: 0.268, reg_loss: 0.045), lr: 0.004999598 validation, acc: 0.895, loss: 0.276 Unseen, acc: 0.895, loss: 0.286 \n",
            "epoch: 162, acc: 0.900, loss: 0.350 (data_loss: 0.305, reg_loss: 0.045), lr: 0.004999595 validation, acc: 0.983, loss: 0.070 Unseen, acc: 0.983, loss: 0.072 \n",
            "epoch: 163, acc: 0.981, loss: 0.111 (data_loss: 0.067, reg_loss: 0.044), lr: 0.004999593 validation, acc: 0.985, loss: 0.057 Unseen, acc: 0.984, loss: 0.054 \n",
            "epoch: 164, acc: 0.888, loss: 0.402 (data_loss: 0.358, reg_loss: 0.044), lr: 0.004999590 validation, acc: 0.920, loss: 0.204 Unseen, acc: 0.909, loss: 0.214 \n",
            "epoch: 165, acc: 0.912, loss: 0.276 (data_loss: 0.233, reg_loss: 0.043), lr: 0.004999588 validation, acc: 0.969, loss: 0.095 Unseen, acc: 0.972, loss: 0.098 \n",
            "epoch: 166, acc: 0.898, loss: 0.588 (data_loss: 0.545, reg_loss: 0.043), lr: 0.004999585 validation, acc: 0.973, loss: 0.102 Unseen, acc: 0.971, loss: 0.119 \n",
            "epoch: 167, acc: 0.965, loss: 0.179 (data_loss: 0.136, reg_loss: 0.043), lr: 0.004999583 validation, acc: 0.940, loss: 0.123 Unseen, acc: 0.940, loss: 0.128 \n",
            "epoch: 168, acc: 0.825, loss: 1.540 (data_loss: 1.498, reg_loss: 0.042), lr: 0.004999580 validation, acc: 0.892, loss: 1.006 Unseen, acc: 0.878, loss: 1.138 \n",
            "epoch: 169, acc: 0.836, loss: 1.763 (data_loss: 1.721, reg_loss: 0.042), lr: 0.004999578 validation, acc: 0.899, loss: 1.280 Unseen, acc: 0.888, loss: 1.445 \n",
            "epoch: 170, acc: 0.889, loss: 1.637 (data_loss: 1.595, reg_loss: 0.042), lr: 0.004999575 validation, acc: 0.906, loss: 1.323 Unseen, acc: 0.894, loss: 1.492 \n",
            "epoch: 171, acc: 0.856, loss: 1.733 (data_loss: 1.691, reg_loss: 0.042), lr: 0.004999573 validation, acc: 0.896, loss: 1.380 Unseen, acc: 0.885, loss: 1.551 \n",
            "epoch: 172, acc: 0.839, loss: 1.811 (data_loss: 1.768, reg_loss: 0.042), lr: 0.004999570 validation, acc: 0.868, loss: 1.498 Unseen, acc: 0.859, loss: 1.672 \n",
            "epoch: 173, acc: 0.851, loss: 1.821 (data_loss: 1.779, reg_loss: 0.042), lr: 0.004999568 validation, acc: 0.884, loss: 1.448 Unseen, acc: 0.871, loss: 1.626 \n",
            "epoch: 174, acc: 0.788, loss: 2.395 (data_loss: 2.353, reg_loss: 0.043), lr: 0.004999565 validation, acc: 0.830, loss: 1.744 Unseen, acc: 0.821, loss: 1.914 \n",
            "epoch: 175, acc: 0.820, loss: 2.099 (data_loss: 2.057, reg_loss: 0.043), lr: 0.004999563 validation, acc: 0.864, loss: 1.551 Unseen, acc: 0.854, loss: 1.735 \n",
            "epoch: 176, acc: 0.791, loss: 3.137 (data_loss: 3.095, reg_loss: 0.043), lr: 0.004999560 validation, acc: 0.807, loss: 2.574 Unseen, acc: 0.790, loss: 2.746 \n",
            "epoch: 177, acc: 0.791, loss: 3.030 (data_loss: 2.987, reg_loss: 0.043), lr: 0.004999558 validation, acc: 0.808, loss: 2.337 Unseen, acc: 0.795, loss: 2.490 \n",
            "epoch: 178, acc: 0.798, loss: 2.724 (data_loss: 2.681, reg_loss: 0.043), lr: 0.004999555 validation, acc: 0.809, loss: 2.084 Unseen, acc: 0.797, loss: 2.216 \n",
            "epoch: 179, acc: 0.800, loss: 2.596 (data_loss: 2.553, reg_loss: 0.043), lr: 0.004999553 validation, acc: 0.810, loss: 1.961 Unseen, acc: 0.797, loss: 2.085 \n",
            "epoch: 180, acc: 0.775, loss: 2.524 (data_loss: 2.481, reg_loss: 0.043), lr: 0.004999550 validation, acc: 0.810, loss: 1.786 Unseen, acc: 0.796, loss: 1.900 \n",
            "epoch: 181, acc: 0.798, loss: 2.286 (data_loss: 2.244, reg_loss: 0.043), lr: 0.004999548 validation, acc: 0.812, loss: 1.653 Unseen, acc: 0.798, loss: 1.761 \n",
            "epoch: 182, acc: 0.801, loss: 2.224 (data_loss: 2.181, reg_loss: 0.043), lr: 0.004999545 validation, acc: 0.815, loss: 1.575 Unseen, acc: 0.801, loss: 1.685 \n",
            "epoch: 183, acc: 0.803, loss: 2.052 (data_loss: 2.009, reg_loss: 0.043), lr: 0.004999543 validation, acc: 0.815, loss: 1.400 Unseen, acc: 0.802, loss: 1.499 \n",
            "epoch: 184, acc: 0.795, loss: 2.115 (data_loss: 2.072, reg_loss: 0.043), lr: 0.004999540 validation, acc: 0.818, loss: 1.371 Unseen, acc: 0.806, loss: 1.434 \n",
            "epoch: 185, acc: 0.789, loss: 1.992 (data_loss: 1.950, reg_loss: 0.042), lr: 0.004999538 validation, acc: 0.837, loss: 1.319 Unseen, acc: 0.827, loss: 1.322 \n",
            "epoch: 186, acc: 0.762, loss: 1.740 (data_loss: 1.698, reg_loss: 0.042), lr: 0.004999535 validation, acc: 0.855, loss: 1.219 Unseen, acc: 0.848, loss: 1.212 \n",
            "epoch: 187, acc: 0.862, loss: 1.399 (data_loss: 1.357, reg_loss: 0.042), lr: 0.004999533 validation, acc: 0.882, loss: 1.173 Unseen, acc: 0.877, loss: 1.167 \n",
            "epoch: 188, acc: 0.817, loss: 1.515 (data_loss: 1.473, reg_loss: 0.042), lr: 0.004999530 validation, acc: 0.811, loss: 1.384 Unseen, acc: 0.818, loss: 1.376 \n",
            "epoch: 189, acc: 0.778, loss: 1.762 (data_loss: 1.720, reg_loss: 0.042), lr: 0.004999528 validation, acc: 0.869, loss: 1.419 Unseen, acc: 0.867, loss: 1.421 \n",
            "epoch: 190, acc: 0.875, loss: 1.594 (data_loss: 1.553, reg_loss: 0.042), lr: 0.004999525 validation, acc: 0.867, loss: 1.291 Unseen, acc: 0.861, loss: 1.293 \n",
            "epoch: 191, acc: 0.871, loss: 1.384 (data_loss: 1.343, reg_loss: 0.042), lr: 0.004999523 validation, acc: 0.887, loss: 1.086 Unseen, acc: 0.879, loss: 1.096 \n",
            "epoch: 192, acc: 0.885, loss: 1.219 (data_loss: 1.178, reg_loss: 0.042), lr: 0.004999520 validation, acc: 0.895, loss: 0.907 Unseen, acc: 0.892, loss: 0.906 \n",
            "epoch: 193, acc: 0.894, loss: 1.056 (data_loss: 1.015, reg_loss: 0.041), lr: 0.004999518 validation, acc: 0.895, loss: 0.789 Unseen, acc: 0.890, loss: 0.789 \n",
            "epoch: 194, acc: 0.896, loss: 0.947 (data_loss: 0.906, reg_loss: 0.041), lr: 0.004999515 validation, acc: 0.892, loss: 0.725 Unseen, acc: 0.889, loss: 0.724 \n",
            "epoch: 195, acc: 0.783, loss: 1.037 (data_loss: 0.996, reg_loss: 0.041), lr: 0.004999513 validation, acc: 0.898, loss: 0.658 Unseen, acc: 0.896, loss: 0.656 \n",
            "epoch: 196, acc: 0.892, loss: 0.829 (data_loss: 0.788, reg_loss: 0.041), lr: 0.004999510 validation, acc: 0.898, loss: 0.671 Unseen, acc: 0.895, loss: 0.671 \n",
            "epoch: 197, acc: 0.864, loss: 0.873 (data_loss: 0.831, reg_loss: 0.041), lr: 0.004999508 validation, acc: 0.894, loss: 0.753 Unseen, acc: 0.891, loss: 0.754 \n",
            "epoch: 198, acc: 0.897, loss: 0.911 (data_loss: 0.869, reg_loss: 0.041), lr: 0.004999505 validation, acc: 0.893, loss: 0.836 Unseen, acc: 0.890, loss: 0.838 \n",
            "epoch: 199, acc: 0.895, loss: 1.014 (data_loss: 0.972, reg_loss: 0.041), lr: 0.004999503 validation, acc: 0.891, loss: 0.913 Unseen, acc: 0.890, loss: 0.912 \n",
            "epoch: 200, acc: 0.894, loss: 1.075 (data_loss: 1.034, reg_loss: 0.041), lr: 0.004999500 validation, acc: 0.883, loss: 0.950 Unseen, acc: 0.887, loss: 0.943 \n",
            "epoch: 201, acc: 0.888, loss: 1.247 (data_loss: 1.205, reg_loss: 0.041), lr: 0.004999498 validation, acc: 0.891, loss: 1.027 Unseen, acc: 0.893, loss: 1.020 \n",
            "epoch: 202, acc: 0.898, loss: 1.305 (data_loss: 1.264, reg_loss: 0.041), lr: 0.004999495 validation, acc: 0.891, loss: 1.070 Unseen, acc: 0.896, loss: 1.061 \n",
            "epoch: 203, acc: 0.886, loss: 1.272 (data_loss: 1.230, reg_loss: 0.041), lr: 0.004999493 validation, acc: 0.895, loss: 0.964 Unseen, acc: 0.896, loss: 0.961 \n",
            "epoch: 204, acc: 0.899, loss: 1.126 (data_loss: 1.085, reg_loss: 0.041), lr: 0.004999490 validation, acc: 0.897, loss: 0.890 Unseen, acc: 0.896, loss: 0.886 \n",
            "epoch: 205, acc: 0.900, loss: 1.042 (data_loss: 1.002, reg_loss: 0.041), lr: 0.004999488 validation, acc: 0.897, loss: 0.851 Unseen, acc: 0.896, loss: 0.851 \n",
            "epoch: 206, acc: 0.901, loss: 1.005 (data_loss: 0.965, reg_loss: 0.040), lr: 0.004999485 validation, acc: 0.897, loss: 0.823 Unseen, acc: 0.897, loss: 0.820 \n",
            "epoch: 207, acc: 0.898, loss: 0.980 (data_loss: 0.940, reg_loss: 0.040), lr: 0.004999483 validation, acc: 0.897, loss: 0.783 Unseen, acc: 0.897, loss: 0.781 \n",
            "epoch: 208, acc: 0.902, loss: 0.927 (data_loss: 0.888, reg_loss: 0.040), lr: 0.004999480 validation, acc: 0.898, loss: 0.734 Unseen, acc: 0.898, loss: 0.734 \n",
            "epoch: 209, acc: 0.866, loss: 1.032 (data_loss: 0.993, reg_loss: 0.039), lr: 0.004999478 validation, acc: 0.870, loss: 0.770 Unseen, acc: 0.874, loss: 0.755 \n",
            "epoch: 210, acc: 0.877, loss: 0.962 (data_loss: 0.923, reg_loss: 0.039), lr: 0.004999475 validation, acc: 0.887, loss: 0.665 Unseen, acc: 0.889, loss: 0.663 \n",
            "epoch: 211, acc: 0.890, loss: 0.798 (data_loss: 0.759, reg_loss: 0.039), lr: 0.004999473 validation, acc: 0.899, loss: 0.561 Unseen, acc: 0.898, loss: 0.563 \n",
            "epoch: 212, acc: 0.802, loss: 0.920 (data_loss: 0.882, reg_loss: 0.038), lr: 0.004999470 validation, acc: 0.877, loss: 0.604 Unseen, acc: 0.874, loss: 0.618 \n",
            "epoch: 213, acc: 0.872, loss: 0.730 (data_loss: 0.693, reg_loss: 0.038), lr: 0.004999468 validation, acc: 0.899, loss: 0.497 Unseen, acc: 0.898, loss: 0.500 \n",
            "epoch: 214, acc: 0.903, loss: 0.591 (data_loss: 0.554, reg_loss: 0.037), lr: 0.004999465 validation, acc: 0.899, loss: 0.471 Unseen, acc: 0.899, loss: 0.474 \n",
            "epoch: 215, acc: 0.802, loss: 0.802 (data_loss: 0.765, reg_loss: 0.037), lr: 0.004999463 validation, acc: 0.881, loss: 0.518 Unseen, acc: 0.877, loss: 0.531 \n",
            "epoch: 216, acc: 0.829, loss: 0.707 (data_loss: 0.670, reg_loss: 0.036), lr: 0.004999460 validation, acc: 0.886, loss: 0.465 Unseen, acc: 0.883, loss: 0.471 \n",
            "epoch: 217, acc: 0.868, loss: 0.591 (data_loss: 0.555, reg_loss: 0.036), lr: 0.004999458 validation, acc: 0.897, loss: 0.395 Unseen, acc: 0.894, loss: 0.396 \n",
            "epoch: 218, acc: 0.881, loss: 0.479 (data_loss: 0.444, reg_loss: 0.036), lr: 0.004999455 validation, acc: 0.897, loss: 0.329 Unseen, acc: 0.897, loss: 0.327 \n",
            "epoch: 219, acc: 0.902, loss: 0.392 (data_loss: 0.357, reg_loss: 0.035), lr: 0.004999453 validation, acc: 0.899, loss: 0.277 Unseen, acc: 0.898, loss: 0.277 \n",
            "epoch: 220, acc: 0.902, loss: 0.311 (data_loss: 0.276, reg_loss: 0.035), lr: 0.004999450 validation, acc: 0.942, loss: 0.224 Unseen, acc: 0.943, loss: 0.223 \n",
            "epoch: 221, acc: 0.944, loss: 0.231 (data_loss: 0.196, reg_loss: 0.035), lr: 0.004999448 validation, acc: 0.987, loss: 0.171 Unseen, acc: 0.984, loss: 0.171 \n",
            "epoch: 222, acc: 0.986, loss: 0.158 (data_loss: 0.124, reg_loss: 0.035), lr: 0.004999445 validation, acc: 0.996, loss: 0.107 Unseen, acc: 0.996, loss: 0.109 \n",
            "epoch: 223, acc: 0.982, loss: 0.140 (data_loss: 0.105, reg_loss: 0.035), lr: 0.004999443 validation, acc: 0.999, loss: 0.079 Unseen, acc: 0.998, loss: 0.080 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNnDUP_U8Xn",
        "outputId": "10a2b391-345b-4148-a5ef-8b30609c4e57"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8493465401785715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbXMisqQKqF",
        "outputId": "ff1f31f5-d8e2-4083-d2c1-edc065f16cbc"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 20 epochs\n",
            "Model hit 85% validation accuracy in 24 epochs\n",
            "Model hit 90% validation accuracy in 25 epochs\n",
            "Model hit 95% validation accuracy in 29 epochs\n",
            "Model hit 97.5% validation accuracy in 43 epochs\n",
            "Max accuracy was 99.85000000000001% at epoch 223.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwSXsZVU8Xo",
        "outputId": "235a6200-a940-47d9-bfc0-624bbd487d63"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_test)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dropout1.infrence(activation1.output,y_test)\n",
        "\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "index = 27\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {fashion_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {fashion_mnist_labels[y_test[index]]}')\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.000% Confident True is Sneaker. True is actually Sneaker\n",
            "Accuracy: 0.999, loss: 0.079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k0Ve2M0bPG3"
      },
      "source": [
        "training_diff = []\n",
        "testing_diff = []\n",
        "combined_diff = []"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByL_RwvlIx3"
      },
      "source": [
        "Individual Training Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTOnqnDXa0ME",
        "outputId": "0f1e081c-4d58-4063-9e4d-afc99cb04ee1"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x, sorted_y)):\n",
        "  accuracy = Accuracy_Categorical()\n",
        "\n",
        "  y = sorted_y[y_sorted_lists]\n",
        "  X = sorted_x[X_sorted_lists]\n",
        "  accuracy.init(y)\n",
        "\n",
        "  dense1.forward(X)\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "  train_train_mean = activation1.output\n",
        "\n",
        "  dropout1.infrence(activation1.output,y)\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, y)\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, y)\n",
        "  print(f'{fashion_mnist_labels[classes]} Train Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')\n",
        "\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T-shirt/top Train Accuracy: 0.995, loss: 0.434\n",
            "Trouser Train Accuracy: 0.998, loss: 0.007\n",
            "Pullover Train Accuracy: 1.000, loss: 0.073\n",
            "Dress Train Accuracy: 1.000, loss: 0.000\n",
            "Coat Train Accuracy: 0.999, loss: 0.013\n",
            "Sandal Train Accuracy: 1.000, loss: 0.073\n",
            "Shirt Train Accuracy: 0.994, loss: 0.049\n",
            "Sneaker Train Accuracy: 1.000, loss: 0.000\n",
            "Bag Train Accuracy: 1.000, loss: 0.129\n",
            "Ankle boot Train Accuracy: 0.994, loss: 0.026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scjb7Wh_sn6b",
        "outputId": "464cb7ec-4c89-4f9c-8bfe-48348a22f211"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "for classes, (X_sorted_lists, y_sorted_lists) in enumerate(zip(sorted_x_val, sorted_y_val)):\n",
        "  accuracy.init(sorted_y_val[y_sorted_lists])\n",
        "  #print(sorted_y[y_sorted_lists].shape)\n",
        "  #print(sorted_x[X_sorted_lists].shape)\n",
        "  dense1.forward(sorted_x_val[X_sorted_lists])\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  testmean = np.mean(activation1.output, axis=0)\n",
        "  testing_diff.append(testmean)\n",
        "  dropout1.infrence(activation1.output,sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  dense4.forward(activation3.output)\n",
        "\n",
        "  activation4.forward(dense4.output)\n",
        "  # Calculate the data loss\n",
        "  loss = loss_function.calculate(activation4.output, sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  predictions = activation4.predictions(activation4.output)\n",
        "  testaccuracy = accuracy.calculate(predictions, sorted_y_val[y_sorted_lists])\n",
        "\n",
        "  print(f'{fashion_mnist_labels[classes]} Test Accuracy: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T-shirt/top Test Accuracy: 0.999, loss: 0.429\n",
            "Trouser Test Accuracy: 1.000, loss: 0.006\n",
            "Pullover Test Accuracy: 0.999, loss: 0.079\n",
            "Dress Test Accuracy: 1.000, loss: 0.000\n",
            "Coat Test Accuracy: 1.000, loss: 0.011\n",
            "Sandal Test Accuracy: 1.000, loss: 0.074\n",
            "Shirt Test Accuracy: 0.992, loss: 0.041\n",
            "Sneaker Test Accuracy: 1.000, loss: 0.000\n",
            "Bag Test Accuracy: 1.000, loss: 0.129\n",
            "Ankle boot Test Accuracy: 0.991, loss: 0.033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u-O8oNZ0qA"
      },
      "source": [
        "# Full mnist test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbD4KrLMnTcR"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMfBGUHeZ4L5",
        "outputId": "8885311a-886d-4bbb-de7b-a797e54aa4fd"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(label)\n",
        "\n",
        "dense1.forward(input)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "train_train_mean = activation1.output\n",
        "\n",
        "dropout1.infrence(activation1.output,label)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, label)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, label)\n",
        "\n",
        "print(f'Found {input.shape[0]} images belonging to {len(set(label))} unique classes')\n",
        "\n",
        "print(f'Full Training Accuracy: {testaccuracy:.5f}, loss: {loss:.3f}')"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 60000 images belonging to 10 unique classes\n",
            "Full Training Accuracy: 0.99785, loss: 0.080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aat-uVF6nYu7"
      },
      "source": [
        "Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hyU1tBDna8x",
        "outputId": "27acaf3a-79c3-4da7-a581-1c18228c0a1c"
      },
      "source": [
        "\n",
        "(X, y), (X_val, y_val) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_val = (X_val.reshape(X_val.shape[0], -1).astype(np.float32) - 127.5) / 127.5 # Reshape X_val if cell below was already ran\n",
        "\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y_val)\n",
        "\n",
        "dense1.forward(X_val)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dropout1.infrence(activation1.output,y_val)\n",
        "\n",
        "dense2.forward(dropout1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "dense4.forward(activation3.output)\n",
        "\n",
        "activation4.forward(dense4.output)\n",
        "\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_val)\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_val)\n",
        "\n",
        "print(f'Found {X_val.shape[0]} images belonging to {len(set(y_val))} unique classes')\n",
        "print(f'Full Testing Accuracy: {testaccuracy:.5f}, loss: {loss:.3f}')"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 10 unique classes\n",
            "Full Testing Accuracy: 0.99810, loss: 0.080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3JzV--WNNT7",
        "outputId": "5a18443a-0d9e-4606-da85-03934237e037"
      },
      "source": [
        "for index in range(10000):\n",
        "  if y_val[index] != np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]:\n",
        "    print(index)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1645\n",
            "1907\n",
            "2447\n",
            "2551\n",
            "3130\n",
            "3880\n",
            "4497\n",
            "4615\n",
            "4743\n",
            "5248\n",
            "5249\n",
            "5512\n",
            "6452\n",
            "6596\n",
            "7800\n",
            "8211\n",
            "8763\n",
            "9474\n",
            "9674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbIMZ7Pk_Tnp"
      },
      "source": [
        "Change idex to get confidence of different samples of testing data. Index values 0-1600 were refrenced in training. Anything past was never seen during training. Lowest confidence is at index 2732 when trained with 488 epochs and numpy seed set to 22."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "JaxWcRIr_BCV",
        "outputId": "39693f0d-bd0a-4ee1-b203-1f5f768e1299"
      },
      "source": [
        "index = 7800\n",
        "\n",
        "print(f'{(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0]*100):.3f}% Confident True is {fashion_mnist_labels[np.where(activation4.output[index] == np.amax(activation4.output[index]))[0][0]]}. True is actually {fashion_mnist_labels[y_val[index]]}')\n",
        "\n",
        "X_val.resize(X_val.shape[0],28,28)\n",
        "image = X_val[index]\n",
        "fig = plt.figure\n",
        "plt.title(f'{fashion_mnist_labels[y_val[index]]}')\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68.429% Confident True is Ankle boot. True is actually Shirt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6KeoLkg7k0u",
        "outputId": "b36c207d-5453-4477-b135-87ea1ae0c958"
      },
      "source": [
        "confidence_list = []\n",
        "for index in range(10000):\n",
        "  confidence_list.append(activation4.output[index][np.where(activation4.output[index] == np.amax(activation4.output[index]))][0])\n",
        "\n",
        "print(confidence_list.index(min(confidence_list)))\n",
        "\n",
        "a = confidence_list[:] \n",
        "a.sort()\n",
        "print(confidence_list.index(a[1]))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7818\n",
            "5710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "h5c5xUTNXk2v",
        "outputId": "22b28c52-b60f-4962-ff7c-363dc1878dce"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}