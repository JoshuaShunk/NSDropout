{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist implementation of New_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('AITraining': virtualenvwrapper)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "interpreter": {
      "hash": "4a10260d53feadc3aac998a3ba3a60b43aac84189bada71a196791e24306642d"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaShunk/NSDropout/blob/main/mnist_implementation_of_New_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2GytIidUnpd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLxFoLMU2jC"
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOLv3jSjsxV"
      },
      "source": [
        "np.random.seed(seed=50)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX886WxClQ07",
        "outputId": "41988f5f-edc3-44a6-ccfe-a594fdebeea2"
      },
      "source": [
        "print(np.random.random(size=3))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.49460165 0.2280831  0.25547392]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkY3EiBU4tR",
        "cellView": "form"
      },
      "source": [
        "#@title Load Layers\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ReLU activation\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input and calculate/save output\n",
        "        # of the sigmoid function\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Derivative - calculates from output of the sigmoid function\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "                             (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "                           (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.02, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                                         (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                          self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        bias_momentums_corrected / \\\n",
        "                        (np.sqrt(bias_cache_corrected) +\n",
        "                         self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self, layer):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # L1 regularization - weights\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights *\n",
        "                                          layer.weights)\n",
        "\n",
        "        # L1 regularization - biases\n",
        "        # calculate only when factor greater than 0\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases *\n",
        "                                          layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "        # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate sample-wise loss\n",
        "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
        "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "        sample_losses = np.mean(sample_losses, axis=-1)\n",
        "\n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        outputs = len(dvalues[0])\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -(y_true / clipped_dvalues -\n",
        "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "\n",
        "# Accuracy calculation for regression model\n",
        "class Accuracy_Regression(Accuracy):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create precision property\n",
        "        self.precision = None\n",
        "\n",
        "    # Calculates precision value\n",
        "    # based on passed-in ground truth values\n",
        "    def init(self, y, reinit=False):\n",
        "        if self.precision is None or reinit:\n",
        "            self.precision = np.std(y) / 250\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        return np.absolute(predictions - y) < self.precision\n",
        "\n",
        "class model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, classes, samples):\n",
        "        self.classes = classes\n",
        "        self.samples = samples\n",
        "        self.X, self.y = spiral_data(samples=self.samples, classes=self.classes)\n",
        "        dense1.forward(self.X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        # Calculate the data loss\n",
        "        self.loss = loss_function.calculate(activation2.output, self.y)\n",
        "        self.predictions = (activation2.output > 0.5) * 1\n",
        "        self.accuracy = np.mean(self.predictions == self.y)\n",
        "        print(f'Accuracy: {self.accuracy}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4GFMbIPUkI"
      },
      "source": [
        "# Old Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoiO43tPbTa"
      },
      "source": [
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                              size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "       \n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "        #print(self.dinputs.shape)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_Og9ZrbKtV"
      },
      "source": [
        "# New Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiuCzwWxbRl0"
      },
      "source": [
        "class Layer_NewDropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        self.rate = 1 - rate\n",
        "        self.iterations = 0\n",
        "\n",
        "    def forward(self, inputs, val_inputs):\n",
        "        self.inputs = inputs\n",
        "        self.val_inputs = val_inputs\n",
        "        #print(len(self.inputs[0]))\n",
        "        nummask = round(len(self.inputs[0]) * self.rate)\n",
        "        \n",
        "        #Averaging Values\n",
        "        self.meanarray1 = np.mean(inputs, axis=0)\n",
        "        self.meanarray2 = np.mean(val_inputs, axis=0)\n",
        "\n",
        "        #if self.iterations % 10 and self.iterations != 0:\n",
        "        if self.iterations != 0:\n",
        "            # Calculating value\n",
        "            #print(self.iterations)\n",
        "            self.difference = self.meanarray1 - self.meanarray2\n",
        "            #print(self.difference)\n",
        "            ind = np.argpartition(self.difference, -nummask)[-nummask:]\n",
        "            #print(ind)\n",
        "            mask = np.ones(self.meanarray1.shape, dtype=bool)\n",
        "            mask[ind] = False\n",
        "            #print(ind)\n",
        "            self.difference[~mask] = 1\n",
        "            self.difference[mask] = np.random.random(1)[0]\n",
        "            #print(self.difference / self.rate)\n",
        "            self.binary_mask = self.difference #/ self.rate\n",
        "            #print(self.binary_mask)\n",
        "        else:\n",
        "            self.binary_mask = np.random.binomial(1, self.rate,\n",
        "                                                  size=inputs.shape) #/ self.rate\n",
        "            #print(self.binary_mask)\n",
        "        self.output = inputs * self.binary_mask\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUEiUOYDrL89"
      },
      "source": [
        "# Creating Spiral Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq_mQiW6rQw0"
      },
      "source": [
        "def twospirals(n_points, noise=.5):\n",
        "    n = np.sqrt(np.random.rand(n_points,1)) * 780 * (2*np.pi)/360\n",
        "    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
        "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
        "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n",
        "            np.hstack((np.zeros(n_points),np.ones(n_points))))"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRB57nFublm3"
      },
      "source": [
        "Initializing Caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kyAX0txV-cF"
      },
      "source": [
        "loss_cache = []\n",
        "val_loss_cache = []\n",
        "acc_cache = []\n",
        "val_acc_cache = []\n",
        "lr_cache = []\n",
        "epoch_cache = []\n",
        "test_acc_cache = []\n",
        "test_loss_cache = []\n",
        "\n",
        "max_val_accuracyint = 0"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7VWnIlF8yx"
      },
      "source": [
        "Initializing Summary List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnu5VToGAq0"
      },
      "source": [
        "summary = []"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Eu0pm-WjKI"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-24YuBKre0f"
      },
      "source": [
        "Vizulizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kUOJ9avrho8",
        "outputId": "3d991fed-9f0b-4d17-964a-7e53a107a23f"
      },
      "source": [
        "(X, y), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n",
            "(10000, 784)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_dSHDNW1Rn"
      },
      "source": [
        "# Initializing Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aly5fwUCW_4l"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4,\n",
        "                     bias_regularizer_l2=5e-4)\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dropout1 = Layer_NewDropout(0.2)\n",
        "\n",
        "dense2 = Layer_Dense(128, 128)\n",
        "\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(128,128)\n",
        "\n",
        "activation3 = Activation_ReLU()\n",
        "\n",
        "dense4 = Layer_Dense(128,10)\n",
        "\n",
        "activation4 = Activation_Softmax()\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(decay=5e-7)\n",
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xmbxDuwXIBk"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yHOjq9XLee",
        "outputId": "7ab5d4be-b14d-4f84-ead0-00040008bd36"
      },
      "source": [
        "epochs = 500\n",
        "dips = 0\n",
        "accuracy_count = 0\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "    dense1.forward(X)\n",
        "\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    if epoch != 0:\n",
        "      cached_val_inputs = cached_val_inputs\n",
        "      cached_train_inputs = activation1.output\n",
        "    else:\n",
        "      cached_val_inputs = np.random.random(size=128) #Never used\n",
        "      cached_train_inputs = activation1.output\n",
        "\n",
        "    dropout1.forward(inputs=activation1.output, val_inputs=cached_val_inputs)\n",
        "\n",
        "    dense2.forward(dropout1.output)\n",
        "\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    dense4.forward(activation3.output)\n",
        "\n",
        "    activation4.forward(dense4.output)\n",
        "\n",
        "    # Calculate the data loss\n",
        "    data_loss = loss_function.calculate(activation4.output, y)\n",
        "    regularization_loss = \\\n",
        "      loss_function.regularization_loss(dense1) + \\\n",
        "      loss_function.regularization_loss(dense2) + \\\n",
        "      loss_function.regularization_loss(dense3) + \\\n",
        "      loss_function.regularization_loss(dense4) \n",
        "    loss = data_loss + regularization_loss\n",
        "    \n",
        "    #Accuracy\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    train_accuracy = accuracy.calculate(predictions, y)\n",
        "\n",
        "    # Backward pass\n",
        "    softmax_classifier_output.backward(activation4.output, y)\n",
        "    activation4.backward(softmax_classifier_output.dinputs)\n",
        "    dense4.backward(activation4.dinputs)\n",
        "    activation3.backward(dense4.dinputs)\n",
        "    dense3.backward(activation3.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "    \n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense4)\n",
        "    optimizer.post_update_params()\n",
        "    dropout1.post_update_params()\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_test)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    dense1_outputs = dense1.output\n",
        "    meanarray = np.mean(dense1.output, axis=0)\n",
        "    cached_val_inputs = activation1.output\n",
        " \n",
        "    trainout = meanarray\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "    dense4.forward(activation3.output)\n",
        "    activation4.forward(dense4.output)\n",
        "    # Calculate the data loss\n",
        "    valloss = loss_function.calculate(activation4.output, y_test)\n",
        "    predictions = activation4.predictions(activation4.output)\n",
        "    valaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "    #Updating List\n",
        "    loss_cache.append(loss)\n",
        "    val_loss_cache.append(valloss)\n",
        "    acc_cache.append(train_accuracy)\n",
        "    val_acc_cache.append(valaccuracy)\n",
        "    lr_cache.append(optimizer.current_learning_rate)\n",
        "    epoch_cache.append(epoch)\n",
        " \n",
        "\n",
        "\n",
        "    #Summary Items\n",
        "    if valaccuracy >= .8 and len(summary) == 0:\n",
        "        nintypercent = f'Model hit 80% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .85 and len(summary) == 1:\n",
        "        nintypercent = f'Model hit 85% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .9 and len(summary) == 2:\n",
        "        nintypercent = f'Model hit 90% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .95 and len(summary) == 3:\n",
        "        nintypercent = f'Model hit 95% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if valaccuracy >= .975 and len(summary) == 4:\n",
        "        nintypercent = f'Model hit 97.5% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)  \n",
        "    if valaccuracy >= 1 and len(summary) == 5:\n",
        "        nintypercent = f'Model hit 100% validation accuracy in {epoch} epochs'\n",
        "        summary.append(nintypercent)\n",
        "    if epoch == epochs:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'\n",
        "        summary.append(max_val_accuracy)\n",
        "      else:\n",
        "        summary.append(max_val_accuracy)\n",
        "    else:\n",
        "      if valaccuracy > max_val_accuracyint:\n",
        "        max_val_accuracyint = valaccuracy\n",
        "        max_val_accuracy = f'Max accuracy was {valaccuracy * 100}% at epoch {epoch}.'     \n",
        "    \n",
        "    if not epoch % 1:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {train_accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' +\n",
        "              f'data_loss: {data_loss:.3f}, ' +\n",
        "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "              f'lr: {optimizer.current_learning_rate:.9f} ' +\n",
        "              f'validation, acc: {valaccuracy:.3f}, loss: {valloss:.3f} ')"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.100, loss: 2.308 (data_loss: 2.303, reg_loss: 0.005), lr: 0.020000000 validation, acc: 0.100, loss: 2.304 \n",
            "epoch: 1, acc: 0.100, loss: 2.312 (data_loss: 2.304, reg_loss: 0.008), lr: 0.019999990 validation, acc: 0.036, loss: 2.368 \n",
            "epoch: 2, acc: 0.046, loss: 2.358 (data_loss: 2.347, reg_loss: 0.011), lr: 0.019999980 validation, acc: 0.179, loss: 2.180 \n",
            "epoch: 3, acc: 0.171, loss: 2.200 (data_loss: 2.188, reg_loss: 0.012), lr: 0.019999970 validation, acc: 0.164, loss: 2.707 \n",
            "epoch: 4, acc: 0.150, loss: 2.824 (data_loss: 2.808, reg_loss: 0.016), lr: 0.019999960 validation, acc: 0.190, loss: 2.266 \n",
            "epoch: 5, acc: 0.100, loss: 2.681 (data_loss: 2.663, reg_loss: 0.017), lr: 0.019999950 validation, acc: 0.202, loss: 2.095 \n",
            "epoch: 6, acc: 0.200, loss: 2.132 (data_loss: 2.112, reg_loss: 0.019), lr: 0.019999940 validation, acc: 0.192, loss: 2.524 \n",
            "epoch: 7, acc: 0.191, loss: 2.528 (data_loss: 2.505, reg_loss: 0.023), lr: 0.019999930 validation, acc: 0.187, loss: 2.145 \n",
            "epoch: 8, acc: 0.166, loss: 2.225 (data_loss: 2.199, reg_loss: 0.026), lr: 0.019999920 validation, acc: 0.245, loss: 2.064 \n",
            "epoch: 9, acc: 0.180, loss: 2.132 (data_loss: 2.102, reg_loss: 0.030), lr: 0.019999910 validation, acc: 0.267, loss: 1.974 \n",
            "epoch: 10, acc: 0.291, loss: 1.983 (data_loss: 1.948, reg_loss: 0.034), lr: 0.019999900 validation, acc: 0.263, loss: 1.893 \n",
            "epoch: 11, acc: 0.241, loss: 2.042 (data_loss: 2.003, reg_loss: 0.039), lr: 0.019999890 validation, acc: 0.190, loss: 2.181 \n",
            "epoch: 12, acc: 0.256, loss: 1.902 (data_loss: 1.860, reg_loss: 0.042), lr: 0.019999880 validation, acc: 0.210, loss: 2.100 \n",
            "epoch: 13, acc: 0.219, loss: 2.094 (data_loss: 2.049, reg_loss: 0.045), lr: 0.019999870 validation, acc: 0.320, loss: 1.679 \n",
            "epoch: 14, acc: 0.257, loss: 2.014 (data_loss: 1.968, reg_loss: 0.047), lr: 0.019999860 validation, acc: 0.262, loss: 1.944 \n",
            "epoch: 15, acc: 0.310, loss: 1.827 (data_loss: 1.779, reg_loss: 0.049), lr: 0.019999850 validation, acc: 0.271, loss: 1.877 \n",
            "epoch: 16, acc: 0.284, loss: 1.829 (data_loss: 1.779, reg_loss: 0.050), lr: 0.019999840 validation, acc: 0.344, loss: 1.688 \n",
            "epoch: 17, acc: 0.343, loss: 1.730 (data_loss: 1.679, reg_loss: 0.051), lr: 0.019999830 validation, acc: 0.361, loss: 1.640 \n",
            "epoch: 18, acc: 0.283, loss: 1.907 (data_loss: 1.856, reg_loss: 0.051), lr: 0.019999820 validation, acc: 0.404, loss: 1.714 \n",
            "epoch: 19, acc: 0.402, loss: 1.785 (data_loss: 1.734, reg_loss: 0.051), lr: 0.019999810 validation, acc: 0.336, loss: 1.760 \n",
            "epoch: 20, acc: 0.359, loss: 1.792 (data_loss: 1.741, reg_loss: 0.052), lr: 0.019999800 validation, acc: 0.373, loss: 1.609 \n",
            "epoch: 21, acc: 0.405, loss: 1.666 (data_loss: 1.614, reg_loss: 0.052), lr: 0.019999790 validation, acc: 0.452, loss: 1.445 \n",
            "epoch: 22, acc: 0.489, loss: 1.470 (data_loss: 1.418, reg_loss: 0.052), lr: 0.019999780 validation, acc: 0.497, loss: 1.345 \n",
            "epoch: 23, acc: 0.555, loss: 1.298 (data_loss: 1.246, reg_loss: 0.053), lr: 0.019999770 validation, acc: 0.503, loss: 1.424 \n",
            "epoch: 24, acc: 0.546, loss: 1.245 (data_loss: 1.192, reg_loss: 0.053), lr: 0.019999760 validation, acc: 0.345, loss: 3.821 \n",
            "epoch: 25, acc: 0.405, loss: 2.303 (data_loss: 2.248, reg_loss: 0.054), lr: 0.019999750 validation, acc: 0.504, loss: 1.784 \n",
            "epoch: 26, acc: 0.484, loss: 1.674 (data_loss: 1.619, reg_loss: 0.055), lr: 0.019999740 validation, acc: 0.482, loss: 1.762 \n",
            "epoch: 27, acc: 0.483, loss: 1.798 (data_loss: 1.740, reg_loss: 0.058), lr: 0.019999730 validation, acc: 0.522, loss: 1.328 \n",
            "epoch: 28, acc: 0.489, loss: 1.526 (data_loss: 1.465, reg_loss: 0.061), lr: 0.019999720 validation, acc: 0.465, loss: 1.373 \n",
            "epoch: 29, acc: 0.449, loss: 1.540 (data_loss: 1.474, reg_loss: 0.065), lr: 0.019999710 validation, acc: 0.449, loss: 1.589 \n",
            "epoch: 30, acc: 0.438, loss: 1.582 (data_loss: 1.513, reg_loss: 0.070), lr: 0.019999700 validation, acc: 0.452, loss: 1.469 \n",
            "epoch: 31, acc: 0.464, loss: 1.418 (data_loss: 1.344, reg_loss: 0.074), lr: 0.019999690 validation, acc: 0.455, loss: 1.361 \n",
            "epoch: 32, acc: 0.463, loss: 1.378 (data_loss: 1.300, reg_loss: 0.078), lr: 0.019999680 validation, acc: 0.524, loss: 1.309 \n",
            "epoch: 33, acc: 0.529, loss: 1.374 (data_loss: 1.293, reg_loss: 0.081), lr: 0.019999670 validation, acc: 0.576, loss: 1.147 \n",
            "epoch: 34, acc: 0.565, loss: 1.259 (data_loss: 1.175, reg_loss: 0.084), lr: 0.019999660 validation, acc: 0.571, loss: 1.153 \n",
            "epoch: 35, acc: 0.581, loss: 1.215 (data_loss: 1.128, reg_loss: 0.086), lr: 0.019999650 validation, acc: 0.612, loss: 1.041 \n",
            "epoch: 36, acc: 0.610, loss: 1.113 (data_loss: 1.025, reg_loss: 0.088), lr: 0.019999640 validation, acc: 0.601, loss: 1.016 \n",
            "epoch: 37, acc: 0.570, loss: 1.290 (data_loss: 1.202, reg_loss: 0.088), lr: 0.019999630 validation, acc: 0.431, loss: 1.775 \n",
            "epoch: 38, acc: 0.461, loss: 1.662 (data_loss: 1.572, reg_loss: 0.090), lr: 0.019999620 validation, acc: 0.477, loss: 2.012 \n",
            "epoch: 39, acc: 0.436, loss: 2.452 (data_loss: 2.361, reg_loss: 0.091), lr: 0.019999610 validation, acc: 0.494, loss: 1.887 \n",
            "epoch: 40, acc: 0.496, loss: 1.999 (data_loss: 1.907, reg_loss: 0.091), lr: 0.019999600 validation, acc: 0.503, loss: 1.518 \n",
            "epoch: 41, acc: 0.478, loss: 1.525 (data_loss: 1.434, reg_loss: 0.091), lr: 0.019999590 validation, acc: 0.380, loss: 3.221 \n",
            "epoch: 42, acc: 0.383, loss: 3.098 (data_loss: 3.007, reg_loss: 0.091), lr: 0.019999580 validation, acc: 0.312, loss: 2.642 \n",
            "epoch: 43, acc: 0.318, loss: 2.492 (data_loss: 2.401, reg_loss: 0.091), lr: 0.019999570 validation, acc: 0.423, loss: 1.530 \n",
            "epoch: 44, acc: 0.418, loss: 1.664 (data_loss: 1.572, reg_loss: 0.091), lr: 0.019999560 validation, acc: 0.450, loss: 1.434 \n",
            "epoch: 45, acc: 0.457, loss: 1.529 (data_loss: 1.438, reg_loss: 0.091), lr: 0.019999550 validation, acc: 0.511, loss: 1.363 \n",
            "epoch: 46, acc: 0.513, loss: 1.427 (data_loss: 1.337, reg_loss: 0.091), lr: 0.019999540 validation, acc: 0.471, loss: 1.578 \n",
            "epoch: 47, acc: 0.461, loss: 1.558 (data_loss: 1.468, reg_loss: 0.090), lr: 0.019999530 validation, acc: 0.562, loss: 1.384 \n",
            "epoch: 48, acc: 0.564, loss: 1.445 (data_loss: 1.355, reg_loss: 0.090), lr: 0.019999520 validation, acc: 0.594, loss: 1.177 \n",
            "epoch: 49, acc: 0.592, loss: 1.288 (data_loss: 1.200, reg_loss: 0.089), lr: 0.019999510 validation, acc: 0.603, loss: 1.156 \n",
            "epoch: 50, acc: 0.612, loss: 1.180 (data_loss: 1.092, reg_loss: 0.087), lr: 0.019999500 validation, acc: 0.629, loss: 1.078 \n",
            "epoch: 51, acc: 0.623, loss: 1.121 (data_loss: 1.035, reg_loss: 0.086), lr: 0.019999490 validation, acc: 0.644, loss: 1.067 \n",
            "epoch: 52, acc: 0.645, loss: 1.118 (data_loss: 1.033, reg_loss: 0.085), lr: 0.019999480 validation, acc: 0.643, loss: 1.069 \n",
            "epoch: 53, acc: 0.648, loss: 1.094 (data_loss: 1.011, reg_loss: 0.082), lr: 0.019999470 validation, acc: 0.652, loss: 0.980 \n",
            "epoch: 54, acc: 0.661, loss: 1.027 (data_loss: 0.947, reg_loss: 0.081), lr: 0.019999460 validation, acc: 0.651, loss: 1.028 \n",
            "epoch: 55, acc: 0.656, loss: 1.050 (data_loss: 0.971, reg_loss: 0.079), lr: 0.019999450 validation, acc: 0.682, loss: 1.080 \n",
            "epoch: 56, acc: 0.682, loss: 1.132 (data_loss: 1.056, reg_loss: 0.077), lr: 0.019999440 validation, acc: 0.686, loss: 1.033 \n",
            "epoch: 57, acc: 0.696, loss: 1.109 (data_loss: 1.034, reg_loss: 0.075), lr: 0.019999430 validation, acc: 0.637, loss: 1.032 \n",
            "epoch: 58, acc: 0.651, loss: 1.023 (data_loss: 0.951, reg_loss: 0.073), lr: 0.019999420 validation, acc: 0.684, loss: 0.899 \n",
            "epoch: 59, acc: 0.694, loss: 0.929 (data_loss: 0.858, reg_loss: 0.071), lr: 0.019999410 validation, acc: 0.709, loss: 0.868 \n",
            "epoch: 60, acc: 0.674, loss: 0.950 (data_loss: 0.881, reg_loss: 0.069), lr: 0.019999400 validation, acc: 0.702, loss: 0.890 \n",
            "epoch: 61, acc: 0.720, loss: 0.874 (data_loss: 0.807, reg_loss: 0.067), lr: 0.019999390 validation, acc: 0.698, loss: 0.918 \n",
            "epoch: 62, acc: 0.709, loss: 0.922 (data_loss: 0.856, reg_loss: 0.066), lr: 0.019999380 validation, acc: 0.731, loss: 0.831 \n",
            "epoch: 63, acc: 0.610, loss: 1.207 (data_loss: 1.143, reg_loss: 0.064), lr: 0.019999370 validation, acc: 0.684, loss: 0.957 \n",
            "epoch: 64, acc: 0.728, loss: 0.873 (data_loss: 0.810, reg_loss: 0.063), lr: 0.019999360 validation, acc: 0.702, loss: 0.914 \n",
            "epoch: 65, acc: 0.710, loss: 0.938 (data_loss: 0.876, reg_loss: 0.062), lr: 0.019999350 validation, acc: 0.689, loss: 0.922 \n",
            "epoch: 66, acc: 0.619, loss: 1.110 (data_loss: 1.049, reg_loss: 0.061), lr: 0.019999340 validation, acc: 0.714, loss: 0.878 \n",
            "epoch: 67, acc: 0.680, loss: 0.991 (data_loss: 0.931, reg_loss: 0.060), lr: 0.019999330 validation, acc: 0.709, loss: 0.926 \n",
            "epoch: 68, acc: 0.690, loss: 0.976 (data_loss: 0.916, reg_loss: 0.060), lr: 0.019999320 validation, acc: 0.686, loss: 1.013 \n",
            "epoch: 69, acc: 0.707, loss: 0.946 (data_loss: 0.887, reg_loss: 0.059), lr: 0.019999310 validation, acc: 0.702, loss: 0.915 \n",
            "epoch: 70, acc: 0.725, loss: 0.846 (data_loss: 0.788, reg_loss: 0.059), lr: 0.019999300 validation, acc: 0.690, loss: 0.992 \n",
            "epoch: 71, acc: 0.724, loss: 0.851 (data_loss: 0.792, reg_loss: 0.058), lr: 0.019999290 validation, acc: 0.741, loss: 0.812 \n",
            "epoch: 72, acc: 0.744, loss: 0.809 (data_loss: 0.751, reg_loss: 0.058), lr: 0.019999280 validation, acc: 0.746, loss: 0.805 \n",
            "epoch: 73, acc: 0.739, loss: 0.827 (data_loss: 0.770, reg_loss: 0.058), lr: 0.019999270 validation, acc: 0.713, loss: 0.914 \n",
            "epoch: 74, acc: 0.746, loss: 0.810 (data_loss: 0.752, reg_loss: 0.057), lr: 0.019999260 validation, acc: 0.543, loss: 1.844 \n",
            "epoch: 75, acc: 0.553, loss: 1.833 (data_loss: 1.775, reg_loss: 0.058), lr: 0.019999250 validation, acc: 0.611, loss: 1.727 \n",
            "epoch: 76, acc: 0.574, loss: 1.747 (data_loss: 1.688, reg_loss: 0.059), lr: 0.019999240 validation, acc: 0.641, loss: 1.436 \n",
            "epoch: 77, acc: 0.648, loss: 1.380 (data_loss: 1.317, reg_loss: 0.063), lr: 0.019999230 validation, acc: 0.624, loss: 1.559 \n",
            "epoch: 78, acc: 0.620, loss: 1.536 (data_loss: 1.468, reg_loss: 0.068), lr: 0.019999220 validation, acc: 0.610, loss: 1.675 \n",
            "epoch: 79, acc: 0.604, loss: 1.705 (data_loss: 1.632, reg_loss: 0.072), lr: 0.019999210 validation, acc: 0.662, loss: 1.271 \n",
            "epoch: 80, acc: 0.670, loss: 1.314 (data_loss: 1.237, reg_loss: 0.077), lr: 0.019999200 validation, acc: 0.662, loss: 1.213 \n",
            "epoch: 81, acc: 0.670, loss: 1.256 (data_loss: 1.174, reg_loss: 0.082), lr: 0.019999190 validation, acc: 0.671, loss: 1.175 \n",
            "epoch: 82, acc: 0.652, loss: 1.317 (data_loss: 1.230, reg_loss: 0.086), lr: 0.019999180 validation, acc: 0.663, loss: 1.155 \n",
            "epoch: 83, acc: 0.641, loss: 1.290 (data_loss: 1.200, reg_loss: 0.090), lr: 0.019999170 validation, acc: 0.617, loss: 1.298 \n",
            "epoch: 84, acc: 0.619, loss: 1.313 (data_loss: 1.220, reg_loss: 0.093), lr: 0.019999160 validation, acc: 0.651, loss: 1.176 \n",
            "epoch: 85, acc: 0.675, loss: 1.130 (data_loss: 1.034, reg_loss: 0.096), lr: 0.019999150 validation, acc: 0.636, loss: 1.108 \n",
            "epoch: 86, acc: 0.648, loss: 1.086 (data_loss: 0.989, reg_loss: 0.097), lr: 0.019999140 validation, acc: 0.656, loss: 1.027 \n",
            "epoch: 87, acc: 0.674, loss: 1.031 (data_loss: 0.933, reg_loss: 0.098), lr: 0.019999130 validation, acc: 0.675, loss: 1.001 \n",
            "epoch: 88, acc: 0.674, loss: 1.052 (data_loss: 0.954, reg_loss: 0.098), lr: 0.019999120 validation, acc: 0.593, loss: 1.229 \n",
            "epoch: 89, acc: 0.623, loss: 1.161 (data_loss: 1.064, reg_loss: 0.097), lr: 0.019999110 validation, acc: 0.702, loss: 0.964 \n",
            "epoch: 90, acc: 0.700, loss: 1.047 (data_loss: 0.952, reg_loss: 0.095), lr: 0.019999100 validation, acc: 0.728, loss: 0.834 \n",
            "epoch: 91, acc: 0.720, loss: 0.934 (data_loss: 0.840, reg_loss: 0.093), lr: 0.019999090 validation, acc: 0.681, loss: 0.991 \n",
            "epoch: 92, acc: 0.690, loss: 1.025 (data_loss: 0.933, reg_loss: 0.092), lr: 0.019999080 validation, acc: 0.709, loss: 0.925 \n",
            "epoch: 93, acc: 0.703, loss: 0.972 (data_loss: 0.882, reg_loss: 0.091), lr: 0.019999070 validation, acc: 0.722, loss: 0.943 \n",
            "epoch: 94, acc: 0.720, loss: 0.992 (data_loss: 0.903, reg_loss: 0.089), lr: 0.019999060 validation, acc: 0.727, loss: 0.889 \n",
            "epoch: 95, acc: 0.716, loss: 0.956 (data_loss: 0.869, reg_loss: 0.087), lr: 0.019999050 validation, acc: 0.720, loss: 0.840 \n",
            "epoch: 96, acc: 0.717, loss: 0.888 (data_loss: 0.804, reg_loss: 0.085), lr: 0.019999040 validation, acc: 0.744, loss: 0.816 \n",
            "epoch: 97, acc: 0.756, loss: 0.842 (data_loss: 0.760, reg_loss: 0.082), lr: 0.019999030 validation, acc: 0.744, loss: 0.830 \n",
            "epoch: 98, acc: 0.747, loss: 0.848 (data_loss: 0.769, reg_loss: 0.079), lr: 0.019999020 validation, acc: 0.697, loss: 1.000 \n",
            "epoch: 99, acc: 0.708, loss: 0.995 (data_loss: 0.919, reg_loss: 0.077), lr: 0.019999010 validation, acc: 0.735, loss: 0.773 \n",
            "epoch: 100, acc: 0.604, loss: 1.163 (data_loss: 1.089, reg_loss: 0.074), lr: 0.019999000 validation, acc: 0.733, loss: 0.914 \n",
            "epoch: 101, acc: 0.745, loss: 0.846 (data_loss: 0.774, reg_loss: 0.072), lr: 0.019998990 validation, acc: 0.707, loss: 1.053 \n",
            "epoch: 102, acc: 0.730, loss: 0.917 (data_loss: 0.847, reg_loss: 0.070), lr: 0.019998980 validation, acc: 0.721, loss: 0.928 \n",
            "epoch: 103, acc: 0.733, loss: 0.860 (data_loss: 0.792, reg_loss: 0.069), lr: 0.019998970 validation, acc: 0.733, loss: 0.863 \n",
            "epoch: 104, acc: 0.712, loss: 0.860 (data_loss: 0.793, reg_loss: 0.067), lr: 0.019998960 validation, acc: 0.718, loss: 0.974 \n",
            "epoch: 105, acc: 0.751, loss: 0.880 (data_loss: 0.815, reg_loss: 0.065), lr: 0.019998950 validation, acc: 0.757, loss: 0.801 \n",
            "epoch: 106, acc: 0.764, loss: 0.792 (data_loss: 0.728, reg_loss: 0.063), lr: 0.019998940 validation, acc: 0.765, loss: 0.708 \n",
            "epoch: 107, acc: 0.767, loss: 0.735 (data_loss: 0.673, reg_loss: 0.062), lr: 0.019998930 validation, acc: 0.745, loss: 0.722 \n",
            "epoch: 108, acc: 0.757, loss: 0.741 (data_loss: 0.681, reg_loss: 0.060), lr: 0.019998920 validation, acc: 0.755, loss: 0.717 \n",
            "epoch: 109, acc: 0.769, loss: 0.734 (data_loss: 0.676, reg_loss: 0.058), lr: 0.019998910 validation, acc: 0.747, loss: 0.745 \n",
            "epoch: 110, acc: 0.759, loss: 0.739 (data_loss: 0.683, reg_loss: 0.056), lr: 0.019998900 validation, acc: 0.671, loss: 0.932 \n",
            "epoch: 111, acc: 0.713, loss: 0.779 (data_loss: 0.725, reg_loss: 0.054), lr: 0.019998890 validation, acc: 0.753, loss: 0.737 \n",
            "epoch: 112, acc: 0.764, loss: 0.741 (data_loss: 0.689, reg_loss: 0.052), lr: 0.019998880 validation, acc: 0.765, loss: 0.703 \n",
            "epoch: 113, acc: 0.698, loss: 0.948 (data_loss: 0.898, reg_loss: 0.050), lr: 0.019998870 validation, acc: 0.758, loss: 0.690 \n",
            "epoch: 114, acc: 0.756, loss: 0.744 (data_loss: 0.695, reg_loss: 0.049), lr: 0.019998860 validation, acc: 0.747, loss: 0.732 \n",
            "epoch: 115, acc: 0.733, loss: 0.803 (data_loss: 0.755, reg_loss: 0.048), lr: 0.019998850 validation, acc: 0.772, loss: 0.699 \n",
            "epoch: 116, acc: 0.778, loss: 0.698 (data_loss: 0.650, reg_loss: 0.048), lr: 0.019998840 validation, acc: 0.768, loss: 0.734 \n",
            "epoch: 117, acc: 0.781, loss: 0.713 (data_loss: 0.666, reg_loss: 0.047), lr: 0.019998830 validation, acc: 0.763, loss: 0.768 \n",
            "epoch: 118, acc: 0.778, loss: 0.717 (data_loss: 0.670, reg_loss: 0.047), lr: 0.019998820 validation, acc: 0.717, loss: 0.895 \n",
            "epoch: 119, acc: 0.767, loss: 0.727 (data_loss: 0.681, reg_loss: 0.046), lr: 0.019998810 validation, acc: 0.748, loss: 0.813 \n",
            "epoch: 120, acc: 0.780, loss: 0.705 (data_loss: 0.659, reg_loss: 0.046), lr: 0.019998800 validation, acc: 0.781, loss: 0.684 \n",
            "epoch: 121, acc: 0.769, loss: 0.716 (data_loss: 0.671, reg_loss: 0.045), lr: 0.019998790 validation, acc: 0.799, loss: 0.635 \n",
            "epoch: 122, acc: 0.772, loss: 0.695 (data_loss: 0.650, reg_loss: 0.044), lr: 0.019998780 validation, acc: 0.698, loss: 0.931 \n",
            "epoch: 123, acc: 0.717, loss: 0.866 (data_loss: 0.823, reg_loss: 0.043), lr: 0.019998770 validation, acc: 0.731, loss: 0.831 \n",
            "epoch: 124, acc: 0.752, loss: 0.765 (data_loss: 0.722, reg_loss: 0.043), lr: 0.019998760 validation, acc: 0.742, loss: 0.738 \n",
            "epoch: 125, acc: 0.758, loss: 0.732 (data_loss: 0.690, reg_loss: 0.042), lr: 0.019998750 validation, acc: 0.760, loss: 0.683 \n",
            "epoch: 126, acc: 0.686, loss: 0.897 (data_loss: 0.855, reg_loss: 0.042), lr: 0.019998740 validation, acc: 0.762, loss: 0.711 \n",
            "epoch: 127, acc: 0.774, loss: 0.700 (data_loss: 0.658, reg_loss: 0.042), lr: 0.019998730 validation, acc: 0.747, loss: 0.795 \n",
            "epoch: 128, acc: 0.753, loss: 0.784 (data_loss: 0.742, reg_loss: 0.043), lr: 0.019998720 validation, acc: 0.737, loss: 0.860 \n",
            "epoch: 129, acc: 0.748, loss: 0.840 (data_loss: 0.796, reg_loss: 0.044), lr: 0.019998710 validation, acc: 0.747, loss: 0.768 \n",
            "epoch: 130, acc: 0.753, loss: 0.761 (data_loss: 0.717, reg_loss: 0.045), lr: 0.019998700 validation, acc: 0.702, loss: 0.966 \n",
            "epoch: 131, acc: 0.764, loss: 0.739 (data_loss: 0.693, reg_loss: 0.046), lr: 0.019998690 validation, acc: 0.742, loss: 0.779 \n",
            "epoch: 132, acc: 0.744, loss: 0.803 (data_loss: 0.756, reg_loss: 0.047), lr: 0.019998680 validation, acc: 0.725, loss: 0.791 \n",
            "epoch: 133, acc: 0.694, loss: 0.898 (data_loss: 0.850, reg_loss: 0.048), lr: 0.019998670 validation, acc: 0.739, loss: 0.794 \n",
            "epoch: 134, acc: 0.746, loss: 0.771 (data_loss: 0.722, reg_loss: 0.049), lr: 0.019998660 validation, acc: 0.584, loss: 1.481 \n",
            "epoch: 135, acc: 0.622, loss: 1.238 (data_loss: 1.187, reg_loss: 0.051), lr: 0.019998650 validation, acc: 0.707, loss: 0.957 \n",
            "epoch: 136, acc: 0.726, loss: 0.862 (data_loss: 0.810, reg_loss: 0.052), lr: 0.019998640 validation, acc: 0.657, loss: 0.997 \n",
            "epoch: 137, acc: 0.696, loss: 0.890 (data_loss: 0.836, reg_loss: 0.053), lr: 0.019998630 validation, acc: 0.739, loss: 0.791 \n",
            "epoch: 138, acc: 0.737, loss: 0.829 (data_loss: 0.774, reg_loss: 0.055), lr: 0.019998620 validation, acc: 0.713, loss: 0.948 \n",
            "epoch: 139, acc: 0.708, loss: 1.008 (data_loss: 0.952, reg_loss: 0.056), lr: 0.019998610 validation, acc: 0.716, loss: 0.882 \n",
            "epoch: 140, acc: 0.608, loss: 1.260 (data_loss: 1.202, reg_loss: 0.058), lr: 0.019998600 validation, acc: 0.694, loss: 0.930 \n",
            "epoch: 141, acc: 0.678, loss: 0.960 (data_loss: 0.900, reg_loss: 0.060), lr: 0.019998590 validation, acc: 0.710, loss: 0.896 \n",
            "epoch: 142, acc: 0.715, loss: 0.918 (data_loss: 0.856, reg_loss: 0.062), lr: 0.019998580 validation, acc: 0.748, loss: 0.749 \n",
            "epoch: 143, acc: 0.736, loss: 0.875 (data_loss: 0.812, reg_loss: 0.063), lr: 0.019998570 validation, acc: 0.729, loss: 0.794 \n",
            "epoch: 144, acc: 0.738, loss: 0.842 (data_loss: 0.779, reg_loss: 0.064), lr: 0.019998560 validation, acc: 0.715, loss: 0.836 \n",
            "epoch: 145, acc: 0.719, loss: 0.881 (data_loss: 0.817, reg_loss: 0.064), lr: 0.019998550 validation, acc: 0.751, loss: 0.752 \n",
            "epoch: 146, acc: 0.759, loss: 0.794 (data_loss: 0.730, reg_loss: 0.064), lr: 0.019998540 validation, acc: 0.736, loss: 0.737 \n",
            "epoch: 147, acc: 0.728, loss: 0.801 (data_loss: 0.737, reg_loss: 0.063), lr: 0.019998530 validation, acc: 0.771, loss: 0.697 \n",
            "epoch: 148, acc: 0.774, loss: 0.740 (data_loss: 0.678, reg_loss: 0.063), lr: 0.019998520 validation, acc: 0.759, loss: 0.704 \n",
            "epoch: 149, acc: 0.764, loss: 0.751 (data_loss: 0.690, reg_loss: 0.062), lr: 0.019998510 validation, acc: 0.770, loss: 0.681 \n",
            "epoch: 150, acc: 0.772, loss: 0.723 (data_loss: 0.663, reg_loss: 0.061), lr: 0.019998500 validation, acc: 0.769, loss: 0.674 \n",
            "epoch: 151, acc: 0.775, loss: 0.712 (data_loss: 0.652, reg_loss: 0.059), lr: 0.019998490 validation, acc: 0.769, loss: 0.650 \n",
            "epoch: 152, acc: 0.776, loss: 0.681 (data_loss: 0.623, reg_loss: 0.058), lr: 0.019998480 validation, acc: 0.768, loss: 0.636 \n",
            "epoch: 153, acc: 0.778, loss: 0.665 (data_loss: 0.609, reg_loss: 0.056), lr: 0.019998470 validation, acc: 0.766, loss: 0.648 \n",
            "epoch: 154, acc: 0.779, loss: 0.675 (data_loss: 0.620, reg_loss: 0.055), lr: 0.019998460 validation, acc: 0.763, loss: 0.649 \n",
            "epoch: 155, acc: 0.780, loss: 0.662 (data_loss: 0.609, reg_loss: 0.053), lr: 0.019998450 validation, acc: 0.774, loss: 0.621 \n",
            "epoch: 156, acc: 0.781, loss: 0.646 (data_loss: 0.595, reg_loss: 0.051), lr: 0.019998440 validation, acc: 0.777, loss: 0.611 \n",
            "epoch: 157, acc: 0.779, loss: 0.653 (data_loss: 0.604, reg_loss: 0.049), lr: 0.019998430 validation, acc: 0.759, loss: 0.675 \n",
            "epoch: 158, acc: 0.766, loss: 0.690 (data_loss: 0.643, reg_loss: 0.047), lr: 0.019998420 validation, acc: 0.766, loss: 0.636 \n",
            "epoch: 159, acc: 0.771, loss: 0.665 (data_loss: 0.619, reg_loss: 0.046), lr: 0.019998410 validation, acc: 0.774, loss: 0.601 \n",
            "epoch: 160, acc: 0.774, loss: 0.647 (data_loss: 0.603, reg_loss: 0.044), lr: 0.019998400 validation, acc: 0.761, loss: 0.719 \n",
            "epoch: 161, acc: 0.783, loss: 0.644 (data_loss: 0.602, reg_loss: 0.043), lr: 0.019998390 validation, acc: 0.733, loss: 0.804 \n",
            "epoch: 162, acc: 0.754, loss: 0.728 (data_loss: 0.686, reg_loss: 0.042), lr: 0.019998380 validation, acc: 0.765, loss: 0.623 \n",
            "epoch: 163, acc: 0.777, loss: 0.632 (data_loss: 0.592, reg_loss: 0.040), lr: 0.019998370 validation, acc: 0.771, loss: 0.629 \n",
            "epoch: 164, acc: 0.769, loss: 0.663 (data_loss: 0.624, reg_loss: 0.039), lr: 0.019998360 validation, acc: 0.762, loss: 0.666 \n",
            "epoch: 165, acc: 0.761, loss: 0.696 (data_loss: 0.657, reg_loss: 0.039), lr: 0.019998350 validation, acc: 0.785, loss: 0.603 \n",
            "epoch: 166, acc: 0.790, loss: 0.616 (data_loss: 0.577, reg_loss: 0.038), lr: 0.019998340 validation, acc: 0.782, loss: 0.629 \n",
            "epoch: 167, acc: 0.787, loss: 0.631 (data_loss: 0.593, reg_loss: 0.038), lr: 0.019998330 validation, acc: 0.760, loss: 0.740 \n",
            "epoch: 168, acc: 0.788, loss: 0.639 (data_loss: 0.601, reg_loss: 0.038), lr: 0.019998320 validation, acc: 0.614, loss: 1.501 \n",
            "epoch: 169, acc: 0.768, loss: 0.724 (data_loss: 0.686, reg_loss: 0.038), lr: 0.019998310 validation, acc: 0.756, loss: 0.729 \n",
            "epoch: 170, acc: 0.768, loss: 0.669 (data_loss: 0.632, reg_loss: 0.037), lr: 0.019998300 validation, acc: 0.766, loss: 0.690 \n",
            "epoch: 171, acc: 0.770, loss: 0.659 (data_loss: 0.621, reg_loss: 0.037), lr: 0.019998290 validation, acc: 0.766, loss: 0.720 \n",
            "epoch: 172, acc: 0.775, loss: 0.675 (data_loss: 0.638, reg_loss: 0.038), lr: 0.019998280 validation, acc: 0.781, loss: 0.659 \n",
            "epoch: 173, acc: 0.783, loss: 0.655 (data_loss: 0.617, reg_loss: 0.038), lr: 0.019998270 validation, acc: 0.764, loss: 0.684 \n",
            "epoch: 174, acc: 0.774, loss: 0.668 (data_loss: 0.630, reg_loss: 0.038), lr: 0.019998260 validation, acc: 0.766, loss: 0.688 \n",
            "epoch: 175, acc: 0.770, loss: 0.690 (data_loss: 0.652, reg_loss: 0.038), lr: 0.019998250 validation, acc: 0.729, loss: 0.791 \n",
            "epoch: 176, acc: 0.744, loss: 0.746 (data_loss: 0.708, reg_loss: 0.038), lr: 0.019998240 validation, acc: 0.751, loss: 0.761 \n",
            "epoch: 177, acc: 0.760, loss: 0.733 (data_loss: 0.694, reg_loss: 0.039), lr: 0.019998230 validation, acc: 0.752, loss: 0.725 \n",
            "epoch: 178, acc: 0.756, loss: 0.709 (data_loss: 0.669, reg_loss: 0.040), lr: 0.019998220 validation, acc: 0.742, loss: 0.772 \n",
            "epoch: 179, acc: 0.748, loss: 0.762 (data_loss: 0.721, reg_loss: 0.041), lr: 0.019998210 validation, acc: 0.782, loss: 0.652 \n",
            "epoch: 180, acc: 0.793, loss: 0.659 (data_loss: 0.617, reg_loss: 0.042), lr: 0.019998200 validation, acc: 0.775, loss: 0.653 \n",
            "epoch: 181, acc: 0.778, loss: 0.650 (data_loss: 0.607, reg_loss: 0.043), lr: 0.019998190 validation, acc: 0.777, loss: 0.664 \n",
            "epoch: 182, acc: 0.778, loss: 0.685 (data_loss: 0.641, reg_loss: 0.044), lr: 0.019998180 validation, acc: 0.716, loss: 0.891 \n",
            "epoch: 183, acc: 0.742, loss: 0.765 (data_loss: 0.721, reg_loss: 0.045), lr: 0.019998170 validation, acc: 0.769, loss: 0.693 \n",
            "epoch: 184, acc: 0.785, loss: 0.654 (data_loss: 0.609, reg_loss: 0.045), lr: 0.019998160 validation, acc: 0.771, loss: 0.695 \n",
            "epoch: 185, acc: 0.778, loss: 0.671 (data_loss: 0.625, reg_loss: 0.046), lr: 0.019998150 validation, acc: 0.782, loss: 0.664 \n",
            "epoch: 186, acc: 0.791, loss: 0.629 (data_loss: 0.583, reg_loss: 0.046), lr: 0.019998140 validation, acc: 0.787, loss: 0.650 \n",
            "epoch: 187, acc: 0.782, loss: 0.648 (data_loss: 0.602, reg_loss: 0.045), lr: 0.019998130 validation, acc: 0.734, loss: 0.857 \n",
            "epoch: 188, acc: 0.774, loss: 0.716 (data_loss: 0.672, reg_loss: 0.045), lr: 0.019998120 validation, acc: 0.762, loss: 0.702 \n",
            "epoch: 189, acc: 0.730, loss: 0.792 (data_loss: 0.748, reg_loss: 0.044), lr: 0.019998110 validation, acc: 0.686, loss: 1.032 \n",
            "epoch: 190, acc: 0.688, loss: 0.934 (data_loss: 0.891, reg_loss: 0.043), lr: 0.019998100 validation, acc: 0.704, loss: 0.946 \n",
            "epoch: 191, acc: 0.751, loss: 0.722 (data_loss: 0.679, reg_loss: 0.043), lr: 0.019998090 validation, acc: 0.726, loss: 0.881 \n",
            "epoch: 192, acc: 0.740, loss: 0.783 (data_loss: 0.740, reg_loss: 0.043), lr: 0.019998080 validation, acc: 0.752, loss: 0.766 \n",
            "epoch: 193, acc: 0.770, loss: 0.684 (data_loss: 0.642, reg_loss: 0.043), lr: 0.019998070 validation, acc: 0.723, loss: 0.898 \n",
            "epoch: 194, acc: 0.776, loss: 0.683 (data_loss: 0.640, reg_loss: 0.043), lr: 0.019998060 validation, acc: 0.740, loss: 0.798 \n",
            "epoch: 195, acc: 0.756, loss: 0.730 (data_loss: 0.687, reg_loss: 0.043), lr: 0.019998050 validation, acc: 0.749, loss: 0.758 \n",
            "epoch: 196, acc: 0.775, loss: 0.656 (data_loss: 0.613, reg_loss: 0.043), lr: 0.019998040 validation, acc: 0.749, loss: 0.756 \n",
            "epoch: 197, acc: 0.766, loss: 0.681 (data_loss: 0.637, reg_loss: 0.043), lr: 0.019998030 validation, acc: 0.760, loss: 0.704 \n",
            "epoch: 198, acc: 0.779, loss: 0.649 (data_loss: 0.605, reg_loss: 0.044), lr: 0.019998020 validation, acc: 0.767, loss: 0.711 \n",
            "epoch: 199, acc: 0.799, loss: 0.635 (data_loss: 0.592, reg_loss: 0.044), lr: 0.019998010 validation, acc: 0.746, loss: 0.893 \n",
            "epoch: 200, acc: 0.773, loss: 0.719 (data_loss: 0.676, reg_loss: 0.044), lr: 0.019998000 validation, acc: 0.775, loss: 0.668 \n",
            "epoch: 201, acc: 0.763, loss: 0.704 (data_loss: 0.660, reg_loss: 0.043), lr: 0.019997990 validation, acc: 0.785, loss: 0.633 \n",
            "epoch: 202, acc: 0.801, loss: 0.613 (data_loss: 0.570, reg_loss: 0.043), lr: 0.019997980 validation, acc: 0.779, loss: 0.656 \n",
            "epoch: 203, acc: 0.800, loss: 0.623 (data_loss: 0.580, reg_loss: 0.043), lr: 0.019997970 validation, acc: 0.784, loss: 0.637 \n",
            "epoch: 204, acc: 0.795, loss: 0.627 (data_loss: 0.585, reg_loss: 0.043), lr: 0.019997960 validation, acc: 0.790, loss: 0.620 \n",
            "epoch: 205, acc: 0.799, loss: 0.616 (data_loss: 0.574, reg_loss: 0.042), lr: 0.019997950 validation, acc: 0.793, loss: 0.609 \n",
            "epoch: 206, acc: 0.801, loss: 0.607 (data_loss: 0.566, reg_loss: 0.042), lr: 0.019997940 validation, acc: 0.788, loss: 0.596 \n",
            "epoch: 207, acc: 0.802, loss: 0.593 (data_loss: 0.552, reg_loss: 0.041), lr: 0.019997930 validation, acc: 0.800, loss: 0.566 \n",
            "epoch: 208, acc: 0.813, loss: 0.570 (data_loss: 0.530, reg_loss: 0.040), lr: 0.019997920 validation, acc: 0.809, loss: 0.554 \n",
            "epoch: 209, acc: 0.808, loss: 0.578 (data_loss: 0.538, reg_loss: 0.039), lr: 0.019997910 validation, acc: 0.793, loss: 0.605 \n",
            "epoch: 210, acc: 0.806, loss: 0.595 (data_loss: 0.557, reg_loss: 0.039), lr: 0.019997900 validation, acc: 0.807, loss: 0.563 \n",
            "epoch: 211, acc: 0.806, loss: 0.574 (data_loss: 0.536, reg_loss: 0.038), lr: 0.019997890 validation, acc: 0.802, loss: 0.569 \n",
            "epoch: 212, acc: 0.806, loss: 0.590 (data_loss: 0.553, reg_loss: 0.037), lr: 0.019997880 validation, acc: 0.786, loss: 0.635 \n",
            "epoch: 213, acc: 0.810, loss: 0.587 (data_loss: 0.551, reg_loss: 0.036), lr: 0.019997870 validation, acc: 0.806, loss: 0.576 \n",
            "epoch: 214, acc: 0.810, loss: 0.575 (data_loss: 0.540, reg_loss: 0.035), lr: 0.019997860 validation, acc: 0.809, loss: 0.544 \n",
            "epoch: 215, acc: 0.804, loss: 0.571 (data_loss: 0.537, reg_loss: 0.034), lr: 0.019997850 validation, acc: 0.744, loss: 0.734 \n",
            "epoch: 216, acc: 0.790, loss: 0.629 (data_loss: 0.596, reg_loss: 0.033), lr: 0.019997840 validation, acc: 0.779, loss: 0.644 \n",
            "epoch: 217, acc: 0.797, loss: 0.621 (data_loss: 0.589, reg_loss: 0.032), lr: 0.019997830 validation, acc: 0.791, loss: 0.590 \n",
            "epoch: 218, acc: 0.782, loss: 0.627 (data_loss: 0.595, reg_loss: 0.032), lr: 0.019997820 validation, acc: 0.788, loss: 0.617 \n",
            "epoch: 219, acc: 0.801, loss: 0.601 (data_loss: 0.569, reg_loss: 0.032), lr: 0.019997810 validation, acc: 0.780, loss: 0.627 \n",
            "epoch: 220, acc: 0.795, loss: 0.600 (data_loss: 0.568, reg_loss: 0.032), lr: 0.019997800 validation, acc: 0.780, loss: 0.613 \n",
            "epoch: 221, acc: 0.804, loss: 0.586 (data_loss: 0.554, reg_loss: 0.032), lr: 0.019997790 validation, acc: 0.803, loss: 0.568 \n",
            "epoch: 222, acc: 0.816, loss: 0.571 (data_loss: 0.538, reg_loss: 0.033), lr: 0.019997780 validation, acc: 0.809, loss: 0.545 \n",
            "epoch: 223, acc: 0.774, loss: 0.673 (data_loss: 0.640, reg_loss: 0.033), lr: 0.019997770 validation, acc: 0.746, loss: 0.716 \n",
            "epoch: 224, acc: 0.759, loss: 0.688 (data_loss: 0.654, reg_loss: 0.034), lr: 0.019997760 validation, acc: 0.745, loss: 0.718 \n",
            "epoch: 225, acc: 0.731, loss: 0.812 (data_loss: 0.778, reg_loss: 0.034), lr: 0.019997750 validation, acc: 0.750, loss: 0.755 \n",
            "epoch: 226, acc: 0.757, loss: 0.730 (data_loss: 0.695, reg_loss: 0.035), lr: 0.019997740 validation, acc: 0.752, loss: 0.796 \n",
            "epoch: 227, acc: 0.774, loss: 0.745 (data_loss: 0.709, reg_loss: 0.036), lr: 0.019997730 validation, acc: 0.777, loss: 0.691 \n",
            "epoch: 228, acc: 0.786, loss: 0.675 (data_loss: 0.638, reg_loss: 0.037), lr: 0.019997720 validation, acc: 0.774, loss: 0.669 \n",
            "epoch: 229, acc: 0.758, loss: 0.723 (data_loss: 0.685, reg_loss: 0.038), lr: 0.019997710 validation, acc: 0.780, loss: 0.637 \n",
            "epoch: 230, acc: 0.782, loss: 0.641 (data_loss: 0.603, reg_loss: 0.039), lr: 0.019997700 validation, acc: 0.771, loss: 0.689 \n",
            "epoch: 231, acc: 0.781, loss: 0.689 (data_loss: 0.649, reg_loss: 0.040), lr: 0.019997690 validation, acc: 0.788, loss: 0.630 \n",
            "epoch: 232, acc: 0.798, loss: 0.633 (data_loss: 0.592, reg_loss: 0.041), lr: 0.019997680 validation, acc: 0.786, loss: 0.634 \n",
            "epoch: 233, acc: 0.804, loss: 0.606 (data_loss: 0.564, reg_loss: 0.042), lr: 0.019997670 validation, acc: 0.762, loss: 0.705 \n",
            "epoch: 234, acc: 0.802, loss: 0.614 (data_loss: 0.571, reg_loss: 0.043), lr: 0.019997660 validation, acc: 0.720, loss: 0.788 \n",
            "epoch: 235, acc: 0.805, loss: 0.617 (data_loss: 0.573, reg_loss: 0.044), lr: 0.019997650 validation, acc: 0.673, loss: 1.005 \n",
            "epoch: 236, acc: 0.692, loss: 0.979 (data_loss: 0.936, reg_loss: 0.044), lr: 0.019997640 validation, acc: 0.642, loss: 1.773 \n",
            "epoch: 237, acc: 0.650, loss: 1.792 (data_loss: 1.746, reg_loss: 0.046), lr: 0.019997630 validation, acc: 0.641, loss: 1.832 \n",
            "epoch: 238, acc: 0.652, loss: 1.804 (data_loss: 1.753, reg_loss: 0.051), lr: 0.019997620 validation, acc: 0.610, loss: 1.513 \n",
            "epoch: 239, acc: 0.614, loss: 1.527 (data_loss: 1.468, reg_loss: 0.058), lr: 0.019997610 validation, acc: 0.575, loss: 1.676 \n",
            "epoch: 240, acc: 0.571, loss: 1.735 (data_loss: 1.670, reg_loss: 0.066), lr: 0.019997600 validation, acc: 0.628, loss: 1.407 \n",
            "epoch: 241, acc: 0.628, loss: 1.383 (data_loss: 1.310, reg_loss: 0.074), lr: 0.019997590 validation, acc: 0.650, loss: 1.426 \n",
            "epoch: 242, acc: 0.654, loss: 1.457 (data_loss: 1.375, reg_loss: 0.082), lr: 0.019997580 validation, acc: 0.578, loss: 1.813 \n",
            "epoch: 243, acc: 0.606, loss: 1.786 (data_loss: 1.696, reg_loss: 0.090), lr: 0.019997570 validation, acc: 0.660, loss: 1.434 \n",
            "epoch: 244, acc: 0.676, loss: 1.473 (data_loss: 1.374, reg_loss: 0.099), lr: 0.019997560 validation, acc: 0.668, loss: 1.241 \n",
            "epoch: 245, acc: 0.682, loss: 1.245 (data_loss: 1.138, reg_loss: 0.107), lr: 0.019997550 validation, acc: 0.686, loss: 1.084 \n",
            "epoch: 246, acc: 0.690, loss: 1.095 (data_loss: 0.981, reg_loss: 0.113), lr: 0.019997540 validation, acc: 0.636, loss: 1.230 \n",
            "epoch: 247, acc: 0.575, loss: 1.587 (data_loss: 1.467, reg_loss: 0.119), lr: 0.019997530 validation, acc: 0.682, loss: 1.192 \n",
            "epoch: 248, acc: 0.699, loss: 1.114 (data_loss: 0.991, reg_loss: 0.123), lr: 0.019997520 validation, acc: 0.719, loss: 1.070 \n",
            "epoch: 249, acc: 0.738, loss: 1.007 (data_loss: 0.880, reg_loss: 0.126), lr: 0.019997510 validation, acc: 0.717, loss: 1.054 \n",
            "epoch: 250, acc: 0.704, loss: 0.984 (data_loss: 0.856, reg_loss: 0.128), lr: 0.019997500 validation, acc: 0.724, loss: 1.085 \n",
            "epoch: 251, acc: 0.727, loss: 1.050 (data_loss: 0.921, reg_loss: 0.130), lr: 0.019997490 validation, acc: 0.729, loss: 0.989 \n",
            "epoch: 252, acc: 0.739, loss: 1.024 (data_loss: 0.893, reg_loss: 0.130), lr: 0.019997480 validation, acc: 0.747, loss: 0.795 \n",
            "epoch: 253, acc: 0.722, loss: 0.924 (data_loss: 0.794, reg_loss: 0.130), lr: 0.019997470 validation, acc: 0.754, loss: 0.792 \n",
            "epoch: 254, acc: 0.756, loss: 0.864 (data_loss: 0.735, reg_loss: 0.128), lr: 0.019997460 validation, acc: 0.710, loss: 0.927 \n",
            "epoch: 255, acc: 0.758, loss: 0.854 (data_loss: 0.727, reg_loss: 0.127), lr: 0.019997450 validation, acc: 0.690, loss: 1.015 \n",
            "epoch: 256, acc: 0.717, loss: 0.992 (data_loss: 0.868, reg_loss: 0.124), lr: 0.019997440 validation, acc: 0.754, loss: 0.793 \n",
            "epoch: 257, acc: 0.765, loss: 0.851 (data_loss: 0.729, reg_loss: 0.122), lr: 0.019997430 validation, acc: 0.774, loss: 0.719 \n",
            "epoch: 258, acc: 0.781, loss: 0.794 (data_loss: 0.675, reg_loss: 0.119), lr: 0.019997420 validation, acc: 0.758, loss: 0.711 \n",
            "epoch: 259, acc: 0.761, loss: 0.794 (data_loss: 0.678, reg_loss: 0.116), lr: 0.019997410 validation, acc: 0.766, loss: 0.689 \n",
            "epoch: 260, acc: 0.776, loss: 0.757 (data_loss: 0.644, reg_loss: 0.113), lr: 0.019997400 validation, acc: 0.781, loss: 0.671 \n",
            "epoch: 261, acc: 0.792, loss: 0.735 (data_loss: 0.626, reg_loss: 0.110), lr: 0.019997390 validation, acc: 0.780, loss: 0.669 \n",
            "epoch: 262, acc: 0.789, loss: 0.724 (data_loss: 0.618, reg_loss: 0.106), lr: 0.019997380 validation, acc: 0.783, loss: 0.655 \n",
            "epoch: 263, acc: 0.798, loss: 0.699 (data_loss: 0.596, reg_loss: 0.103), lr: 0.019997370 validation, acc: 0.783, loss: 0.657 \n",
            "epoch: 264, acc: 0.791, loss: 0.715 (data_loss: 0.616, reg_loss: 0.099), lr: 0.019997360 validation, acc: 0.767, loss: 0.654 \n",
            "epoch: 265, acc: 0.771, loss: 0.730 (data_loss: 0.635, reg_loss: 0.096), lr: 0.019997350 validation, acc: 0.766, loss: 0.682 \n",
            "epoch: 266, acc: 0.777, loss: 0.732 (data_loss: 0.640, reg_loss: 0.092), lr: 0.019997340 validation, acc: 0.748, loss: 0.722 \n",
            "epoch: 267, acc: 0.779, loss: 0.715 (data_loss: 0.626, reg_loss: 0.089), lr: 0.019997330 validation, acc: 0.751, loss: 0.697 \n",
            "epoch: 268, acc: 0.774, loss: 0.709 (data_loss: 0.623, reg_loss: 0.085), lr: 0.019997320 validation, acc: 0.774, loss: 0.676 \n",
            "epoch: 269, acc: 0.777, loss: 0.740 (data_loss: 0.658, reg_loss: 0.082), lr: 0.019997310 validation, acc: 0.776, loss: 0.676 \n",
            "epoch: 270, acc: 0.784, loss: 0.722 (data_loss: 0.643, reg_loss: 0.079), lr: 0.019997300 validation, acc: 0.771, loss: 0.672 \n",
            "epoch: 271, acc: 0.776, loss: 0.721 (data_loss: 0.644, reg_loss: 0.076), lr: 0.019997290 validation, acc: 0.775, loss: 0.665 \n",
            "epoch: 272, acc: 0.761, loss: 0.752 (data_loss: 0.678, reg_loss: 0.074), lr: 0.019997280 validation, acc: 0.782, loss: 0.650 \n",
            "epoch: 273, acc: 0.792, loss: 0.679 (data_loss: 0.608, reg_loss: 0.071), lr: 0.019997270 validation, acc: 0.784, loss: 0.615 \n",
            "epoch: 274, acc: 0.791, loss: 0.655 (data_loss: 0.586, reg_loss: 0.069), lr: 0.019997260 validation, acc: 0.788, loss: 0.624 \n",
            "epoch: 275, acc: 0.782, loss: 0.722 (data_loss: 0.656, reg_loss: 0.066), lr: 0.019997250 validation, acc: 0.782, loss: 0.623 \n",
            "epoch: 276, acc: 0.797, loss: 0.645 (data_loss: 0.581, reg_loss: 0.064), lr: 0.019997240 validation, acc: 0.773, loss: 0.639 \n",
            "epoch: 277, acc: 0.785, loss: 0.667 (data_loss: 0.604, reg_loss: 0.063), lr: 0.019997230 validation, acc: 0.753, loss: 0.768 \n",
            "epoch: 278, acc: 0.803, loss: 0.629 (data_loss: 0.568, reg_loss: 0.061), lr: 0.019997220 validation, acc: 0.754, loss: 0.764 \n",
            "epoch: 279, acc: 0.797, loss: 0.635 (data_loss: 0.576, reg_loss: 0.059), lr: 0.019997210 validation, acc: 0.778, loss: 0.680 \n",
            "epoch: 280, acc: 0.796, loss: 0.634 (data_loss: 0.577, reg_loss: 0.058), lr: 0.019997200 validation, acc: 0.764, loss: 0.741 \n",
            "epoch: 281, acc: 0.786, loss: 0.694 (data_loss: 0.638, reg_loss: 0.056), lr: 0.019997190 validation, acc: 0.781, loss: 0.608 \n",
            "epoch: 282, acc: 0.787, loss: 0.641 (data_loss: 0.586, reg_loss: 0.054), lr: 0.019997180 validation, acc: 0.782, loss: 0.623 \n",
            "epoch: 283, acc: 0.793, loss: 0.645 (data_loss: 0.592, reg_loss: 0.053), lr: 0.019997170 validation, acc: 0.776, loss: 0.660 \n",
            "epoch: 284, acc: 0.787, loss: 0.676 (data_loss: 0.624, reg_loss: 0.052), lr: 0.019997160 validation, acc: 0.781, loss: 0.634 \n",
            "epoch: 285, acc: 0.795, loss: 0.654 (data_loss: 0.604, reg_loss: 0.051), lr: 0.019997150 validation, acc: 0.789, loss: 0.601 \n",
            "epoch: 286, acc: 0.787, loss: 0.653 (data_loss: 0.603, reg_loss: 0.050), lr: 0.019997140 validation, acc: 0.800, loss: 0.589 \n",
            "epoch: 287, acc: 0.812, loss: 0.602 (data_loss: 0.553, reg_loss: 0.049), lr: 0.019997130 validation, acc: 0.803, loss: 0.599 \n",
            "epoch: 288, acc: 0.812, loss: 0.606 (data_loss: 0.558, reg_loss: 0.048), lr: 0.019997120 validation, acc: 0.793, loss: 0.633 \n",
            "epoch: 289, acc: 0.813, loss: 0.607 (data_loss: 0.559, reg_loss: 0.048), lr: 0.019997110 validation, acc: 0.790, loss: 0.673 \n",
            "epoch: 290, acc: 0.812, loss: 0.635 (data_loss: 0.588, reg_loss: 0.047), lr: 0.019997100 validation, acc: 0.800, loss: 0.616 \n",
            "epoch: 291, acc: 0.799, loss: 0.624 (data_loss: 0.578, reg_loss: 0.046), lr: 0.019997090 validation, acc: 0.790, loss: 0.631 \n",
            "epoch: 292, acc: 0.808, loss: 0.616 (data_loss: 0.572, reg_loss: 0.045), lr: 0.019997080 validation, acc: 0.792, loss: 0.610 \n",
            "epoch: 293, acc: 0.811, loss: 0.590 (data_loss: 0.547, reg_loss: 0.044), lr: 0.019997070 validation, acc: 0.810, loss: 0.566 \n",
            "epoch: 294, acc: 0.819, loss: 0.576 (data_loss: 0.533, reg_loss: 0.043), lr: 0.019997060 validation, acc: 0.810, loss: 0.572 \n",
            "epoch: 295, acc: 0.814, loss: 0.589 (data_loss: 0.548, reg_loss: 0.042), lr: 0.019997050 validation, acc: 0.808, loss: 0.566 \n",
            "epoch: 296, acc: 0.824, loss: 0.557 (data_loss: 0.516, reg_loss: 0.041), lr: 0.019997040 validation, acc: 0.799, loss: 0.598 \n",
            "epoch: 297, acc: 0.814, loss: 0.580 (data_loss: 0.540, reg_loss: 0.040), lr: 0.019997030 validation, acc: 0.796, loss: 0.606 \n",
            "epoch: 298, acc: 0.808, loss: 0.604 (data_loss: 0.565, reg_loss: 0.039), lr: 0.019997020 validation, acc: 0.787, loss: 0.604 \n",
            "epoch: 299, acc: 0.791, loss: 0.627 (data_loss: 0.589, reg_loss: 0.038), lr: 0.019997010 validation, acc: 0.778, loss: 0.634 \n",
            "epoch: 300, acc: 0.793, loss: 0.631 (data_loss: 0.594, reg_loss: 0.037), lr: 0.019997000 validation, acc: 0.803, loss: 0.566 \n",
            "epoch: 301, acc: 0.802, loss: 0.618 (data_loss: 0.582, reg_loss: 0.036), lr: 0.019996990 validation, acc: 0.800, loss: 0.573 \n",
            "epoch: 302, acc: 0.805, loss: 0.596 (data_loss: 0.560, reg_loss: 0.036), lr: 0.019996980 validation, acc: 0.784, loss: 0.637 \n",
            "epoch: 303, acc: 0.805, loss: 0.611 (data_loss: 0.575, reg_loss: 0.036), lr: 0.019996970 validation, acc: 0.805, loss: 0.604 \n",
            "epoch: 304, acc: 0.820, loss: 0.599 (data_loss: 0.564, reg_loss: 0.036), lr: 0.019996960 validation, acc: 0.799, loss: 0.570 \n",
            "epoch: 305, acc: 0.801, loss: 0.590 (data_loss: 0.554, reg_loss: 0.036), lr: 0.019996950 validation, acc: 0.816, loss: 0.544 \n",
            "epoch: 306, acc: 0.827, loss: 0.543 (data_loss: 0.507, reg_loss: 0.036), lr: 0.019996940 validation, acc: 0.804, loss: 0.564 \n",
            "epoch: 307, acc: 0.814, loss: 0.566 (data_loss: 0.530, reg_loss: 0.036), lr: 0.019996930 validation, acc: 0.811, loss: 0.541 \n",
            "epoch: 308, acc: 0.821, loss: 0.547 (data_loss: 0.511, reg_loss: 0.037), lr: 0.019996920 validation, acc: 0.816, loss: 0.540 \n",
            "epoch: 309, acc: 0.825, loss: 0.542 (data_loss: 0.506, reg_loss: 0.037), lr: 0.019996910 validation, acc: 0.816, loss: 0.541 \n",
            "epoch: 310, acc: 0.829, loss: 0.539 (data_loss: 0.503, reg_loss: 0.037), lr: 0.019996900 validation, acc: 0.808, loss: 0.558 \n",
            "epoch: 311, acc: 0.748, loss: 0.720 (data_loss: 0.684, reg_loss: 0.036), lr: 0.019996890 validation, acc: 0.641, loss: 1.100 \n",
            "epoch: 312, acc: 0.675, loss: 0.962 (data_loss: 0.926, reg_loss: 0.036), lr: 0.019996880 validation, acc: 0.708, loss: 0.894 \n",
            "epoch: 313, acc: 0.728, loss: 0.838 (data_loss: 0.801, reg_loss: 0.037), lr: 0.019996870 validation, acc: 0.656, loss: 1.073 \n",
            "epoch: 314, acc: 0.647, loss: 1.117 (data_loss: 1.078, reg_loss: 0.039), lr: 0.019996860 validation, acc: 0.709, loss: 1.082 \n",
            "epoch: 315, acc: 0.691, loss: 1.182 (data_loss: 1.139, reg_loss: 0.042), lr: 0.019996850 validation, acc: 0.735, loss: 0.848 \n",
            "epoch: 316, acc: 0.725, loss: 0.854 (data_loss: 0.808, reg_loss: 0.047), lr: 0.019996840 validation, acc: 0.711, loss: 0.905 \n",
            "epoch: 317, acc: 0.718, loss: 0.899 (data_loss: 0.847, reg_loss: 0.052), lr: 0.019996831 validation, acc: 0.740, loss: 0.855 \n",
            "epoch: 318, acc: 0.752, loss: 0.770 (data_loss: 0.713, reg_loss: 0.057), lr: 0.019996821 validation, acc: 0.688, loss: 1.211 \n",
            "epoch: 319, acc: 0.717, loss: 0.966 (data_loss: 0.904, reg_loss: 0.062), lr: 0.019996811 validation, acc: 0.737, loss: 0.921 \n",
            "epoch: 320, acc: 0.752, loss: 0.787 (data_loss: 0.720, reg_loss: 0.067), lr: 0.019996801 validation, acc: 0.700, loss: 1.032 \n",
            "epoch: 321, acc: 0.731, loss: 0.906 (data_loss: 0.834, reg_loss: 0.072), lr: 0.019996791 validation, acc: 0.720, loss: 0.919 \n",
            "epoch: 322, acc: 0.734, loss: 0.921 (data_loss: 0.845, reg_loss: 0.077), lr: 0.019996781 validation, acc: 0.733, loss: 0.853 \n",
            "epoch: 323, acc: 0.741, loss: 0.895 (data_loss: 0.814, reg_loss: 0.081), lr: 0.019996771 validation, acc: 0.746, loss: 0.772 \n",
            "epoch: 324, acc: 0.759, loss: 0.804 (data_loss: 0.719, reg_loss: 0.085), lr: 0.019996761 validation, acc: 0.750, loss: 0.731 \n",
            "epoch: 325, acc: 0.759, loss: 0.799 (data_loss: 0.712, reg_loss: 0.088), lr: 0.019996751 validation, acc: 0.767, loss: 0.725 \n",
            "epoch: 326, acc: 0.729, loss: 0.935 (data_loss: 0.845, reg_loss: 0.090), lr: 0.019996741 validation, acc: 0.694, loss: 1.190 \n",
            "epoch: 327, acc: 0.748, loss: 0.929 (data_loss: 0.837, reg_loss: 0.092), lr: 0.019996731 validation, acc: 0.691, loss: 1.337 \n",
            "epoch: 328, acc: 0.735, loss: 1.073 (data_loss: 0.979, reg_loss: 0.094), lr: 0.019996721 validation, acc: 0.715, loss: 0.963 \n",
            "epoch: 329, acc: 0.723, loss: 0.906 (data_loss: 0.810, reg_loss: 0.096), lr: 0.019996711 validation, acc: 0.740, loss: 0.873 \n",
            "epoch: 330, acc: 0.751, loss: 0.882 (data_loss: 0.784, reg_loss: 0.097), lr: 0.019996701 validation, acc: 0.755, loss: 0.780 \n",
            "epoch: 331, acc: 0.769, loss: 0.799 (data_loss: 0.701, reg_loss: 0.098), lr: 0.019996691 validation, acc: 0.765, loss: 0.728 \n",
            "epoch: 332, acc: 0.770, loss: 0.793 (data_loss: 0.694, reg_loss: 0.099), lr: 0.019996681 validation, acc: 0.756, loss: 0.732 \n",
            "epoch: 333, acc: 0.779, loss: 0.762 (data_loss: 0.662, reg_loss: 0.100), lr: 0.019996671 validation, acc: 0.729, loss: 0.795 \n",
            "epoch: 334, acc: 0.795, loss: 0.718 (data_loss: 0.619, reg_loss: 0.100), lr: 0.019996661 validation, acc: 0.683, loss: 0.898 \n",
            "epoch: 335, acc: 0.693, loss: 0.967 (data_loss: 0.868, reg_loss: 0.099), lr: 0.019996651 validation, acc: 0.659, loss: 1.201 \n",
            "epoch: 336, acc: 0.646, loss: 1.349 (data_loss: 1.250, reg_loss: 0.099), lr: 0.019996641 validation, acc: 0.733, loss: 0.830 \n",
            "epoch: 337, acc: 0.743, loss: 0.872 (data_loss: 0.773, reg_loss: 0.099), lr: 0.019996631 validation, acc: 0.703, loss: 0.871 \n",
            "epoch: 338, acc: 0.719, loss: 0.926 (data_loss: 0.826, reg_loss: 0.100), lr: 0.019996621 validation, acc: 0.750, loss: 0.746 \n",
            "epoch: 339, acc: 0.750, loss: 0.850 (data_loss: 0.749, reg_loss: 0.101), lr: 0.019996611 validation, acc: 0.770, loss: 0.706 \n",
            "epoch: 340, acc: 0.779, loss: 0.783 (data_loss: 0.680, reg_loss: 0.103), lr: 0.019996601 validation, acc: 0.780, loss: 0.681 \n",
            "epoch: 341, acc: 0.791, loss: 0.749 (data_loss: 0.644, reg_loss: 0.105), lr: 0.019996591 validation, acc: 0.778, loss: 0.700 \n",
            "epoch: 342, acc: 0.763, loss: 0.868 (data_loss: 0.762, reg_loss: 0.106), lr: 0.019996581 validation, acc: 0.745, loss: 0.769 \n",
            "epoch: 343, acc: 0.791, loss: 0.735 (data_loss: 0.627, reg_loss: 0.107), lr: 0.019996571 validation, acc: 0.701, loss: 0.923 \n",
            "epoch: 344, acc: 0.764, loss: 0.779 (data_loss: 0.671, reg_loss: 0.108), lr: 0.019996561 validation, acc: 0.742, loss: 0.770 \n",
            "epoch: 345, acc: 0.792, loss: 0.717 (data_loss: 0.609, reg_loss: 0.108), lr: 0.019996551 validation, acc: 0.783, loss: 0.664 \n",
            "epoch: 346, acc: 0.800, loss: 0.718 (data_loss: 0.611, reg_loss: 0.107), lr: 0.019996541 validation, acc: 0.778, loss: 0.669 \n",
            "epoch: 347, acc: 0.745, loss: 0.851 (data_loss: 0.747, reg_loss: 0.105), lr: 0.019996531 validation, acc: 0.783, loss: 0.651 \n",
            "epoch: 348, acc: 0.794, loss: 0.711 (data_loss: 0.608, reg_loss: 0.103), lr: 0.019996521 validation, acc: 0.753, loss: 0.728 \n",
            "epoch: 349, acc: 0.784, loss: 0.710 (data_loss: 0.610, reg_loss: 0.100), lr: 0.019996511 validation, acc: 0.743, loss: 0.777 \n",
            "epoch: 350, acc: 0.748, loss: 0.847 (data_loss: 0.750, reg_loss: 0.097), lr: 0.019996501 validation, acc: 0.757, loss: 0.750 \n",
            "epoch: 351, acc: 0.760, loss: 0.815 (data_loss: 0.722, reg_loss: 0.093), lr: 0.019996491 validation, acc: 0.784, loss: 0.641 \n",
            "epoch: 352, acc: 0.785, loss: 0.713 (data_loss: 0.624, reg_loss: 0.090), lr: 0.019996481 validation, acc: 0.781, loss: 0.659 \n",
            "epoch: 353, acc: 0.791, loss: 0.701 (data_loss: 0.615, reg_loss: 0.087), lr: 0.019996471 validation, acc: 0.785, loss: 0.647 \n",
            "epoch: 354, acc: 0.799, loss: 0.674 (data_loss: 0.590, reg_loss: 0.084), lr: 0.019996461 validation, acc: 0.782, loss: 0.650 \n",
            "epoch: 355, acc: 0.795, loss: 0.678 (data_loss: 0.597, reg_loss: 0.081), lr: 0.019996451 validation, acc: 0.796, loss: 0.624 \n",
            "epoch: 356, acc: 0.806, loss: 0.656 (data_loss: 0.577, reg_loss: 0.079), lr: 0.019996441 validation, acc: 0.800, loss: 0.626 \n",
            "epoch: 357, acc: 0.810, loss: 0.658 (data_loss: 0.582, reg_loss: 0.076), lr: 0.019996431 validation, acc: 0.799, loss: 0.619 \n",
            "epoch: 358, acc: 0.808, loss: 0.657 (data_loss: 0.584, reg_loss: 0.073), lr: 0.019996421 validation, acc: 0.802, loss: 0.593 \n",
            "epoch: 359, acc: 0.806, loss: 0.640 (data_loss: 0.570, reg_loss: 0.070), lr: 0.019996411 validation, acc: 0.797, loss: 0.597 \n",
            "epoch: 360, acc: 0.812, loss: 0.626 (data_loss: 0.559, reg_loss: 0.067), lr: 0.019996401 validation, acc: 0.805, loss: 0.584 \n",
            "epoch: 361, acc: 0.817, loss: 0.613 (data_loss: 0.548, reg_loss: 0.064), lr: 0.019996391 validation, acc: 0.805, loss: 0.585 \n",
            "epoch: 362, acc: 0.816, loss: 0.621 (data_loss: 0.560, reg_loss: 0.062), lr: 0.019996381 validation, acc: 0.801, loss: 0.585 \n",
            "epoch: 363, acc: 0.817, loss: 0.607 (data_loss: 0.548, reg_loss: 0.059), lr: 0.019996371 validation, acc: 0.820, loss: 0.547 \n",
            "epoch: 364, acc: 0.829, loss: 0.578 (data_loss: 0.521, reg_loss: 0.056), lr: 0.019996361 validation, acc: 0.815, loss: 0.558 \n",
            "epoch: 365, acc: 0.815, loss: 0.593 (data_loss: 0.539, reg_loss: 0.054), lr: 0.019996351 validation, acc: 0.776, loss: 0.634 \n",
            "epoch: 366, acc: 0.809, loss: 0.587 (data_loss: 0.536, reg_loss: 0.052), lr: 0.019996341 validation, acc: 0.764, loss: 0.657 \n",
            "epoch: 367, acc: 0.789, loss: 0.640 (data_loss: 0.591, reg_loss: 0.050), lr: 0.019996331 validation, acc: 0.801, loss: 0.600 \n",
            "epoch: 368, acc: 0.801, loss: 0.632 (data_loss: 0.585, reg_loss: 0.047), lr: 0.019996321 validation, acc: 0.808, loss: 0.587 \n",
            "epoch: 369, acc: 0.820, loss: 0.596 (data_loss: 0.551, reg_loss: 0.045), lr: 0.019996311 validation, acc: 0.817, loss: 0.550 \n",
            "epoch: 370, acc: 0.821, loss: 0.566 (data_loss: 0.522, reg_loss: 0.044), lr: 0.019996301 validation, acc: 0.781, loss: 0.620 \n",
            "epoch: 371, acc: 0.794, loss: 0.624 (data_loss: 0.581, reg_loss: 0.043), lr: 0.019996291 validation, acc: 0.767, loss: 0.671 \n",
            "epoch: 372, acc: 0.779, loss: 0.685 (data_loss: 0.643, reg_loss: 0.042), lr: 0.019996281 validation, acc: 0.792, loss: 0.601 \n",
            "epoch: 373, acc: 0.802, loss: 0.604 (data_loss: 0.562, reg_loss: 0.042), lr: 0.019996271 validation, acc: 0.799, loss: 0.600 \n",
            "epoch: 374, acc: 0.821, loss: 0.587 (data_loss: 0.545, reg_loss: 0.042), lr: 0.019996261 validation, acc: 0.781, loss: 0.639 \n",
            "epoch: 375, acc: 0.800, loss: 0.626 (data_loss: 0.583, reg_loss: 0.043), lr: 0.019996251 validation, acc: 0.798, loss: 0.626 \n",
            "epoch: 376, acc: 0.790, loss: 0.652 (data_loss: 0.608, reg_loss: 0.044), lr: 0.019996241 validation, acc: 0.798, loss: 0.618 \n",
            "epoch: 377, acc: 0.796, loss: 0.637 (data_loss: 0.592, reg_loss: 0.045), lr: 0.019996231 validation, acc: 0.786, loss: 0.657 \n",
            "epoch: 378, acc: 0.792, loss: 0.657 (data_loss: 0.612, reg_loss: 0.046), lr: 0.019996221 validation, acc: 0.777, loss: 0.671 \n",
            "epoch: 379, acc: 0.754, loss: 0.805 (data_loss: 0.759, reg_loss: 0.046), lr: 0.019996211 validation, acc: 0.722, loss: 0.907 \n",
            "epoch: 380, acc: 0.736, loss: 0.891 (data_loss: 0.844, reg_loss: 0.047), lr: 0.019996201 validation, acc: 0.743, loss: 0.713 \n",
            "epoch: 381, acc: 0.755, loss: 0.716 (data_loss: 0.668, reg_loss: 0.048), lr: 0.019996191 validation, acc: 0.774, loss: 0.666 \n",
            "epoch: 382, acc: 0.773, loss: 0.704 (data_loss: 0.654, reg_loss: 0.049), lr: 0.019996181 validation, acc: 0.763, loss: 0.793 \n",
            "epoch: 383, acc: 0.778, loss: 0.731 (data_loss: 0.680, reg_loss: 0.051), lr: 0.019996171 validation, acc: 0.756, loss: 0.884 \n",
            "epoch: 384, acc: 0.802, loss: 0.644 (data_loss: 0.592, reg_loss: 0.053), lr: 0.019996161 validation, acc: 0.714, loss: 1.024 \n",
            "epoch: 385, acc: 0.746, loss: 0.878 (data_loss: 0.824, reg_loss: 0.054), lr: 0.019996151 validation, acc: 0.769, loss: 0.800 \n",
            "epoch: 386, acc: 0.771, loss: 0.787 (data_loss: 0.731, reg_loss: 0.056), lr: 0.019996141 validation, acc: 0.751, loss: 0.881 \n",
            "epoch: 387, acc: 0.767, loss: 0.827 (data_loss: 0.767, reg_loss: 0.060), lr: 0.019996131 validation, acc: 0.728, loss: 0.865 \n",
            "epoch: 388, acc: 0.761, loss: 0.773 (data_loss: 0.708, reg_loss: 0.065), lr: 0.019996121 validation, acc: 0.746, loss: 0.844 \n",
            "epoch: 389, acc: 0.769, loss: 0.788 (data_loss: 0.717, reg_loss: 0.071), lr: 0.019996111 validation, acc: 0.729, loss: 0.908 \n",
            "epoch: 390, acc: 0.736, loss: 0.952 (data_loss: 0.876, reg_loss: 0.076), lr: 0.019996101 validation, acc: 0.713, loss: 0.971 \n",
            "epoch: 391, acc: 0.735, loss: 0.897 (data_loss: 0.816, reg_loss: 0.081), lr: 0.019996091 validation, acc: 0.718, loss: 1.054 \n",
            "epoch: 392, acc: 0.668, loss: 1.524 (data_loss: 1.441, reg_loss: 0.083), lr: 0.019996081 validation, acc: 0.667, loss: 1.314 \n",
            "epoch: 393, acc: 0.691, loss: 1.282 (data_loss: 1.197, reg_loss: 0.085), lr: 0.019996071 validation, acc: 0.659, loss: 1.345 \n",
            "epoch: 394, acc: 0.689, loss: 1.288 (data_loss: 1.201, reg_loss: 0.087), lr: 0.019996061 validation, acc: 0.705, loss: 1.019 \n",
            "epoch: 395, acc: 0.707, loss: 1.064 (data_loss: 0.976, reg_loss: 0.088), lr: 0.019996051 validation, acc: 0.731, loss: 0.948 \n",
            "epoch: 396, acc: 0.734, loss: 0.992 (data_loss: 0.902, reg_loss: 0.090), lr: 0.019996041 validation, acc: 0.720, loss: 0.958 \n",
            "epoch: 397, acc: 0.730, loss: 1.008 (data_loss: 0.917, reg_loss: 0.092), lr: 0.019996031 validation, acc: 0.739, loss: 0.818 \n",
            "epoch: 398, acc: 0.719, loss: 0.946 (data_loss: 0.853, reg_loss: 0.093), lr: 0.019996021 validation, acc: 0.730, loss: 0.883 \n",
            "epoch: 399, acc: 0.745, loss: 0.903 (data_loss: 0.808, reg_loss: 0.094), lr: 0.019996011 validation, acc: 0.769, loss: 0.720 \n",
            "epoch: 400, acc: 0.781, loss: 0.754 (data_loss: 0.659, reg_loss: 0.095), lr: 0.019996001 validation, acc: 0.748, loss: 0.802 \n",
            "epoch: 401, acc: 0.765, loss: 0.805 (data_loss: 0.710, reg_loss: 0.095), lr: 0.019995991 validation, acc: 0.768, loss: 0.758 \n",
            "epoch: 402, acc: 0.781, loss: 0.789 (data_loss: 0.693, reg_loss: 0.095), lr: 0.019995981 validation, acc: 0.775, loss: 0.689 \n",
            "epoch: 403, acc: 0.782, loss: 0.763 (data_loss: 0.669, reg_loss: 0.094), lr: 0.019995971 validation, acc: 0.780, loss: 0.668 \n",
            "epoch: 404, acc: 0.783, loss: 0.757 (data_loss: 0.663, reg_loss: 0.093), lr: 0.019995961 validation, acc: 0.779, loss: 0.654 \n",
            "epoch: 405, acc: 0.789, loss: 0.708 (data_loss: 0.616, reg_loss: 0.092), lr: 0.019995951 validation, acc: 0.789, loss: 0.627 \n",
            "epoch: 406, acc: 0.784, loss: 0.716 (data_loss: 0.626, reg_loss: 0.090), lr: 0.019995941 validation, acc: 0.790, loss: 0.636 \n",
            "epoch: 407, acc: 0.803, loss: 0.678 (data_loss: 0.590, reg_loss: 0.088), lr: 0.019995931 validation, acc: 0.798, loss: 0.636 \n",
            "epoch: 408, acc: 0.813, loss: 0.663 (data_loss: 0.578, reg_loss: 0.085), lr: 0.019995921 validation, acc: 0.802, loss: 0.624 \n",
            "epoch: 409, acc: 0.815, loss: 0.659 (data_loss: 0.577, reg_loss: 0.082), lr: 0.019995911 validation, acc: 0.805, loss: 0.599 \n",
            "epoch: 410, acc: 0.812, loss: 0.644 (data_loss: 0.565, reg_loss: 0.079), lr: 0.019995901 validation, acc: 0.807, loss: 0.607 \n",
            "epoch: 411, acc: 0.816, loss: 0.634 (data_loss: 0.558, reg_loss: 0.076), lr: 0.019995891 validation, acc: 0.812, loss: 0.585 \n",
            "epoch: 412, acc: 0.824, loss: 0.618 (data_loss: 0.546, reg_loss: 0.072), lr: 0.019995881 validation, acc: 0.808, loss: 0.592 \n",
            "epoch: 413, acc: 0.819, loss: 0.619 (data_loss: 0.550, reg_loss: 0.069), lr: 0.019995871 validation, acc: 0.816, loss: 0.567 \n",
            "epoch: 414, acc: 0.827, loss: 0.589 (data_loss: 0.523, reg_loss: 0.065), lr: 0.019995861 validation, acc: 0.783, loss: 0.655 \n",
            "epoch: 415, acc: 0.818, loss: 0.624 (data_loss: 0.561, reg_loss: 0.062), lr: 0.019995851 validation, acc: 0.813, loss: 0.560 \n",
            "epoch: 416, acc: 0.827, loss: 0.581 (data_loss: 0.522, reg_loss: 0.058), lr: 0.019995841 validation, acc: 0.807, loss: 0.582 \n",
            "epoch: 417, acc: 0.805, loss: 0.630 (data_loss: 0.575, reg_loss: 0.055), lr: 0.019995831 validation, acc: 0.747, loss: 0.734 \n",
            "epoch: 418, acc: 0.801, loss: 0.621 (data_loss: 0.569, reg_loss: 0.052), lr: 0.019995821 validation, acc: 0.777, loss: 0.660 \n",
            "epoch: 419, acc: 0.819, loss: 0.589 (data_loss: 0.539, reg_loss: 0.050), lr: 0.019995811 validation, acc: 0.800, loss: 0.604 \n",
            "epoch: 420, acc: 0.821, loss: 0.580 (data_loss: 0.532, reg_loss: 0.048), lr: 0.019995801 validation, acc: 0.797, loss: 0.654 \n",
            "epoch: 421, acc: 0.815, loss: 0.600 (data_loss: 0.554, reg_loss: 0.046), lr: 0.019995791 validation, acc: 0.764, loss: 0.757 \n",
            "epoch: 422, acc: 0.775, loss: 0.761 (data_loss: 0.717, reg_loss: 0.044), lr: 0.019995781 validation, acc: 0.768, loss: 0.701 \n",
            "epoch: 423, acc: 0.778, loss: 0.704 (data_loss: 0.660, reg_loss: 0.043), lr: 0.019995771 validation, acc: 0.767, loss: 0.706 \n",
            "epoch: 424, acc: 0.777, loss: 0.717 (data_loss: 0.673, reg_loss: 0.044), lr: 0.019995761 validation, acc: 0.767, loss: 0.722 \n",
            "epoch: 425, acc: 0.783, loss: 0.712 (data_loss: 0.666, reg_loss: 0.046), lr: 0.019995751 validation, acc: 0.777, loss: 0.711 \n",
            "epoch: 426, acc: 0.789, loss: 0.701 (data_loss: 0.653, reg_loss: 0.048), lr: 0.019995741 validation, acc: 0.785, loss: 0.670 \n",
            "epoch: 427, acc: 0.797, loss: 0.676 (data_loss: 0.625, reg_loss: 0.051), lr: 0.019995731 validation, acc: 0.798, loss: 0.639 \n",
            "epoch: 428, acc: 0.800, loss: 0.653 (data_loss: 0.600, reg_loss: 0.053), lr: 0.019995721 validation, acc: 0.805, loss: 0.618 \n",
            "epoch: 429, acc: 0.819, loss: 0.609 (data_loss: 0.554, reg_loss: 0.055), lr: 0.019995711 validation, acc: 0.794, loss: 0.650 \n",
            "epoch: 430, acc: 0.815, loss: 0.622 (data_loss: 0.565, reg_loss: 0.057), lr: 0.019995701 validation, acc: 0.795, loss: 0.677 \n",
            "epoch: 431, acc: 0.808, loss: 0.665 (data_loss: 0.607, reg_loss: 0.058), lr: 0.019995691 validation, acc: 0.751, loss: 0.734 \n",
            "epoch: 432, acc: 0.780, loss: 0.688 (data_loss: 0.630, reg_loss: 0.058), lr: 0.019995681 validation, acc: 0.801, loss: 0.602 \n",
            "epoch: 433, acc: 0.810, loss: 0.623 (data_loss: 0.565, reg_loss: 0.058), lr: 0.019995671 validation, acc: 0.808, loss: 0.590 \n",
            "epoch: 434, acc: 0.819, loss: 0.611 (data_loss: 0.553, reg_loss: 0.058), lr: 0.019995661 validation, acc: 0.814, loss: 0.583 \n",
            "epoch: 435, acc: 0.806, loss: 0.630 (data_loss: 0.572, reg_loss: 0.058), lr: 0.019995651 validation, acc: 0.797, loss: 0.607 \n",
            "epoch: 436, acc: 0.818, loss: 0.608 (data_loss: 0.551, reg_loss: 0.057), lr: 0.019995641 validation, acc: 0.790, loss: 0.625 \n",
            "epoch: 437, acc: 0.808, loss: 0.620 (data_loss: 0.564, reg_loss: 0.056), lr: 0.019995631 validation, acc: 0.820, loss: 0.589 \n",
            "epoch: 438, acc: 0.830, loss: 0.600 (data_loss: 0.545, reg_loss: 0.055), lr: 0.019995621 validation, acc: 0.813, loss: 0.566 \n",
            "epoch: 439, acc: 0.814, loss: 0.606 (data_loss: 0.553, reg_loss: 0.053), lr: 0.019995611 validation, acc: 0.810, loss: 0.574 \n",
            "epoch: 440, acc: 0.821, loss: 0.588 (data_loss: 0.536, reg_loss: 0.051), lr: 0.019995601 validation, acc: 0.815, loss: 0.573 \n",
            "epoch: 441, acc: 0.826, loss: 0.581 (data_loss: 0.531, reg_loss: 0.049), lr: 0.019995591 validation, acc: 0.824, loss: 0.546 \n",
            "epoch: 442, acc: 0.831, loss: 0.562 (data_loss: 0.514, reg_loss: 0.047), lr: 0.019995581 validation, acc: 0.796, loss: 0.654 \n",
            "epoch: 443, acc: 0.830, loss: 0.566 (data_loss: 0.520, reg_loss: 0.046), lr: 0.019995571 validation, acc: 0.790, loss: 0.709 \n",
            "epoch: 444, acc: 0.834, loss: 0.565 (data_loss: 0.521, reg_loss: 0.044), lr: 0.019995561 validation, acc: 0.816, loss: 0.576 \n",
            "epoch: 445, acc: 0.828, loss: 0.563 (data_loss: 0.522, reg_loss: 0.041), lr: 0.019995551 validation, acc: 0.819, loss: 0.569 \n",
            "epoch: 446, acc: 0.832, loss: 0.559 (data_loss: 0.520, reg_loss: 0.040), lr: 0.019995541 validation, acc: 0.827, loss: 0.539 \n",
            "epoch: 447, acc: 0.819, loss: 0.555 (data_loss: 0.518, reg_loss: 0.038), lr: 0.019995531 validation, acc: 0.814, loss: 0.568 \n",
            "epoch: 448, acc: 0.829, loss: 0.556 (data_loss: 0.520, reg_loss: 0.036), lr: 0.019995521 validation, acc: 0.803, loss: 0.583 \n",
            "epoch: 449, acc: 0.808, loss: 0.588 (data_loss: 0.554, reg_loss: 0.035), lr: 0.019995511 validation, acc: 0.826, loss: 0.527 \n",
            "epoch: 450, acc: 0.838, loss: 0.520 (data_loss: 0.487, reg_loss: 0.033), lr: 0.019995501 validation, acc: 0.821, loss: 0.548 \n",
            "epoch: 451, acc: 0.833, loss: 0.547 (data_loss: 0.515, reg_loss: 0.032), lr: 0.019995491 validation, acc: 0.810, loss: 0.593 \n",
            "epoch: 452, acc: 0.831, loss: 0.538 (data_loss: 0.507, reg_loss: 0.031), lr: 0.019995481 validation, acc: 0.809, loss: 0.558 \n",
            "epoch: 453, acc: 0.825, loss: 0.541 (data_loss: 0.511, reg_loss: 0.030), lr: 0.019995471 validation, acc: 0.811, loss: 0.588 \n",
            "epoch: 454, acc: 0.822, loss: 0.570 (data_loss: 0.540, reg_loss: 0.030), lr: 0.019995461 validation, acc: 0.768, loss: 0.717 \n",
            "epoch: 455, acc: 0.780, loss: 0.685 (data_loss: 0.655, reg_loss: 0.030), lr: 0.019995451 validation, acc: 0.733, loss: 0.821 \n",
            "epoch: 456, acc: 0.704, loss: 0.860 (data_loss: 0.830, reg_loss: 0.030), lr: 0.019995441 validation, acc: 0.747, loss: 0.722 \n",
            "epoch: 457, acc: 0.774, loss: 0.660 (data_loss: 0.629, reg_loss: 0.031), lr: 0.019995431 validation, acc: 0.737, loss: 0.740 \n",
            "epoch: 458, acc: 0.762, loss: 0.723 (data_loss: 0.690, reg_loss: 0.033), lr: 0.019995421 validation, acc: 0.755, loss: 0.808 \n",
            "epoch: 459, acc: 0.769, loss: 0.783 (data_loss: 0.748, reg_loss: 0.035), lr: 0.019995411 validation, acc: 0.775, loss: 0.740 \n",
            "epoch: 460, acc: 0.775, loss: 0.748 (data_loss: 0.710, reg_loss: 0.038), lr: 0.019995401 validation, acc: 0.737, loss: 1.036 \n",
            "epoch: 461, acc: 0.755, loss: 0.863 (data_loss: 0.821, reg_loss: 0.042), lr: 0.019995391 validation, acc: 0.684, loss: 1.770 \n",
            "epoch: 462, acc: 0.703, loss: 1.558 (data_loss: 1.511, reg_loss: 0.047), lr: 0.019995381 validation, acc: 0.776, loss: 0.709 \n",
            "epoch: 463, acc: 0.773, loss: 0.729 (data_loss: 0.678, reg_loss: 0.051), lr: 0.019995371 validation, acc: 0.759, loss: 0.761 \n",
            "epoch: 464, acc: 0.763, loss: 0.789 (data_loss: 0.732, reg_loss: 0.056), lr: 0.019995361 validation, acc: 0.744, loss: 0.797 \n",
            "epoch: 465, acc: 0.749, loss: 0.805 (data_loss: 0.742, reg_loss: 0.063), lr: 0.019995351 validation, acc: 0.743, loss: 0.830 \n",
            "epoch: 466, acc: 0.772, loss: 0.738 (data_loss: 0.668, reg_loss: 0.070), lr: 0.019995341 validation, acc: 0.727, loss: 0.824 \n",
            "epoch: 467, acc: 0.743, loss: 0.843 (data_loss: 0.767, reg_loss: 0.075), lr: 0.019995331 validation, acc: 0.764, loss: 0.790 \n",
            "epoch: 468, acc: 0.766, loss: 0.786 (data_loss: 0.706, reg_loss: 0.080), lr: 0.019995321 validation, acc: 0.732, loss: 1.018 \n",
            "epoch: 469, acc: 0.742, loss: 0.991 (data_loss: 0.908, reg_loss: 0.084), lr: 0.019995311 validation, acc: 0.737, loss: 0.933 \n",
            "epoch: 470, acc: 0.741, loss: 0.926 (data_loss: 0.840, reg_loss: 0.087), lr: 0.019995301 validation, acc: 0.759, loss: 0.766 \n",
            "epoch: 471, acc: 0.766, loss: 0.787 (data_loss: 0.699, reg_loss: 0.089), lr: 0.019995291 validation, acc: 0.769, loss: 0.725 \n",
            "epoch: 472, acc: 0.751, loss: 0.822 (data_loss: 0.732, reg_loss: 0.090), lr: 0.019995281 validation, acc: 0.771, loss: 0.721 \n",
            "epoch: 473, acc: 0.750, loss: 0.807 (data_loss: 0.716, reg_loss: 0.091), lr: 0.019995271 validation, acc: 0.743, loss: 0.809 \n",
            "epoch: 474, acc: 0.765, loss: 0.798 (data_loss: 0.708, reg_loss: 0.091), lr: 0.019995261 validation, acc: 0.743, loss: 0.775 \n",
            "epoch: 475, acc: 0.774, loss: 0.732 (data_loss: 0.642, reg_loss: 0.090), lr: 0.019995251 validation, acc: 0.749, loss: 0.767 \n",
            "epoch: 476, acc: 0.771, loss: 0.744 (data_loss: 0.656, reg_loss: 0.088), lr: 0.019995241 validation, acc: 0.791, loss: 0.677 \n",
            "epoch: 477, acc: 0.803, loss: 0.673 (data_loss: 0.587, reg_loss: 0.086), lr: 0.019995231 validation, acc: 0.783, loss: 0.713 \n",
            "epoch: 478, acc: 0.801, loss: 0.679 (data_loss: 0.595, reg_loss: 0.084), lr: 0.019995221 validation, acc: 0.780, loss: 0.751 \n",
            "epoch: 479, acc: 0.798, loss: 0.679 (data_loss: 0.598, reg_loss: 0.081), lr: 0.019995211 validation, acc: 0.775, loss: 0.788 \n",
            "epoch: 480, acc: 0.803, loss: 0.700 (data_loss: 0.622, reg_loss: 0.078), lr: 0.019995201 validation, acc: 0.789, loss: 0.666 \n",
            "epoch: 481, acc: 0.801, loss: 0.669 (data_loss: 0.595, reg_loss: 0.074), lr: 0.019995191 validation, acc: 0.798, loss: 0.621 \n",
            "epoch: 482, acc: 0.809, loss: 0.639 (data_loss: 0.568, reg_loss: 0.071), lr: 0.019995181 validation, acc: 0.804, loss: 0.609 \n",
            "epoch: 483, acc: 0.821, loss: 0.621 (data_loss: 0.554, reg_loss: 0.067), lr: 0.019995171 validation, acc: 0.806, loss: 0.598 \n",
            "epoch: 484, acc: 0.822, loss: 0.612 (data_loss: 0.548, reg_loss: 0.064), lr: 0.019995161 validation, acc: 0.806, loss: 0.587 \n",
            "epoch: 485, acc: 0.824, loss: 0.598 (data_loss: 0.538, reg_loss: 0.061), lr: 0.019995151 validation, acc: 0.808, loss: 0.571 \n",
            "epoch: 486, acc: 0.820, loss: 0.592 (data_loss: 0.535, reg_loss: 0.057), lr: 0.019995141 validation, acc: 0.811, loss: 0.570 \n",
            "epoch: 487, acc: 0.828, loss: 0.577 (data_loss: 0.522, reg_loss: 0.054), lr: 0.019995131 validation, acc: 0.816, loss: 0.558 \n",
            "epoch: 488, acc: 0.826, loss: 0.581 (data_loss: 0.529, reg_loss: 0.052), lr: 0.019995121 validation, acc: 0.814, loss: 0.550 \n",
            "epoch: 489, acc: 0.829, loss: 0.560 (data_loss: 0.511, reg_loss: 0.049), lr: 0.019995111 validation, acc: 0.816, loss: 0.542 \n",
            "epoch: 490, acc: 0.831, loss: 0.552 (data_loss: 0.505, reg_loss: 0.047), lr: 0.019995101 validation, acc: 0.822, loss: 0.549 \n",
            "epoch: 491, acc: 0.832, loss: 0.555 (data_loss: 0.511, reg_loss: 0.044), lr: 0.019995091 validation, acc: 0.809, loss: 0.576 \n",
            "epoch: 492, acc: 0.821, loss: 0.578 (data_loss: 0.536, reg_loss: 0.042), lr: 0.019995081 validation, acc: 0.802, loss: 0.586 \n",
            "epoch: 493, acc: 0.810, loss: 0.594 (data_loss: 0.554, reg_loss: 0.040), lr: 0.019995071 validation, acc: 0.815, loss: 0.563 \n",
            "epoch: 494, acc: 0.831, loss: 0.540 (data_loss: 0.502, reg_loss: 0.039), lr: 0.019995061 validation, acc: 0.810, loss: 0.623 \n",
            "epoch: 495, acc: 0.822, loss: 0.571 (data_loss: 0.534, reg_loss: 0.037), lr: 0.019995051 validation, acc: 0.810, loss: 0.670 \n",
            "epoch: 496, acc: 0.831, loss: 0.578 (data_loss: 0.542, reg_loss: 0.036), lr: 0.019995041 validation, acc: 0.818, loss: 0.576 \n",
            "epoch: 497, acc: 0.830, loss: 0.554 (data_loss: 0.520, reg_loss: 0.035), lr: 0.019995031 validation, acc: 0.815, loss: 0.553 \n",
            "epoch: 498, acc: 0.825, loss: 0.551 (data_loss: 0.517, reg_loss: 0.034), lr: 0.019995021 validation, acc: 0.818, loss: 0.528 \n",
            "epoch: 499, acc: 0.834, loss: 0.527 (data_loss: 0.494, reg_loss: 0.033), lr: 0.019995011 validation, acc: 0.820, loss: 0.522 \n",
            "epoch: 500, acc: 0.834, loss: 0.522 (data_loss: 0.490, reg_loss: 0.032), lr: 0.019995001 validation, acc: 0.823, loss: 0.532 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0u0Jm7QCrw"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNnDUP_U8Xn",
        "outputId": "bc0c38c4-dc96-4884-d61c-2ebd345119bc"
      },
      "source": [
        "print(np.mean(acc_cache))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7230780106453759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbXMisqQKqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b1881b-bc96-4458-8e88-92de81c24405"
      },
      "source": [
        "for milestone in summary:\n",
        "  print(milestone)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model hit 80% validation accuracy in 207 epochs\n",
            "Max accuracy was 82.71% at epoch 446.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVqT3yaXS5k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwSXsZVU8Xo",
        "outputId": "131c8137-77e8-4660-e959-fe0c0b2a7f90"
      },
      "source": [
        "accuracy = Accuracy_Categorical()\n",
        "\n",
        "accuracy.init(y)\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "# Calculate the data loss\n",
        "loss = loss_function.calculate(activation4.output, y_test)\n",
        "\n",
        "predictions = activation4.predictions(activation4.output)\n",
        "testaccuracy = accuracy.calculate(predictions, y_test)\n",
        "\n",
        "print(f'training, acc: {testaccuracy:.3f}, loss: {loss:.3f}')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training, acc: 0.823, loss: 0.532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXGM4hyXmr7"
      },
      "source": [
        "Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5c5xUTNXk2v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "74d94d43-2f83-4f6a-f9c5-a0fb35b9362a"
      },
      "source": [
        "plt.plot(epoch_cache, val_loss_cache, label='Validation Loss')\n",
        "plt.plot(epoch_cache, loss_cache, label='Training Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_cache, val_acc_cache, label='Validation Accuracy')\n",
        "plt.plot(epoch_cache, acc_cache, label='Training Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(epoch_cache, lr_cache, label='LR')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5xU1fn/32f69gJLR4oU6dWKBWxfjS12jSYS/cUSI9HEFpPYEhM1Ro1J1KiJGjtqNBDFDoJiQwSlqTRhqVvYvlPv+f1x77Sd2b7DzrDP+/Wa186999w7Z3Zmzuc+5TxHaa0RBEEQei627u6AIAiC0L2IEAiCIPRwRAgEQRB6OCIEgiAIPRwRAkEQhB6OCIEgCEIPR4RAEAShhyNCIAgtoJTarJQ6trv7IQipRIRAEAShhyNCIAjtRCnlVkrdr5Tabj3uV0q5rWO9lVL/U0pVKaUqlVJLlFI269gNSqltSqlapdTXSqljuvedCIKJo7s7IAgZyK+BQ4DJgAb+C/wG+C3wS6AUKLHaHgJopdRo4GfAgVrr7UqpoYB973ZbEJIjFoEgtJ8LgNu11ru11mXAbcAPrWMBoD8wRGsd0Fov0WZBrxDgBsYqpZxa681a6w3d0ntBaIIIgSC0nwHAdzHb31n7AP4ErAfeUkptVErdCKC1Xg9cDdwK7FZKPa+UGoAgpAEiBILQfrYDQ2K297P2obWu1Vr/Ums9HDgV+EU4FqC1flZrfbh1rgbu2rvdFoTkiBAIQus4lVKe8AN4DviNUqpEKdUbuBl4GkApdbJSaoRSSgHVmC4hQyk1Wil1tBVU9gKNgNE9b0cQ4hEhEITWeR1z4A4/PMAy4EvgK2A58Hur7UjgHaAO+Ah4UGu9EDM+cCdQDuwE+gC/2ntvQRCaR8nCNIIgCD0bsQgEQRB6OCIEgiAIPRwRAkEQhB6OCIEgCEIPJ+NKTPTu3VsPHTq0u7shCIKQUXz++eflWuuSZMcyTgiGDh3KsmXLursbgiAIGYVS6rvmjolrSBAEoYcjQiAIgtDDESEQBEHo4WRcjEAQhL1DIBCgtLQUr9fb3V0R2oHH42HQoEE4nc42nyNCIAhCUkpLS8nLy2Po0KGYNfSEdEdrTUVFBaWlpQwbNqzN54lrSBCEpHi9Xnr16iUikEEopejVq1e7rTgRAkEQmkVEIPPoyGcmQtAGVm2r5oste7q7G4IgCClBhKANnPzXDzj9waXd3Q1B6FHMmjWLN998M27f/fffzxVXXNHsOTNnzoxMOP3e975HVVVVQptbb72Ve+65p8XXfvXVV1mzZk1k++abb+add95pT/eTsmjRIk4++eROX6erESEQBCEtOf/883n++efj9j3//POcf/75bTr/9ddfp7CwsEOv3VQIbr/9do499tgOXSsTECEQBCEtOeuss3jttdfw+/0AbN68me3bt3PEEUdwxRVXMH36dMaNG8ctt9yS9PyhQ4dSXl4OwB133MGoUaM4/PDD+frrryNtHn30UQ488EAmTZrEmWeeSUNDA0uXLmXevHlcd911TJ48mQ0bNjB79mxeeuklAN59912mTJnChAkTuPjii/H5fJHXu+WWW5g6dSoTJkxg3bp1bX6vzz33HBMmTGD8+PHccMMNAIRCIWbPns348eOZMGEC9913HwAPPPAAY8eOZeLEiZx33nnt/K8mR9JHBUFoldvmr2bN9pouvebYAfnccsq4Zo8XFxdz0EEHsWDBAk477TSef/55zjnnHJRS3HHHHRQXFxMKhTjmmGP48ssvmThxYtLrfP755zz//POsWLGCYDDI1KlTmTZtGgBnnHEGP/nJTwD4zW9+wz//+U+uuuoqTj31VE4++WTOOuusuGt5vV5mz57Nu+++y6hRo/jRj37EQw89xNVXXw1A7969Wb58OQ8++CD33HMPjz32WKv/h+3bt3PDDTfw+eefU1RUxPHHH8+rr77K4MGD2bZtG6tWrQKIuLnuvPNONm3ahNvtTur66ghiEQiCkLbEuodi3UJz585l6tSpTJkyhdWrV8e5cZqyZMkSTj/9dLKzs8nPz+fUU0+NHFu1ahVHHHEEEyZM4JlnnmH16tUt9ufrr79m2LBhjBo1CoCLLrqIxYsXR46fccYZAEybNo3Nmze36T1+9tlnzJw5k5KSEhwOBxdccAGLFy9m+PDhbNy4kauuuoo33niD/Px8ACZOnMgFF1zA008/jcPRNffyYhEIgtAqLd25p5LTTjuNa665huXLl9PQ0MC0adPYtGkT99xzD5999hlFRUXMnj27w7OfZ8+ezauvvsqkSZN44oknWLRoUaf663a7AbDb7QSDwU5dq6ioiJUrV/Lmm2/y8MMPM3fuXP71r3/x2muvsXjxYubPn88dd9zBV1991WlBEItAEIS0JTc3l1mzZnHxxRdHrIGamhpycnIoKChg165dLFiwoMVrHHnkkbz66qs0NjZSW1vL/PnzI8dqa2vp378/gUCAZ555JrI/Ly+P2trahGuNHj2azZs3s379egCeeuopjjrqqE69x4MOOoj333+f8vJyQqEQzz33HEcddRTl5eUYhsGZZ57J73//e5YvX45hGGzdupVZs2Zx1113UV1dTV1dXadeH8QiEAQhzTn//PM5/fTTIy6iSZMmMWXKFA444AAGDx7MjBkzWjx/6tSpnHvuuUyaNIk+ffpw4IEHRo797ne/4+CDD6akpISDDz44Mvifd955/OQnP+GBBx6IBInBrOPz+OOPc/bZZxMMBjnwwAO5/PLL2/V+3n33XQYNGhTZfvHFF7nzzjuZNWsWWmtOOukkTjvtNFauXMmPf/xjDMMA4I9//COhUIgLL7yQ6upqtNbMmTOnw5lRsSitdacvsjeZPn263tsL0wy98TUANt950l59XUHoTtauXcuYMWO6uxtCB0j22SmlPtdaT0/WXlxDgiAIPRwRAkEQhB6OCIEgCEIPR4RAEAShhyNCIAiC0MMRIRAEQejhiBAIgpCWVFRUMHnyZCZPnky/fv0YOHBgZDtciK45li1bxpw5c1p9jcMOO6xL+pqu5aXbSsomlCmlPMBiwG29zkta61uatJkN/AnYZu36m9a69SpNgiDs8/Tq1YsVK1YA5hoCubm5XHvttZHjwWCw2dIK06dPZ/r0pCnzcSxdKuuMQGotAh9wtNZ6EjAZOEEpdUiSdi9orSdbDxEBQRCaZfbs2Vx++eUcfPDBXH/99Xz66acceuihTJkyhcMOOyxSYjr2Dv3WW2/l4osvZubMmQwfPpwHHnggcr3c3NxI+5kzZ3LWWWdxwAEHcMEFFxCebPv6669zwAEHMG3aNObMmdOuO//uLi/dVlJmEWjzvxguguG0Hpk1jVkQBJMFN8LOr7r2mv0mwIl3tvu00tJSli5dit1up6amhiVLluBwOHjnnXe46aabePnllxPOWbduHQsXLqS2tpbRo0dzxRVX4HQ649p88cUXrF69mgEDBjBjxgw+/PBDpk+fzmWXXcbixYsZNmxYmxfFgfQoL91WUhojUErZlVIrgN3A21rrT5I0O1Mp9aVS6iWl1OBmrnOpUmqZUmpZWVlZKrssCEKac/bZZ2O32wGorq7m7LPPZvz48VxzzTXNlpE+6aSTcLvd9O7dmz59+rBr166ENgcddBCDBg3CZrMxefJkNm/ezLp16xg+fDjDhg0DaJcQpEN56baS0lfTWoeAyUqpQuAVpdR4rfWqmCbzgee01j6l1GXAk8DRSa7zCPAImLWGUtlnQRCS0IE791SRk5MTef7b3/6WWbNm8corr7B582ZmzpyZ9JxweWhovkR0W9p0BXuzvHRb2StZQ1rrKmAhcEKT/RVaa5+1+RgwbW/0RxCEfYPq6moGDhwIwBNPPNHl1x89ejQbN26MLDLzwgsvtPncdCgv3VZSmTVUAgS01lVKqSzgOOCuJm36a613WJunAmtT1R9BEPY9rr/+ei666CJ+//vfc9JJXV8dOCsriwcffJATTjiBnJycuBLWTUnH8tJtJWVlqJVSEzFdPXZMy2Ou1vp2pdTtwDKt9Tyl1B8xBSAIVAJXaK1bXPFZylALwt5BylCb1NXVkZubi9aaK6+8kpEjR3LNNdd0d7dapL1lqFOZNfQlMCXJ/ptjnv8K+FWq+iAIgtBZHn30UZ588kn8fj9Tpkzhsssu6+4udTmyQpkgCEILXHPNNWlvAXQWKTEhCEKzZNoKhkLHPjMRAkEQkuLxeKioqBAxyCC01lRUVODxeNp1nriGBEFIyqBBgygtLUUmcWYWHo8nLnupLYgQCIKQFKfTGZlRK+zbiGtIEAShhyNCIAiC0MMRIRAEQejhiBAIgiD0cEQIBEEQejgiBF1IdUOAf3+0WfKuBUHIKCR9tAu58T9fsmDVTsYNKGDakKLu7o4gCEKbEIugC6luDADgDYS6uSeCIAhtR4SgC7HbFAAhQ1xDgiBkDiIEXYhNWUIgMQJBEDIIEYIuJGwRGGIRCIKQQYgQdCERi0CEQBCEDEKEoAux2yAbLypQ391dEQRBaDOSPtqF2G2KNZ6L0a/aYPKe7u6OIAhCmxCLoAsJu4YURjf3RBAEoe2IELSD1mYMh4VAEAQhk0iZECilPEqpT5VSK5VSq5VStyVp41ZKvaCUWq+U+kQpNTRV/ekKWssKDWcNCYIgZBKptAh8wNFa60nAZOAEpdQhTdpcAuzRWo8A7gPuSmF/Ok1ruUBiEQiCkImkTAi0SZ216bQeTcfS04AnrecvAccolb6jqdGKSWAXR5sgCBlISocupZRdKbUC2A28rbX+pEmTgcBWAK11EKgGeiW5zqVKqWVKqWXduZC2uIYEQdgXSakQaK1DWuvJwCDgIKXU+A5e5xGt9XSt9fSSkpKu7WR7+tGKc0hcQ4IgZCJ7xZmhta4CFgInNDm0DRgMoJRyAAVAxd7oU0cQi0AQhH2RVGYNlSilCq3nWcBxwLomzeYBF1nPzwLe02m8qktrPROLQBCETCSVM4v7A08qpeyYgjNXa/0/pdTtwDKt9Tzgn8BTSqn1QCVwXgr702lacw2JRSAIQiaSMiHQWn8JTEmy/+aY517g7FT1oasR15AgCPsikvDYDlpLHxXXkCAImYgIQTtoLXgh8wgEQchEZOhqB626hsQiEAQhAxEhaA+tCEHapjsJgiC0QM8UgndvhzX/bfdprcUI4g6nbxasIAhCHD1zYZolfzb/3lrdrtNaG9rjhMIIgt3Zvn4JgiB0Az3TIuggrc11iztqBFPaF0EQhK5ChKAdtLYmfYJFIAiCkAGIELSD1mYWxxkMoUBqOyMIgtBFiBC0h9ayhuIsglBq+yIIgtBF9GwhMFpfZD52cG89WBy7Ia4hQRAygx4nBEvX745u+FrPGoq7yW9P+qgIgSAIGUKPE4KLHvsoutG4p9X27ZkaEB8slhiBIAiZQY8TAjsxvvu2CEE7XENxSIxAEIQMoccJgSNWCNqQ2RPn7Wklf1TSRwVByER6nBDYiQkQt2Gwbk+lCEkfFQQhE+lxQuBst0UQ4xqSCWWCIOyD9DghcKgYIWinRdDqhLLYDYkRCIKQIfQ8IWivRRCXPtpaW8kaEgQh8+hxQuBU7XPfxLuGZB6BIAj7Hj1OCOIsgjbctce7hlpGYgSCIGQiKRMCpdRgpdRCpdQapdRqpdTPk7SZqZSqVkqtsB43p6o/YeJiBKG2WAQxz9tlEUiMQBCEzCCVC9MEgV9qrZcrpfKAz5VSb2ut1zRpt0RrfXIK+xGHM3Zof+VS8FbBwZc12z5uQlmrWUMxG5I+KghChpAyi0BrvUNrvdx6XgusBQam6vXaikM1sQI+frDF9rqZ58nbimtIEITMY6/ECJRSQ4EpwCdJDh+qlFqplFqglBrXzPmXKqWWKaWWlZWVdaovDppUHNUtVyCNixG0mjUUsyFZQ4IgZAgpFwKlVC7wMnC11rqmyeHlwBCt9STgr8Crya6htX5Eaz1daz29pKSkU/2JCxabF2/5hHZUHzViy1q3ocS1IAhCOpBSIVBKOTFF4Bmt9X+aHtda12it66znrwNOpVTvVPYpLlgMrVsEbZ1ZXLmJ0fXLYk6UYLEgCJlByoLFSikF/BNYq7W+t5k2/YBdWmutlDoIU5gqUtUn6KRrqKUowUd/54c75ka3JWtIEIQMIZVZQzOAHwJfKaVWWPtuAvYD0Fo/DJwFXKGUCgKNwHm6tRzNTuJQ7RSC2Oct9SzoxW14o9sSLBYEIUNImRBorT8AVCtt/gb8LVV9SEbTGIHWRoudNNqaPqoNHDomQCyuIUEQMoQeN7PY2V6LoK2uISOILa7EtQiBIAiZQY8SglXbqgkE/PE7W8nuaXOwuOnA34rACIIgpAupjBGkHef/9S2+8jwQt68111Cb00ebuoLEIhAEIUPoMRaB1pqjbcuTHGhHsLilhgkWgQiBIAiZQY8RgrI6H6NspYkHumpmcVMhEItAEIQMoccIQcXnr3ClY17igTZNKNNcZH8Tu7eyhYZNhUDSRwVByAzaJARKqRyllM16Pkopdao1azhjqModwfzQIYkH2mARjFebuM35JEOXXNt8QwkWC4KQobTVIlgMeJRSA4G3MCeKPZGqTqWCQ6cfyK7jH0o80NoaA4AL8+7e4dvTfMOmFoC4hgRByBDaKgRKa90AnAE8qLU+G0haKTSdcTvtiTtbtQg0Rvjf1FLbpq4hCRYLgpAhtFkIlFKHAhcAr1n7koyq6Y3Hkfh2lQ61aBVoHZMt1JIQNJ2PIBaBIAgZQluF4GrgV8ArWuvVSqnhwMLUdSs1eJJYBAoNQV+L57XJImjqGhKLQBCEDKFNE8q01u8D7wNYQeNyrfWcVHYsFSQTAgDqy6BwcNJDhtZEppzJhDJBEPZB2po19KxSKl8plQOsAtYopa5Lbde6Ho/Txt+CpyXsryvbwo7qxqTnmK6h8NzjlmoNiRAIgpCZtNU1NNZaXez7wAJgGGbmUEYxqCibe4LnMtT7bNz+u198j0P/+F7Sc2KHfiXBYkEQ9kHaKgROa97A94F5WusAra/lnnYM653Du788ikOH94rb76zfAUAwlDjQa63NOIK50fzFxSIQBCFDaasQ/APYDOQAi5VSQ4Cm6w9nBPuX5JLjjsYKAjYP/ZU5Y3hHtTehvQbskfLSLQWLm6xzIEIgCEKG0CYh0Fo/oLUeqLX+njb5DpiV4r6lDLfTziajLwC1zt70VeZEsa17GhLaag22tlgEWoRAEITMpK3B4gKl1L1KqWXW48+Y1kFGkuW0c7z/T4z2PkG1yqO33RSA0j3JAsY6uuBMi66h+PRREQJBEDKFtrqG/gXUAudYjxrg8VR1KtVMGFhAAAc+XFSGsimyNXC2fRHn/G88NMaXkTAtAlMIVDtcQxIsFgQhU2jrwjT7a63PjNm+LWZB+ozjR4cOYfzAAuZ+tpWy1dn0ZRsX2t8xD5Z9DftFi9OZMYKwa6ilrKH4Yzok1UcFQcgM2moRNCqlDg9vKKVmAMkT7zMApRTThhQxsCiLXQEPebqOPTrPPFi3O66t1mBTHXANiUUgCEKG0FaL4HLg30qpAmt7D3BRSycopQYD/wb6Yt5YP6K1/kuTNgr4C/A9oAGYrbVOsoxYahhYmEUpuRRQx0z7SnNn1Za4Nppo+miL8wgkfVQQhAylrVlDK7XWk4CJwESt9RTg6FZOCwK/1FqPBQ4BrlRKjW3S5kRgpPW4FEhSJzp1DCzKokY3iXlXfRe3aRix6aPNWwSGpI8KgpChtGuFMq11jTXDGOAXrbTdEb6711rXAmuBgU2anQb820pJ/RgoVEr1b0+fOsPAwiyqmyY/VccvZ6nRraaPrtxaRU1DkzkIIgSCIGQInVmqUrXexGqo1FBgCvBJk0MDga0x26UkigVKqUvDqatlZWXt72kz9CvwkEOTAdxXG7fZlqyhb3bVxlgNFiIEgiBkCJ0RgjaVmFBK5QIvA1fHWBPteyGtH9FaT9daTy8pKenIJZLitNv4LntC/E5fYhej8wiSC0GWy54gBOIaEgQhU2hRCJRStUqpmiSPWmBAaxe36hO9DDyjtf5PkibbgNj6z4OsfXuN+l7juMT/y8i2TmIR2FuZUKZQSSwCSR8VBCEzaFEItNZ5Wuv8JI88rXWLGUdWRtA/gbVa63ubaTYP+JEyOQSo1lrv6NA76SADC7OiqaNAdVVl3PG4rKFmjKAGfxA7MqFMEITMpK3pox1hBmap6q9iJp/dBOwHoLV+GHgdM3V0PWb66I9T2J+kDCzK4ktyI9ueUH3c8XiLILlrqDEQwqHENSQIQmaSMiHQWn9AKwFlrbUGrkxVH9rCoKJsqmNSSD0qQOi7j7G/9gu45E00MUXnmrEIGn2BhH0tzjkQBEFIIzoTLN4nGFKczR7y2Kz78akxGoDQ/Gtg92ooXYbW0aJzzQ3uXp8/bjuklWQNCYKQMfR4IThkeC8e/uGB3DzkKeaGZgIQ9FsppSF/vEXQTLDYF4gXgiB2KTEhCELG0OOFwGZTHD+uHy67jVqdZe4M+sy/vtq4GEFz8wiaWgRB7F1mEeiW6hsJgiB0AT1eCMJoranDFIKshu3mzvpyQLdadM7rj48RBLGjuiB9dOmGco6+6Z98u2JJp68ldJ7lW/bQ4Je0YGHfQ4TAIqR1JI00kibaUI4Rs0JZc+mjfn+8RRDA0XLJ6jby0YYKFrp/ychXT+70tYTOsbWygTMeXMrt89cAsL2qEV9Q3H/CvoEIgUXI0KzRQ1hpDI/urC+LKzHhCDVaVkI8ySyCrnANFWa7ohviIupWvt5Zy8FqLcduuht/IMRhd77HdS9+2d3dEoQuQYTAIhjSgOL5UMxSzJ8/QVb5lzHpo8DDhyec608mBF0QLC7KdkY3kgiQsPf4bvceXnD/jmPr5uNvqKI/FYxY+6AItLBPIEJgETLMH/ROXRy3P6f8y2itIYDaxInP/qZZQ7prhMDtsEc3qrc031BIOVt2RBcsCnjr+ZvrAebY5sLutd3YKyGdeGv1Tl79Yq9WyOkyRAgsQtad3W5dBIAP827c4d0TbxEkQYfiB30zWNz5GEFIa8q0tRZQk/LYwt6lrjFapTbka4hWrZWJg4LFpU99ztUvZOYKviIEFkHLIthlCYFTB6nRWTh8FYkF5ZpgNLn7D3SRa0hrzW5daG7U7uz09YROEPN5Zi/5A27CVqC4hoTMR4TAwrCEwO8xheAVYwaVOh+nt7LZ+QNhbE1SRbsqfTRkaPyWZUIgY5eI3ieIrR2V/e1/GWbbZW5IlVlhH0CEwCJsERRku5nkfYQbApeyhzwcvj1xFoG2ORPONSw3UKM2s3yCXZQ+GjI0wfBHVF8GL/8/aNzT6esK7afZIoLhyYeCkMGIEFiErMG8MMtFNbkEcVCp83B6K+NiBNqZnXiyNUh4MYUggB3VBa4hQ5tFsAH47kP46kXY/kWnryu0n+aFwJt8vyBkECIEFkeMNFc+mzCoILKvQufj8FbEZQ1pR1bCueFBPywEoS7KGgoZ0clslbutbISgv4UzhJTRnAsoIEIgZD4iBBa/OvEAllw/i+PG9I3s20MuDl9VZDD2aUfyAd4aJLzadBv5cKKMxNLU7cXQOuKWyg1aC+aExBXRHTRr4YlFIOwDiBBYOOw2BhdnM3lwYWRfrc7GYfhwKXNQfyZ0bNIfvkObxxvwAKaA2EO+Tgd4Da0jgWoX1h1pqPMCI7QfQ2IEwj5MKlcoy0iKclzcfdZEfEGDDfPfAKAAc9WyBtzYgomDu90w3TXhonWRpS8bKqFgYIf7EjJ0YuqqDDzdgmpWCCSbS8h8xCJIwjnTBzN5UGFkYC9Q9RjY8GqXmRba5K48YhFoNwDlOt880FDRqX6EDJ04mU1cQ92Cbm6CoAizsA8gQtAM+VmOyPoE+TSglaLRCga//uJj4K2JtLVb8YB6SzgqMYWgsaasU30wYlZHiyCuoe6huWBx0Asf3AfbM3NGqSCACEGzFGQ5qcVMFc1XDWhlx2cJwffW3Qhv3hRp67BmmdZpK0ZguYaufXIR3+6q7XAfYktgR5A70G6h+WCxD965FR45aq/2RxC6EhGCZsjzOKmzLAIzRmCj0XL9AGhvdeR502Bx2DX0d9cDFL58Tof7kDRGIK6hbqHZeQT++r3bESFtyaWBzZ4fwLrXursr7SZlQqCU+pdSardSalUzx2cqpaqVUiusx82p6ktHsNsU2mXe2eererSyRVxDAIHsPpHnYSGoi2QN5UWOlexe2uE+GIa4htKFZi0CX8ctPmHfYn9lrWy4+E/d25EOkEqL4AnghFbaLNFaT7Yet6ewLx1CecwBvYB6ULbIhDEAf51pERiGxoU5ONfoHMAMGtfoxIln7SWkkwSLxTXUPTRXMsRft3f70RN57VqYe1F396JVVJJnmULKhEBrvRioTNX19wa2LHOWsUcF0MrGB8YEfuqfw7fGQAINVYBZoyic4/9Nn+NZMvFOdtKLMh2dj9DR2adGUtfQvjez+NlPtrC7Nr0nZoWLCF4f+Elk4iAAPhGClPPZo7Dm1e7uRdtRIgTt5VCl1Eql1AKl1LjmGimlLlVKLVNKLSsr61wmTntwe3IIaetDVTb8OHndOIRK8tCNphAYOmoR/PvKEznijCvIcdmpjHEPUd+xPhsalGqaPtq9QlC6p4HtVV2XO7+tqpGbXvmKy576vMuumRIs19CkqYfynY7OPte+mubOEISMoTuFYDkwRGs9Cfgr0Kzka60f0VpP11pPLykp2WsdLMh2ReYSoKKrhdXoHLAGgKChcSsrtdBuuo76FXjwx9411kdXt2oPIZ1+E8oOv2shh935XpddLxgy319FXZpbOtY8guK8rMiiRQBaYgSChcrgtSm6TQi01jVa6zrr+euAUynVu7v6k4z8mBRSVPRfVUM2Dr8pBCErRhBSzohJ2K/AY65bHKaugxaBBIvTB8sicDqcGLE/G69YBIJJVAjENdRmlFL9lDJHTqXUQVZfOjcVt4spzHJGJpVhi7UIsnEGzDvBkBUjCNmigeS++R78sdU7OmoRJBECvY+lj2bK2u82SwjcLidFxFgB3qpu6lEPJFO+LBkYI0hZrSGl1HPATAP4rQQAACAASURBVKC3UqoUuAVMm1pr/TBwFnCFUioINALnaZ1en3RJnjvGNRRrEeTgNurBMCIWgREjBAMLs/jCGMlx9uXmjsaODRamayj+X6IDvgy839gHMEJgA5fTST9l5kDU6CzyRAj2HiE/ONytt+t2Mu8XmjIh0Fqf38rxvwF/S9XrdwV98z2RSWU2e9Qi2KZ7mWmdFesx3PvhIogRs3LZkF45XBc6hQ+M8bzqvgVbzOSz9qA12FXUIqjTHrL2wayhTEBhWgR2hyMSE1qrh3CwXted3epZ+OvTVgiSunEziO7OGkpr+uRHLQJ7jBCsdU4wn2xcaKaPqgCGPWoRDO2VjcbGl3p/6m05HXYfNC06V64LMAL7lmsolF5GYPNYweKJg3tFdm00+ndXb3omgYbu7kGzxFrvGfKNjkOEoAX65nsiMQIV4xpSvYazQ/eCBddjq/g2wTU0tHdO5HmdyoUOWgShcNE5TwE+Rx5lFKD3sQllISMzfjbhmcUupxNmXE1D4SjqrZnkwl7Cn8ZCYGjsqvOrEnYXIgQt0CfPHZM1FLUIpg/txe2BCwGwla0zXUMxFkGvnOjzGnI6HCOITCib8kNeP/pN0021j7mGMkEIdGwVWJsdjruNTee8s+8LwY4vofzb7u5FhFAa13UKxkz+1BkYIxAhaIFctyMSI4gtMTBjRC+WGaMAUA27cTexCJRSrLj5OE6ZNMCcc9BRi8DQZkqazQFZBfhxovexNYszQQjiiv9ZNwS5bkdcEcJMpqrBz/UvraTe16TU9j+OgL9N755OJeGtFZu6uwvNYn5HxCLYJ1FKcfExE80Na9nJXjkuxg8siKw5YKsvx6WC6BiLAKAw20VRtpM9RnaHYwSGxhyAlA2X3U4AB4667bD8KbP+fXOLpWQQGSEETS0CTCFoYN8Qgvve/oa5y0r5zxfbursrLVJdnb4ZWiFD48hgi0CWqmyFoiIrOOivZ8n1s8jPcpLvcVCUm0W9LsDWUIaLANrmSjg3x+2g0sjqsEVgrlmswWbHaVdU6CzsviqY9zOzwblPw5hTOvrW0oJgJghBEosgx+2IlB3PdOp81hwJR3rfFxY7m1kcKA2ItQjS/xudSHp/8ulAtiUEvmoGF2dTkOVEKUXffA9VtkLsjeW4CKLtiXeHuW4HVUaOWZeoA9kxoZARtQgcNtbrAfENOigw6YSRAVlDcUJgWQRuhw2f2jeEoDFgDrDZLnsrLbuXo9f+Jm0nlSVdOySDECFojT5jk+7O9zjZo0whyMaL4cxOaJPtslOh81AhX4fKFRs6ehfqstv4Rg+ObxBIg4XTvTWd+nFmgmvIMIj6fy0hUEphOHNaOCtzaPCb781hS+/hwGH4zKVB05CQjnUNZR7p/cmnA4X7Jd2dn+WgTBfgrN9Jnmog5MpLaJPjdkTLUde1v8xEZFUsZaMkz83XhikEpdoqydTdlS9rtsOdg+Gjjs8LzAQhCBpGgmsIgCTin4k0WkLgD6XnHa0fJw3hwHyappCGQrHB4syLEYgQtEYzdUPyPU7WGPvhriulRNVgOBOFINftoAxzTQPqdrX/tcOrYtlsDO2dQ5W9mNcHXs0F/psIaDv4atld46Wsdu/NLXDj52HnfYQqNkHVFnPnmnkdvl4mCEGyYDGAcu0bFkFjwBKCYHoKgY0QNeE07jRdCCikdaQKQCYGi0UI2sL1m+C6DXG78rOcfBwYEdkOuVuzCNovBDp8h6ZsOO02RvTJ4y91x/Cd7kctWQQaqjnoD+9y4B3vtPvaHeUI21ecYP8M3rgxmlLbiSJbYSFI5zpdyYLFAHgKk5+QxtT7gjT444OuYddQnBCkiy/ecrnUaksI0nR2cSjWasxARAjaQnYx5MRXyC7IcvKpf2hkW7vyE07Lddsp12GLoAXX0B0DYGmie0Xr+MFnTP98vt5lVr6s01n46/dusNgwdMT/qQ0jRgg6/jUKC0G6jDvJMGeNJloEtuyi+IYZkM477pY3mXDrW3H7Iq6hYDQPvrwmTe68Lfdo1CJIz0llIQMckjXU88j3OPDFrGGs3bkJbXLcDvaQi6HszVsERggC9fDWrxMOxcYIAA4ZXhw5Vkc2wca9GyMIaR0xe7U2orOcOyEEmZA+agaLDTS2ONPFmVPQpGEarxVRvQ2++whIdMdFXEMxMYLrnl+29/rWAtoIrwee3q4hM44UFoI0Nm+bQYSgg+RnOeO2tbsgoU1BlhONjQZXb6guTX6hljJ/rHVyw3ehM0ZErZJasjAaTYughGbSUwNeKPumhXfRPkJGVAgMrWMCdx3/4mdC+mg4WKybCF5+dpOU4XQu/3HfOHj8hKSHwp9BrGuopi49Blyfz4x/RUq9pGuwWNJHeyb5HlMIIgvQJHEN9cv3cNCwYlb5+6PLmilX3EI6XFPX0IDCrMixOp1FyFvDSFXKZ56fwrJ/JV5g+b/NMgGBrkm5C8W6hrQRFbEuiBGkM4YVLNYqPs8+/B2IkNarxzX/fw5rcawQOHV6vBef3xTXqEWQrq6hmFpD6f+VTkCEoIMMKzEzRiJpbZ7EYLFSipMm9OerwAAo+9p0A4WC8MXT0bWHW7QIwq6h6EC7+LpZXHPsKLz2HJSvlhHKKguw/t3E82u2mULTRQE2041juYYMHb1uF8QI0jlYHC4o1lQICrIyRAhaGZnC60b7YlxD9jQRAq9lEVRjZWgF0lcIHFJrqOexf0kuP525f7TejCsxRgAwuDiLb/QgVNALnzwM3yyA/14J79xqNoi1CJrMFNZGYoByv17Z/PzYkeisYrIDlbhp4QcbrnHURZNwYpfONC2CsBDs2xZB+G5P2+KFIMfdZCZuusYIYhIVkhVGCxiJriFH2giBaRHUZpJFkM53Nc0gQtAJ+hV4mBuaaW40zSCxGFyUzaLQJOpy9oN3bzctA4CVz5t/Y+/W79zPdOdYaB0fLI6lumg8WbqRw2yrzbbJXjwsLF0kBEHDwEnQ6lvXWgTpTHhmcVOLIIE09F+/8kUp5ds3Rrab3jhorQlYlkC8EARjG6W2ky3gtVxDdSqDhECCxT2L3rlu/hI8g4neR1HZvZK2GVSUTRlFzBt9NwS9hD54wDzgrzN/YE3992v+G3kasQiSDEC+AQcBcIL9UwCMZOWpw+sgdNFiNoYBrrAQGKGoW6szJSYywKEamVncRPC0hq+NQZHt5xYuo6IufRYO8gcNrnlhJb+e+2lknxvzexJeHjxk6KQxAoeO+T51o8vLbwmB3ZmND2f6CoGOWZhGZ17QWISgE/TOdaOxUUMO9mbMwSyXnYGFWfzuM/A7crH7rbv0kB8aKiHYJEYQM2irFiyC/H4jKNP55CvzfF1XlvjiKbAIIrnSsVlDnbh+RqSPNhMsPmZMX36WfTdP7n8vAB+uWMMjSzYmu0S3ELRuJLyN0cEzbBEEQtpqY/69xfEkl2z6RaSdPVYIutHl5bNcQy63Ey/utBWCYGyMwMi8WIEIQScoyYumD9rtzZuDfzxjAo0Bg3X+vvEHarYlWgQxd1+RrCFbokUwqDibtcaQ6I76JBPWrBjBZ99ub7Zv7SFkaJzWwu3EuoY6IQRGBghByMBcj7bJ51CS5+btG0/isCOOA+Aw22oml8/vji4mJTzYe4gO6m5lfr+8wfi5Az92vMkBDdG5A85Y11A3psWGLQKPy2Wt9lfZbX1pCSM2fVSEIIpS6l9Kqd1KqVXNHFdKqQeUUuuVUl8qpaamqi+pIk4IWggQHTmqhCtn7U+VFfD61BhtHqjZlmgRxP7oIq6hxI9pUFFWtI4RYGusSJzZalkE97/5VWtvpU0EDY0zYhHEpI92Ij21IzGCYMjgf19uj7g3Uk3QMLCrRIsgzMghgwkpBz9wvMeJG+6Aig1J2+1twtlA7lghsCyCd9fustok/x/GBYtD3bcOQHW9ebORl5PNVt0HKtNzlbJgnBCk77oJzZFKi+AJIPkMFpMTgZHW41LgoRT2JSXkuqPr+rRWwbd/QRZrtHkH/6fAuebO6tKEQdTna6S6wfoRGs27hvrle5gfOgyAZ4OzsIV8cHtRdI1ZrSMxghYzi9pByNDRYLGho6l8nbAIOiIEz3yyhZ89+wUvft7MJL0uJlKGOollBoBS2PP6Rbe/eok3Vu2krunSj3uZsNsnSyUKwTUvrMQwooHiCNbNRNw8gr1oEfzpzXXMuPO9yHZZtSkEvfKy2WT0MUU2DUt5BEJRt2k4RrC5vJ4H3v12r92wdIaUCYHWejHQkh13GvBvbfIxUKiU6p+q/qSKu8+cSN98d2JOeRMGFmZxX/AsLsl9iImH/R9+bbdcQ/GZJtvLKjn3EbMUgG6hlo/DbmORMZlh3qf51BgTPWDNJ/hiQ2mkemlXCkH4y17vC3RJjKAjweJ6q2ja+t17Z/ZrNFjcQtZQbp/I06rStVz+9OfcPn/1Xuhd84SFIM41FPO8ujGQWHHUKuEQZxHsxRjB3xduYFtV1EourzFvNnKzPWw2+oKvGh4+fK/1p614AzFF56zf3Zznv+Det79ha2UarBvSCt0ZIxgIbI3ZLrX2JaCUulQptUwptaysLElQtBs558DBfHLTsbgdLacW9i/04MNF0ZBx9MrzsEsXE9xTmjCI9lVVbN+5k2PvfT8aLG7mTvT5Sw9BY6M8xkXEGzfAutf56WMLI7u6UgjCWUPbqhqidV86sUBORyyCXjlmjafyvVR+2xswsGGgmrMIAAZFF3k3qrdxneN57l51ZLfevYZdQ8liBADldT6CMXNDgIg70RH7nemGrKFw7KiixrzZ6FOQwxZtie3u1Wnnh/cGQhEhUEbYfQrvuK4lb96Pu7NrbSIjgsVa60e01tO11tNLSkq6uzsdYnBRNnluBzNG9KIo28V2ehGq2powiLoI8DPHq+bdbsQ1lHwAOmR4L16bc3i0wmmYTx6iQEWzKzzKz9AbX+PJpZs71HfD0Dz7yRYa/KGIa8iGRkeykjo+IHdECJSVp11ev3dcFo2BIA4MlK2FJb6PvI7teRPYYPTH0bCbKx3WGg3duHhQIKRxEGScbXNknwc/F9jfIYdGyup8BEIGBcRYVlZ/nd0sBOFgdmWdKQR9C3N4x5hGab9jzQZ7Nu/1PrWEKQRh15D5tzDbyQjbdoq+e6Mbe9Y2ulMItgGxay8Osvbtk+S4HXz662P5/uSBFGW72KmLcW/7GL0h6g+tLRjFe8YUTrF/hMJA07xrKEy2yxEnBNqVCztXUUBiyuB97zQpQFe9DTYspDUWrNrJTa98xf3vfIPDypV2EKKhNjxPobHDAeOOlKH2WYPE3rII6n0hXARQzsR1qSPk9mH+9CdZZEzG3RiTwdWNWS5Bw+BGx3Ocav8osm+mbSV3OP/FzY6nKKs1haAw5qYhbBG44uYR7P2soQZ/iAZ/kPO9cwEoyc8hhJ2rvjvCbBCemJkm+IIGjnCpcsulW5gdrU6c7gWIulMI5gE/srKHDgGqtdY7urE/KSfLZUcpRXGOC7827y7Vdx9GjjeoHN4MHUh/VclItS3qVmjBJVGc7aKSaJ2jh50/hMZKRtqigdSwX9hjua/qfUHmrdwOj8yEp76fcM2FX++m1hu9C6yxnu+s9kYsAhcBQo3VVLoGmI22fd7Wf0McYSFoj2XgCxocpNYyvnpRh16zvTT4g7gIYHO0IASYYr9LF+I2YuI+jXtS3LvmCYY0023x4l+kzLv/PmoP5XV+AiHNT+z/ixwPW3lOI0Zku8GqafCF2FhWz5F2M+PN6TQH1W+15T3elTQZsdvwxVgEYdeQih38a9N7aEtl+uhzwEfAaKVUqVLqEqXU5Uqpy60mrwMbgfXAo8BPU9WXdKMo28lKvX/C/s8rnGzSZvZJf1XZYtZQmDyPAwMbi0KTuCHwEz7YY66aNdkWTWGc43gFN37cTvM6t8xbzZznvojOPYhJD6xuDHDxE5/xwmfR8E04MTZgGJH0UTcBcvEyv2GcefC7pW3/B8QQDha3pxy1L2gw1/077jbuScx6SQH1vhAuFcTWkkWAmUW2UxfH7+xOITB0tCiihcuyDifbNnD8J7PRdbv5gSNqFTbWmBaM04hxWT55CgCPLt7IwX9I3Wp4sTcD9f4gG3bGWFN2B+/84ijqyKY6byRs+Thl/egI3mBssNj8Pfn9MVZyxfpu6FXbacHp2Tm01ue3clwDV6bq9dOZohwXz4aOYaKzlHN4O7K/VJewG7NmUR+1hy3ebPMTakEIbDZzmJ4duIFct4M8n5kfPllFv3i5yssl9gW847gAgC2VTWri+GtpsOexs9qLx2lHa8zMjaqt0FiJtvrkCHo5xXIzFKk6bEpTqkv4lsGM3NJBIeiIRRCIBgr/s7yU6UOL2b/ELPrX6A+R5WqlJlA7afAH8aggqhWLINftYLPuF7+zoTstAgNvzOJJAMfbTcutSNVRVLsC45u5GFqxpmgW46veo7ZyJ1la4zJ88beJ3hrueH1t5LoOe9fdQ97831Us/qaM//x0RmRfgz9E6Y6YxZyCfnoVmu9lR/4kCkrfMm+UWgrg70ViYwTKcg1pX220QX15d3SrzWREsHhfoyjbRbbbxWeBYXH7S3Vvxo0aCUBf9mCEWrcIYvn998ezkyKCNg8jbPGziYeoXUzWll+16Zjrr2fOcys4+s/vR3Lfd9V44f7x8I8jIymGPw88Rh9lxgXCLoZasvk4dAB666dmnaR23gG3RQjuXLCOoTe+Ftn2xaQ83vDyVxzz5/cBWLWtmjE3v8Gbq3e2qw+tUe8PkaUC4PC02C7H7WC9bpL41o0WQSCkaWwiBE0pWf8SNqWpLZnKBqM/WV+/SoM/hIcm8ZfKaOmMGm/Xzo949qMNLKr/PsaHZh2u/lTw8iffsnNXzOeY14/8LCdKwfydBaa7qhv/t03xBkI4msws1r6YIHyazogOI0LQDdhtiv375LJHx69hsEEP4PAxg/C7Cumr9mALj9htvOuZsl8hbqeDLU5TYGp0dCGbcx2LuLvmuuRrJ/vqWPytmZa7ubyeEvYwemd04N1VY5q4++vvEk6t01l8EjoA5a+DuT+C169vU1/DRISgBdfQw++bbq5wSqEvLvc9et4XW02RWvR116YYN/iCuFUQ7C0PqnkeBw142KZjChB2c7DY14oQZNeaM3VzivrwYugo8iu/pHz3drJVEyEo/5ZfOZ6hhCqqG7s2i6gv5oBesOyvAHzkuYoTv7qGleu3mA3OfRqKh2G3KbSG0kbLMksHIQh4IeC15hHEWwQqti5SN1qGbUGEoJu4+eQxBPc/nmsDlzHd+xCN/3cPd183hwsP3o+QK58fOt5hgLLMyTZaBEU5Lsb2z2epzxSCerISG1VtQTc1Cfx1ZFvulPVldTzsup+f1/45cvjBRaabKVmN+vOPHMcHxvjojoaKNvU1THtcQw2BEE98uIknlyYvqxyewWnr4irA9f4QboLQhmAxwDcxFUm7O1gcO4GsKd8aUeslv7gvGzED/7U7NuDBxwajPw8ErWSCzx7jMsdr3OJ8kuoGH7z7O6jaYmaddTIjZoAyvzM+R24kGeEI+yryleXCzO4d174aa+2PcHXd7uRP+8M9o/AFQ9HvopU+muWPcQelg2i1gAhBNzFtSDFPXHIIL4WOopwCPIf8PwYWZaOUorFkIgA/Deeit1YH3yLX5WDCwAI+8o8AiEzA0fmDeKP/FWaj6q2Jv1tfLdlO8zU27K6nD/E/sGedd3Cb4/G4sgMbDHMSeLbHTVVM1hL2lmdYNyWk2y4E9b4gt85fQx7RQGYOprViGDpyDXsXK0GDP4hLBVq1CMIlR64OXMlTwWPNZUz3JFpRe4ugocmOcfE8Gvxe3PFVemjkuT2nmMZsUxi85ZvJxkcdWdwbPId6VwnsNuMDHvyEtq2AJffA/RPMrLNky6S2g/ANTx3ZZOFN2I8nfp5MtbZWK0uHwdVfB75qvAGD3jYzJqB0iI/WbeF+/22RZrqxfTdIexsRgm7m9Cnmj0/FFK0rP/5v7NDFjLRZ0ypasQgevnAqp08ZiM2mOG5sPxYYB3Gp/xpm+69nzvAFqDlf8O2gMwHQVVsTL+Cvx+2MWgRN/cqH2ddwkePtSMYJwD9CJwPgLDKnghzq/Stb6WfOTWgHoVDbhaDOF8RhUxSoqO81W5kDR2WDP3INW0dXiAoFYfsXCbvrfdZEujYEi8G8Y/2T41IWqYPNWbDdRDBkROsM/fAVfnLzP+OOrzKiMSpbTm+MAvOzDFV+R5by41dmTKTSPcgs7QAY2GhsbJJssOrlTvVzoGURVBlZ5MS4pO52Pmo+aSIEVWGLwNu9FsHGsuj3sDEQorcy/0fBYJCbnnwrcszQCrXyeQLLn9nrfWwrIgTdzH3nTmbznSfF7etTkMNSY2x0h7PlIOUJ4/tz37mTAZgxoheHj+qLPuBkGvGQk5sHDheunCLqtIfAntJEf/wLFxBoNO9mNpXV0UjyAS924tHc0Cwmeh8ha4BZ52gHvVgSGgu7vmoxlfS7inoufuIz6q2gdHMWQbJCXfW+IHabojBmslyudQe5q8YbiR10WAjevwsemUn5+vg5EQ3+oGkNtWIReJzRn9OAwixWhwab7pOyb1o4K3UEDE0WPur3Oxr2Pxpc2XHHV+moEKicXhQU9aaOHPIrVpKjfEwfOZDiHBc7HQMi7Qxs+Oua3Ilb1kJHCd/5+/1+RhUnGZJihOCuMydELYL//AR2fNmp1+4Mt8yLivxnm8opwpxvkW3UsdD9y8gxmzK/y855P6X8q7dJR0QI0pDCbBeDf/A33p7xHFzyNvSb2OZzlVL8++KDuPnksda1TFdNca6b7boXwYrNNPhCDCQ+oDrGuwIwM0KaE4KIz9bClduLwphiexHf+Js3Ndu/2+ev4b11u/lgvfnjTxYsPufhj7jwn58knBu2CHqp6ASni+xvAmYBunDGU6ij9X12mP+Dp99YHLe73hfCqf2tZg3FWnXDS3JYFrQG2qfP6Fh/OkkwZJiuIWd20uPbc8czP3QIHxtjcGTl078gi4/1GMbseY8JaiPKlU1JrputMbUgR6htOKs3x18o0NipOEGxMm9CXME6Bidb+tuVE3l67oH7MX7/mHU4kkyI3GvEvOcf2N/DgYFPN+8abdBuggvvAuDvC9fzj/fTo1w5iBCkLQeNGcZxx30PBh/UocXhw26KcIG2/gVZrNcDsZWvo84X5EPPz+Pa22MWInG1oUjdy1ccxutzDo+ruvpM6Fhqe02E2ubTN8OzlLMsV1RsiYlwVtCnmyv5cH0FJ967kA/fNwf6WbYvMCo24bDbGKSiIna+YyHF1PDxxkrqrLTGBn/HCpL5beZAH6qNXt8wNJW1DWZhtlZcQwBLrp/FEz8+kKNGlfChMZ66sT+A6q1tqo3zxqqd3PXGOryBrimoFgxpspQvwRIA4Nhb6Vecz1WBOZzn/y1Ou7mS3hzfT6M3As5sinNcrDWilWBG2LZzxIZ7mrxQI8TmzLeTQqvWUa5qpK+nyXvvNTLh+293xEx/aqiArZ/SJoK+uDTYzuKOib/c4TTjJFXkxLUpHXgi5/t/zX2uS/lf6BAclespr/Pxj/c3MP/LrlkwqisQIdhHKcpx8ZfzJnPGVPMuvV+Bh3XGfrhrt5DnTfTjl6iqiHjEBmMTOPFPcOHLTBtSRJ98Dw67jVy3g965LgI42ND7aHM6/Yd/SXp6OPUwPFjHuoRCWltioJlh+4pzKx9kxsJz2F9t43HXnzj4rdNx2BRDVbzQnDAEnvt0Cy9+vjXu2u2lrMHsy3TvR5HZ1rtqvRCyfvCtuIYABhdnM3N0H4b0ygEUa4fPNg98vSD5CSueg69eAuD+d77hoUUbeKmL1lkIWq4hFSsE338YzngUDr+GcQOiLhenw8bYAfk04OHd0BRrZxbFuS4+CI6hVVoQ/5YIGTricsyngV4u64bk4rfglAfgkrcSzqmNmccQdObBc+eDt+UyGN5AiCduvRAemNJlk7s8ocRlM4uJF8SGUx7hI2Mcf6mZyUbdn97s4bn5C6jxBqms676V35oiQrAPc9pk08cLlhDowSg0b/CzhLYD7NWcNSYLFwHymriA4jjwEhhxbNyuY8b04cpZIxjaK5tnN1t3RG/fDEnWUa5pDFJAHb5qa4WsGDdOyNBUNviZqr7lGdcfme0wB4Gxysy8cQZrcdgVQ9Qu6mNKJ1x9QDXj1Ca8AfNaDf6WJzxtrWzg8Q8TV7qqqzIHiKPUcnwrngdgS0VDpPR2WyyCMEN7mf+HdYG+MOhA+Pgh8NXB12/AZ4/BB/fBQ4fDq5fDy5cARMpl/ObVVdzWBWsZBA3TNaRiXUOTz4eJ5wBw8LBoOQynXTF2QD4ASw2rbEjdbvrkufmuRrO133Etv1gHa+k0BkLkh4VANXBQlTV/xZUD0y6C7OKEc2q9Qa4NXMYP/Tfy8fiboaG82VpXm8rreXLpZnZUe5kWrrtUti6uzaOLN7J6e3W7++5rGisBAk2KNYzokxdxz+YONAX1qq8vAszquemyaI0IQQ8h1+3gY2Msi0KT+MoYysvu73NZ6Dpm+f5Mjc7mYvvr3LruFBa6f0E/tYeVrinxF7j0fTjjsaST2/5y3hR+PGMY1xw3iv9Uj2b3WKv++tIHEtpWNwZY7r6M094+EoCqhgAX2t9ms+cH/PDhhazeXsNYW3zK5VTbt5HnCsVQtYu1Ouon7vP+Dbzm/jUAM21fcEb5Qy36rK98djm3zV/DzupoquLq7dU0VkdLGrjnXwmfPMKWyoaoq6wNFkGkT3lu3A4bH66vQB/9G3M1uie+B8+dC6/9Et651Qysh/nXCRxXH13v+PEPN7f5tZojEAyRpfwoV07S4wcPj058c9ps5HucHNAvj2XGKHNn+bdMG1JEgz/EDfZruThwQ8I1wmnEHbUIGvzBiGsIYESZubBSUneWhUbzUugolhgT+VRNMne+3P/ohQAAF+1JREFUfIkptE2Y/fin3DJvNet317HDmujne+IMvvzUrK9U4w1wx+trufCxxJgUmHGW5upZ+evjs5aW/WA1T4Wsm6QLX4az/oXNpiKCqwdEV+MtpBZ/0KC+g9ZrVyNC0IOoUznMDtzAwwc8jvukP/JmYAqbdH/yVQNubbo/wql83oL9meh9NHrygMkw8ewWrz9jRG+CODho+XHUjT3PFIKY9XsNQ9MYCGG3sijYtYZQzS6ucpnzJbzb1/LYko0RCyDMlBgh8Pr97Kd2mYPV8b+Pa+fBx28dT3NK/X8i7pZkhDOWdtdGheCpj76jmFpWFh1HSFs+6QXXsXVPo1leAtplEdhsirOmDeKN1Tt5o340TJsNO1ZS5hrEasMSsT7jiJT02/IRNxqPRc7vT+fzzsMzW5UnL+nx4hwXS66fxctXHBapWfXcTw7hxd/8GCadD6f8hUMssVi6oYLNxYfxl1FPsoV+BJV5l7tCjyBgzzLnEnQgSN/Y6CVXefnEOCD+gDO5eAH8/QdTufyo/RnVN5c1e+zmPJuGCljxbELbsJtw2ebKSOzDjY8xr5+F3vopX+80XTnNZS+f+dBSJt2W6J4KGTpSshuAwv3Yr39v7gyez40HLDCt5vFmyvatp47jipn7c+JhU7kl57cAHJZfzpX2V9mzOz0q74sQ9CBem3MEC6+dyd9/MJVZo6NLK861n0Sg72TePGkp80KHAqCLhlFLFt+5RsLZT7Tp+r1z3Uzdz6x+em/AypRZ8yoE/eia7XDXEGbaYvL0/3UC59f/G+02zxlr+45Pvt3BZNsGvAMP5cFR5sA42WYG+AytyPHuwq2CzDz0YDjsqrjXf3T/DzHcpnuDmPLeTcm3Atzbq6JC0NhQR4mthv0GD40KFTBh5e8YmGdZQa1kDTXl9tPGM7wkh7+8+y3GQZdB0VBuVj/j+/7fcQW/Ysf5b8EFiYJ1gu1TPvJcBRsXQUPHS1TYAuYgZ2tGCMCMaUwbUhTZLspxUZDjhtMfhiGH0jvXzcg+ZirPlMFFGCVjOcp3L+cVz+UPxb/nbv1D3t7vatj6MXxwb9uyh5b+LbKkqq/OfH//Cx3CTYFLom1asAhG9s3jxhMPYHjvXNbtrMG41Mry2vR+Qtsiyy3z6eZK8jBdnnP7XUulzqXxrd+zbkcNU9S3PGv7bdKA98rS6qQxp6oGP7mxsbSSMfTJ8/Dfnx3B784+JK5t/4IsbjjhAEb0ySVYMBSAB/2/5jrnXDwf3Nns+9ybiBD0IMb0z2dYb/NOK8ftYOG1M3n4wqmc89tncV6+iJK+A5gTuIqDvH9n8Ak/Z3S/AgL/bxGMO73Nr/HyFYcx+7Ch/OurILsLJ8O7t8N9Y1nwxB+x+ap52Hl/tLGvmkHGNmx20696l/NRvvFcxFjbd7gmnI63ZBKlOlpewKY0/3SZGSsHjLFcAgdfHjl+xLbHGBmwCuvVNJ+RkecxB4cd1dEf8oyyF8jCR8HkU/naFi0RflzdPAYXWELQDtcQmDOcf37MSNbtrOXC/+7hi9MXsaB6P2aM7s8C7wQOvet97l4eP3DOLlzJwTYzL18/ex7cPQyqt/H+N2Uc/edFEWumTa9vLSVqzypopWXL3HjiARRmO7nq6BH0yXejNSzb1kj1gKNw5Jbwhu0osDnhvd/B549DMD4IqrWO+MLfXrUd3vp1JKW2ptKsfVWtc3g2dEz0pGZSXmM5flxfSvc08lZFb5h+sSkuTWIF+dZn/cWWKgpsDehhRzHtzGt4Kngc2Vvfp/TbL5nrup1xoXUJkwljY01Nffl7GvwUxkxspGQ0ABMHFeJsoTJrXv8RcdvZ2z9qVjwNQ/OLuSv4eGPqZyWLEPRghvXO4YTxlo9XKcb2N++md1PEwOI83rj6SEb0SZbY3TxKKX578lgmDS7k2roLMbBBfRnfq/w3AB4Vn5o6UJWTH0gshGebeiG9c12UadNaMGzmD/oAmzUzuni4+ffEu+D6xMDvN9+uY3tV8uwnl910g+yIiREM8K5nh2MQtuFHsuzwR1lhRMVgXJYVFGyHayjMyRMHMPuwoSzdUMHpD5oT7f589iQe//GB/OjQITz4Rfygeav3Lg7sZ/4sVdDq/z+O4L1PlrOxrJ61O9q+SIw9YhF0TgiOGdOXFTcfz9DeOYyPyTTKdttp8AeZt7qSOZ47zJ3/uwbuHRPnErzrja85/K6FaK35wzOvRy+87XOGvm6WRr9g1mTGWcFqs9Otl1U5ddIARvbJ5ZZ5q2g85Gpz4uVjx8F3S/l2Vy0n/mUJu2NWsSu2NaI8BQzvncPC3O8RwMnYDY/htFbdi+0zwDe7ogN9XRMBrqwP0JsY11D/Sa32F+Cq48dTccqTVJ35An8InE9O7SbYvSZp243l9fxn+TZz7ZAUI0IgRPA4u6a2u92m+NmsESyuG8Bw79PcGvgRhlbcHTiXOh3vXhmkyvH498DMm6g5+0UqjrwDrvwMXDkcMbKE+QOuZkfRdGw//A/BvJgSz/kxz7OL4bQHI35ln3bQT5fx71f+F7fIeXVDgFvnraas1sd+ahf+neso37Udw9AUBCuocZrrYc+aOpY3QgdGzhunrbUd2mkRhP8Xt546jnvOnsSPZwzljtPH0yvXzazRfbjt1HEctn8vZrvvZUffoyLnjC+Pppru0MXQUIFj43ucbV/Ep/99KG6eQUWtl5efe4yVH77BtS+ujMzFgKhFgLt511B7mTS4kC9vPZ7/G9eXs6YNosjKSptXOYjzbXfjn3Et+Oth/s9Ba7yBEA+/v4EHGq7H9++z4+I/3zx+Ob0NM1Pr4CO/x2tzjoCB09vcF4fdxm2njWNXjY8x96ziz/s/gS4cDC/O5qk3lrD2/7d35+FVlNcDx78nudmAECBA1IQIYQdlkaiAYBFFKFqlLCIiFGWpVH9V2QSkWtC6oIIiiiBVaN3ArRVLLQi4sLgQZVXDogFZJAkJCQbIen5/zCQkJEKA3FzJPZ/nuU9m3pln7nsul/vOvOv+TNLSUukY4PzQ1pIsCI1AROh0cQs+zr+IG6VYddKuteT9sKbo7n9v+vEbifSskjcwaVnZ1JUM8kJqw+h10Lp8gwarh3iI7NCHWhf3Yl34tc6N0sY3yjx3gzubbmGvI2/y2sI05tz0RP82pFZA/+YeraKYcl1LUg5nM/eTXrydfyWHqcZL+b2Ir5HOlJyZbNdofhforjTV7hZq1moAra8tukajutV54I4hwBAAPKNWwlPN4ar7IeCEe5j2gyGuG/rBJNbszqV71lIm7hrBT68t57zBc0GEuZ/sZMHaJAASQh4gcvdh9j5flwmtFnFPQRqpIc4I7gtqhRF5zb1M/bwBE48+TfyPLzvvcQZPBIX6d4ihf4eYEmkiwnVtzuf+dw/SKWMUPev04YWcSeRKMHOPdmdDQWNWFbTnuxp3cFH2Zn4ftAbS4dOFGXQdMR2Afy98ittTp5OfGMDBnLFsuqQO7Ro7A8A87hMBoTWpSDVDg5g7xPnBXjDsMjKP5bJ5bwaT3oFWqxqw7Mq/EPfZZJhai+Quj1OPcDoEbIcftjPKc/yuu1neNg5rGAeufZ4mIe6T57D3nYKknDrFRdK4XnV2pmTx7PosojpPZ/DWEUxOup2xIUKE2xX6odxbqUFW0XQVN13agLfXteCaQOdue0fBBTTZvBjP5sUkV29K/dFLS3QmSDuSQ2zk8eqqtKxcIiUTrV4XoopNB3Ma4ho2YlliZ3p+MQ9p1QdiOpQ4vtEtCKoFe/9n2p4ITAkD4hswulvpZTTPxIiucUzq3ZKOcXU4TDWa1K9BtoQwZmhftvdbxtN5/UgqiEJHrIRaDU59wfDz4L5d8JtfWPMgIhoZ+A+693EaHXcHxHDejkUkL32YW5/7kH3fOSNQwzlCpDutQbSk0nXLX6ivqRwLPd6APvKqFjw4cQrBLdyCKfwCqF1yIaGK0KNlFLF1qtG3fQxjhg5Axm3nkz5f8lTeTYS2vh4JCOTrnBh+H+g0fqdoBFfsmUd++m4mv7OJTsmvc1DDCaSAl4OfoOGia4rGbwTlVfwTwYliI6txUXQEAzrE8Gjfi4kIC6L7RxeSGedMShi7+j5uDlxZdH7bgJIjexfEPUnDjsWmiQgKg+olp50+GRHhzTs6s/HBa+nWvB5T1uYxt/Y4QvVoUSEAcJvnA2cd5jCnqrFZVDiNOjhjI3ZF9WBW3vF2sPpZ2+HJpkTsOt5bKP3I8ZujjKO57Ek/Ql3JICA8qtx5PdHASxsw9djNpFIbfbU/JJcc33B4zxZASTmcXfYFKpD8WgY0lFd8fLyuX7/e19kwpyG/QNmTfoTvU7JYlZjMtBud9Qvmf/o9zaLCubJZPS+8aS57MnJIeOZmbpRPyFaPs7gMsK0gmmYBexmZM4bx4ctplu30518ddw9dhk4teZ1Du2HHh9B+yGlPsX2m8vIL+CgxhW7N63HPog1kb1nC4IhNdLv6epZkteB3q3oBkFgQQ/OAPUzIHUk9MhgftBgAjbsKGfovls67n977ZsPE3aVm8PSW2Su38+SybdQMDeTGCzJ5aN/IomMf5belW+BGEj3NaZ7nNur/JbXCPteEXWn0m+Mspfo3z98Z7FlR+qSBr0BLZw1mVGH3OrYFt+S62euIyD9ElKRxS+BKBntWkOmpw4CsCSRqLE8NaEtMRBCNPhjKlANXsiy3HatCxtLooo7l7lVXlvmffs9rS1fwn+rTCMvLhK7jyOs6gU9ee4TuSU8zImcsSUSz7OHbi7r4nikRSVDVMuverCAwVdqDb37G1K09yzy2pPsyenSKZ+NDnbk84DtWt36ILgP+XMk5PLms7DzeSthDn3bRRFQLYkfyz3w561YGuQvOqyeMDjnzSMsJoO15ofRKXcBozxKOXTGBNdsPcHXyQnggvXRVmpfk5hfw3oZ9vPL5Lr7dn8lSuYe4gJ/Yf+l9PLomi1nBs8ntfC9B0e0gKwUuG3nqi56GjCO5XD3jY3J+TuNOz7/4o+c/bI1/hNb734GCXBi5qsyG6E+3pzBj+Tbu+E1j/vjPBFpLEguDH6OuZPJC/g3sC2nM/iPCi8Ez2KuRXJE9i61hI6l+6RDoPf2M86uqTHhrE599lcCqkPF4yGNt4zFcsuPZEh0rdt+0nNhWl53x+4AVBMaPbfzxEHPmzCRaUvhGG1LngiYE7Evg6ug8+vzpMRCh78z/cs3B16jXczwDupZ/pldfKChQhry4lk7Jb3BX/j+gzUBmho/j6x8PMe2G1jwy4wnmBc8E4EDg+dTIP0T1v1bsGs7lNWbxBlZ8lUjbmAgW/KknbSe/xSvBj9B2+PNwYSevva+qMu7NTbz9lTNn046//RZPgEBBXrmePmYsS2Tx+j20//lj5gSXnjMrW4OYljfEmWiuzxxod8tZ5Tc7L5/pHySybM3nLA57jPMLSv97HQyJJfKu5U716BnyWUEgIr2AZ4BAYL6qPnbC8WHAE0Dh8LrZqjqfk7CCwJyuJRv3UaBKz9bnkXI4m67TVzG+Z3PuvMrp073qu2RuW/Alr464nCualL9+2qdUnd4mLa8v0QYweOZ7vJoxpGg/MziKmpN9sx7Cmh2pjH4lgecHd6BL07rMWJbIsbwCJvcuxyR2FSA9K4e8AqVe+Ok38ucXKI0nL2VE2xDG9unCrdOep1vgRm4P/G/R4jnqCUXGba+wxvilm/fz+KIPuVdeo3WbS2ny/T+Ro2m8Xn8Mg5JnsC6yL6E3zqB9bO1TX6wMPikIRCQQ2Ab0APYAXwKDVPWbYucMA+JVtfQsaL/ACgJztg5kHiOyejCeYgN/Dh3JISIsqMSaAueijCO5/LwrgY9SatIofTWXtbgQT/Oyq8Yqg6qes5/psdx8ggMDCAgQVnx7gGrBHvYvGErfwNXOuIFej1f4k80PqVlkHs2lbYNazpTZh38igZYkzb+VfoGr+azBcDoOn3FG1/ZVQdAJ+Kuq9nT3JwGo6qPFzhmGFQTGmHPErp3fEL1zEZ6rp1Ra5wGAdd8kEbHhBVp27IXEdTuja5ysIPBmB9VooPgCuXuAy8s4r5+IXInz9HCvqpZaVFdERgGjAGJjY72QVWOMObULG7eCxlNPfWIF69SqIbTy3rxEvh5HsARoqKptgOXAwrJOUtV5qhqvqvH16nmhq6ExxvgxbxYEe4Hio4RiON4oDICqHlTVwtES84GSQ+uMMcZ4nTcLgi+BpiLSSESCgZuB94qfIFJsVWy4AfjWi/kxxhhTBq+1EahqnojcBfwPp/voS6q6VUSmAetV9T3gzyJyA5AHpAHDvJUfY4wxZbMBZcYY4wdO1mvI143FxhhjfMwKAmOM8XNWEBhjjJ8759oIRCQF2HXKE8tWF0itwOycCyxm/2Ax+4eziflCVS1zINY5VxCcDRFZ/0uNJVWVxewfLGb/4K2YrWrIGGP8nBUExhjj5/ytIJjn6wz4gMXsHyxm/+CVmP2qjcAYY0xp/vZEYIwx5gRWEBhjjJ/zm4JARHqJSKKI7BCRib7OT0URkZdEJFlEthRLqyMiy0Vku/u3tpsuIjLL/Qw2icglvsv5mRGRBiKySkS+EZGtInK3m16VYw4VkS9EZKMb81Q3vZGIfO7Gtsid5RcRCXH3d7jHG/oy/2dDRAJF5GsRed/dr9Ixi0iSiGwWkQ0ist5N8/p32y8KAnf95OeA3wKtgEEi0sq3uaowC4BeJ6RNBFaoalNghbsPTvxN3dcoYE4l5bEi5QFjVbUV0BG40/23rMoxZwPdVbUt0A7oJSIdgceBmaraBEgHhrvnDwfS3fSZ7nnnqrspOT29P8R8laq2KzZewPvfbVWt8i+gE/C/YvuTgEm+zlcFxtcQ2FJsPxE4390+H0h0t+cCg8o671x9Af8GevhLzEA14CucZV9TAY+bXvQdx5n6vZO77XHPE1/n/QxijXF/+LoD7wPiBzEnAXVPSPP6d9svnggoe/3kaB/lpTJEqep+d/snIMrdrlKfg/v43x74nCoes1tFsgFIxlnWdSdwSFXz3FOKx1UUs3s8A4is3BxXiKeBCUCBux9J1Y9ZgWUikuCu1Q6V8N325uL15ldAVVVEqlwfYRGpAbwN3KOqmSJSdKwqxqyq+UA7EakFvAu08HGWvEpErgeSVTVBRLr5Oj+VqIuq7hWR+sByEfmu+EFvfbf95YnglOsnVzEHCpcBdf8mu+lV4nMQkSCcQuBVVX3HTa7SMRdS1UPAKpxqkVoiUngzVzyuopjd4xHAwUrO6tm6ArhBRJKAN3Cqh56haseMqu51/ybjFPiXUQnfbX8pCE65fnIV8x7wB3f7Dzj16IXpQ93eBh2BjGKPnOcEcW79/w58q6ozih2qyjHXc58EEJEwnDaRb3EKhP7uaSfGXPhZ9AdWqluJfK5Q1UmqGqOqDXH+v65U1cFU4ZhFpLqIhBduA9cCW6iM77avG0cqsRGmN7ANp271fl/npwLjeh3YD+Ti1BEOx6kbXQFsBz4E6rjnCk7vqZ3AZiDe1/k/g3i74NSjbgI2uK/eVTzmNsDXbsxbgAfc9DjgC2AH8CYQ4qaHuvs73ONxvo7hLOPvBrxf1WN2Y9vovrYW/k5Vxnfbppgwxhg/5y9VQ8YYY36BFQTGGOPnrCAwxhg/ZwWBMcb4OSsIjDHGz1lBYMwJRCTfnf2x8FVhs9WKSEMpNlOsMb8GNsWEMaUdVdV2vs6EMZXFngiMKSd3rvjp7nzxX4hIEze9oYisdOeEXyEisW56lIi8664jsFFEOruXChSRF921BZa5o4WN8RkrCIwpLeyEqqGBxY5lqOrFwGyc2TEBngUWqmob4FVglps+C/hYnXUELsEZLQrO/PHPqWpr4BDQz8vxGHNSNrLYmBOIyM+qWqOM9CScBWK+dye++0lVI0UkFWce+Fw3fb+q1hWRFCBGVbOLXaMhsFydRUYQkfuAIFV92PuRGVM2eyIw5vToL2yfjuxi2/lYW53xMSsIjDk9A4v9Xedur8WZIRNgMPCpu70CGA1FC8tEVFYmjTkddidiTGlh7mpghT5Q1cIupLVFZBPOXf0gN+3/gJdFZDyQAtzmpt8NzBOR4Th3/qNxZoo15lfF2giMKSe3jSBeVVN9nRdjKpJVDRljjJ+zJwJjjPFz9kRgjDF+zgoCY4zxc1YQGGOMn7OCwBhj/JwVBMYY4+f+H2urOFxhsOI+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}